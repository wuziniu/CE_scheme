{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6025f1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import copy\n",
    "import pickle\n",
    "import sys\n",
    "import time\n",
    "sys.path.append(\"/Users/ziniuw/Desktop/research/Learned_QO/CE_scheme\")\n",
    "from BayesCard.Evaluation.utils import parse_query\n",
    "from Schemas.stats.schema import gen_stats_light_schema\n",
    "from Join_scheme.data_prepare import identify_key_values, process_stats_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d81e2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/Users/ziniuw/Desktop/past_research/End-to-End-CardEst-Benchmark/datasets/stats_simplified/{}.csv\"\n",
    "schema = gen_stats_light_schema(data_path)\n",
    "all_keys, equivalent_keys = identify_key_values(schema)\n",
    "print(equivalent_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabd33ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "bound_ensemble.bns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563660c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_card = 17849233970\n",
    "query57 = \"SELECT COUNT(*) FROM posts as p, postLinks as pl, postHistory as ph, votes as v, badges as b, users as u WHERE p.Id = pl.RelatedPostId AND u.Id = p.OwnerUserId AND u.Id = b.UserId AND u.Id = ph.UserId AND u.Id = v.UserId AND p.CommentCount>=0 AND p.CommentCount<=13 AND ph.PostHistoryTypeId=5 AND ph.CreationDate<='2014-08-13 09:20:10'::timestamp AND v.CreationDate>='2010-07-19 00:00:00'::timestamp AND b.Date<='2014-09-09 10:24:35'::timestamp AND u.Views>=0 AND u.DownVotes>=0 AND u.CreationDate>='2010-08-04 16:59:53'::timestamp AND u.CreationDate<='2014-07-22 15:15:22'::timestamp;\"\n",
    "t = time.time()\n",
    "tables_all, table_queries, join_cond, join_keys = bound_ensemble.parse_query_simple(query57)\n",
    "print(time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89833c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time.time()\n",
    "a = bound_ensemble.get_all_id_conidtional_distribution(table_queries, join_keys)\n",
    "print(time.time()-t)\n",
    "for table in a:\n",
    "    print(a[table][0], np.sum(a[table][1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edc0111",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"join keys:\", join_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b1d085",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Join_scheme.join_graph import process_condition, get_join_hyper_graph\n",
    "equivalent_group = get_join_hyper_graph(join_keys, bound_ensemble.equivalent_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a870ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "from Join_scheme.join_graph import process_condition, get_join_hyper_graph\n",
    "from Join_scheme.data_prepare import identify_key_values\n",
    "from BayesCard.Evaluation.cardinality_estimation import timestamp_transorform, construct_table_query\n",
    "\n",
    "\n",
    "class Factor:\n",
    "    \"\"\"\n",
    "    This the class defines a multidimensional conditional probability.\n",
    "    \"\"\"\n",
    "    def __init__(self, variables, pdfs, equivalent_variables=[]):\n",
    "        self.variables = variables\n",
    "        self.equivalent_variables = equivalent_variables\n",
    "        self.pdfs = pdfs\n",
    "        self.cardinalities = dict()\n",
    "        for i, var in enumerate(self.variables):\n",
    "            self.cardinalities[var] = pdfs.shape[i]\n",
    "            if len(equivalent_variables) != 0:\n",
    "                self.cardinalities[equivalent_variables[i]] = pdfs.shape[i]\n",
    "\n",
    "\n",
    "class Bound_ensemble:\n",
    "    \"\"\"\n",
    "    This the class where we store all the trained models and perform inference on the bound.\n",
    "    \"\"\"\n",
    "    def __init__(self, bns, table_buckets, schema):\n",
    "        self.bns = bns\n",
    "        self.table_buckets = table_buckets\n",
    "        self.schema = schema\n",
    "        self.all_keys, self.equivalent_keys = identify_key_values(schema)\n",
    "\n",
    "    def parse_query_simple(self, query):\n",
    "        \"\"\"\n",
    "        If your selection query contains no aggregation and nested sub-queries, you can use this function to parse a\n",
    "        join query. Otherwise, use parse_query function.\n",
    "        \"\"\"\n",
    "        query = query.replace(\" where \", \" WHERE \")\n",
    "        query = query.replace(\" from \", \" FROM \")\n",
    "        query = query.replace(\" and \", \" AND \")\n",
    "        query = query.split(\";\")[0]\n",
    "        query = query.strip()\n",
    "        tables_all = {}\n",
    "        join_cond = []\n",
    "        table_query = {}\n",
    "        join_keys = {}\n",
    "        tables_str = query.split(\" WHERE \")[0].split(\" FROM \")[-1]\n",
    "        for table_str in tables_str.split(\",\"):\n",
    "            table_str = table_str.strip()\n",
    "            if \" as \" in table_str:\n",
    "                tables_all[table_str.split(\" as \")[-1]] = table_str.split(\" as \")[0]\n",
    "            else:\n",
    "                tables_all[table_str.split(\" \")[-1]] = table_str.split(\" \")[0]\n",
    "\n",
    "        # processing conditions\n",
    "        conditions = query.split(\" WHERE \")[-1].split(\" AND \")\n",
    "        for cond in conditions:\n",
    "            table, cond, join, join_key = process_condition(cond, tables_all)\n",
    "            if not join:\n",
    "                attr = cond[0]\n",
    "                op = cond[1]\n",
    "                value = cond[2]\n",
    "                if \"Date\" in attr:\n",
    "                    assert \"::timestamp\" in value\n",
    "                    value = timestamp_transorform(value.strip().split(\"::timestamp\")[0])\n",
    "                if table not in table_query:\n",
    "                    table_query[table] = dict()\n",
    "                construct_table_query(self.bns[table], table_query[table], attr, op, value)\n",
    "            else:\n",
    "                join_cond.append(cond)\n",
    "                for tab in join_key:\n",
    "                    if tab in join_keys:\n",
    "                        join_keys[tab].add(join_key[tab])\n",
    "                    else:\n",
    "                        join_keys[tab] = set([join_key[tab]])\n",
    "\n",
    "        return tables_all, table_query, join_cond, join_keys\n",
    "\n",
    "    def get_all_id_conidtional_distribution(self, table_queries, join_keys, equivalent_group):\n",
    "        res = dict()\n",
    "        for table in join_keys:\n",
    "            key_attrs = list(join_keys[table])\n",
    "            if table in table_queries:\n",
    "                table_query = table_queries[table]\n",
    "            else:\n",
    "                table_query = {}\n",
    "            id_attrs, probs = self.bns[table].query_id_prob(table_query, key_attrs)\n",
    "            new_id_attrs = []\n",
    "            for K in id_attrs:\n",
    "                for PK in equivalent_group:\n",
    "                    if K in equivalent_group[PK]:\n",
    "                        new_id_attrs.append(PK)\n",
    "            assert len(new_id_attrs) == len(id_attrs)\n",
    "            res[table] = Factor(id_attrs, probs, new_id_attrs)\n",
    "        return res\n",
    "\n",
    "    def eliminate_one_key_group_general(self, tables, key_group, factors):\n",
    "        rest_groups = dict()\n",
    "        rest_group_tables = dict()\n",
    "        for table in tables:\n",
    "            assert key_group in factors[table].equivalent_variables\n",
    "            temp = copy.deepcopy(factors[table].equivalent_variables)\n",
    "            temp.remove(key_group)\n",
    "            for keys in temp:\n",
    "                if keys in rest_groups:\n",
    "                    assert factors[table].cardinalities[keys] == rest_groups[keys]\n",
    "                    rest_group_tables[keys].append(table)\n",
    "                else:\n",
    "                    rest_groups[keys] = factors[table].cardinalities[keys]\n",
    "                    rest_group_tables[keys] = [table]\n",
    "\n",
    "        new_factor_variables = []\n",
    "        new_factor_cardinalities = []\n",
    "        for key in rest_groups:\n",
    "            new_factor_variables.append(key)\n",
    "            new_factor_cardinalities.append(rest_groups[key])\n",
    "        new_factor_pdf = np.zeros(tuple(new_factor_cardinalities))\n",
    "\n",
    "    def eliminate_one_key_group(self, tables, key_group, factors, relevant_keys):\n",
    "        \"\"\"This version only supports 2D distributions\"\"\"\n",
    "        rest_group = None\n",
    "        rest_group_cardinalty = 0\n",
    "        eliminated_tables = []\n",
    "        rest_group_tables = []\n",
    "        for table in tables:\n",
    "            assert key_group in factors[table].equivalent_variables\n",
    "            temp = copy.deepcopy(factors[table].equivalent_variables)\n",
    "            temp.remove(key_group)\n",
    "            if len(temp) == 0:\n",
    "                eliminated_tables.append(table)\n",
    "            for key in temp:\n",
    "                if rest_group:\n",
    "                    assert factors[table].cardinalities[key] == rest_group_cardinalty\n",
    "                    rest_group_tables.append(table)\n",
    "                else:\n",
    "                    rest_group = key\n",
    "                    rest_group_cardinalty = factors[table].cardinalities[key]\n",
    "                    rest_group_tables = [table]\n",
    "\n",
    "        all_probs_eliminated = []\n",
    "        all_modes_eliminated = []\n",
    "        for table in eliminated_tables:\n",
    "            bin_modes = self.table_buckets[table].oned_bin_modes[relevant_keys[key_group][table]]\n",
    "            all_probs_eliminated.append(factors[table].pdfs)\n",
    "            all_modes_eliminated.append(np.minimum(bin_modes, factors[table].pdfs))\n",
    "\n",
    "        if rest_group:\n",
    "            new_factor_pdf = np.zeros(rest_group_cardinalty)\n",
    "        else:\n",
    "            return self.compute_bound_oned(all_probs_eliminated, all_modes_eliminated)\n",
    "\n",
    "        for i in range(rest_group_cardinalty):\n",
    "            for table in rest_group_tables:\n",
    "                idx_f = factors[table].equivalent_variables.index(key_group)\n",
    "                idx_b = self.table_buckets[table].id_attributes.index(relevant_keys[key_group][table])\n",
    "                bin_modes = self.table_buckets[table].twod_bin_modes[relevant_keys[key_group][table]]\n",
    "                if idx_f == 0 and idx_b == 0:\n",
    "                    all_probs_eliminated.append(factors[table].pdfs[:, i])\n",
    "                    all_modes_eliminated.append(np.minimum(bin_modes[:, i], factors[table].pdfs[:, i]))\n",
    "                elif idx_f == 0 and idx_b == 1:\n",
    "                    all_probs_eliminated.append(factors[table].pdfs[:, i])\n",
    "                    all_modes_eliminated.append(np.minimum(bin_modes[i, :], factors[table].pdfs[:, i]))\n",
    "                elif idx_f == 1 and idx_b == 0:\n",
    "                    all_probs_eliminated.append(factors[table].pdfs[i, :])\n",
    "                    all_modes_eliminated.append(np.minimum(bin_modes[:, i], factors[table].pdfs[i, :]))\n",
    "                else:\n",
    "                    all_probs_eliminated.append(factors[table].pdfs[i, :])\n",
    "                    all_modes_eliminated.append(np.minimum(bin_modes[i, :], factors[table].pdfs[i, :]))\n",
    "            new_factor_pdf[i] = self.compute_bound_oned(all_probs_eliminated, all_modes_eliminated)\n",
    "\n",
    "        for table in rest_group_tables:\n",
    "            factors[table] = Factor([rest_group], new_factor_pdf, [rest_group])\n",
    "\n",
    "        return None\n",
    "\n",
    "    def compute_bound_oned(self, all_probs, all_modes):\n",
    "        all_probs = np.stack(all_probs, axis=0)\n",
    "        all_modes = np.stack(all_modes, axis=0)\n",
    "        multiplier = np.prod(all_modes, axis=0)\n",
    "        non_zero_idx = np.where(multiplier != 0)[0]\n",
    "        min_number = np.amin(all_probs[:, non_zero_idx]/all_modes[:, non_zero_idx], axis=0)\n",
    "        multiplier[non_zero_idx] = multiplier[non_zero_idx] * min_number\n",
    "        return np.sum(multiplier)\n",
    "\n",
    "    def get_optimal_elimination_order(self, equivalent_group, join_keys, factors):\n",
    "        cardinalities = dict()\n",
    "        lengths = dict()\n",
    "        tables_involved = dict()\n",
    "        relevant_keys = dict()\n",
    "        for group in equivalent_group:\n",
    "            relevant_keys[group] = dict()\n",
    "            lengths[group] = len(equivalent_group[group])\n",
    "            cardinalities[group] = []\n",
    "            tables_involved[group] = set([])\n",
    "            for keys in equivalent_group[group]:\n",
    "                for table in join_keys:\n",
    "                    if keys in join_keys[table]:\n",
    "                        cardinalities[group].append(len(join_keys[table]))\n",
    "                        tables_involved[group].add(table)\n",
    "                        variables = factors[table].variables\n",
    "                        variables[variables.index(keys)] = group\n",
    "                        factors[table].variables = variables\n",
    "                        relevant_keys[group][table] = keys\n",
    "                        break\n",
    "            cardinalities[group] = np.asarray(cardinalities[group])\n",
    "\n",
    "        optimal_order = list(equivalent_group.keys())\n",
    "        for i in range(len(optimal_order)):\n",
    "            min_idx = i\n",
    "            for j in range(i+1, len(optimal_order)):\n",
    "                min_group = optimal_order[min_idx]\n",
    "                curr_group = optimal_order[j]\n",
    "                if np.max(cardinalities[curr_group]) < np.max(cardinalities[min_group]):\n",
    "                    min_idx = j\n",
    "                else:\n",
    "                    min_max_tables = np.max(cardinalities[min_group])\n",
    "                    min_num_max_tables = len(np.where(cardinalities[min_group] == min_max_tables)[0])\n",
    "                    curr_max_tables = np.max(cardinalities[curr_group])\n",
    "                    curr_num_max_tables = len(np.where(cardinalities[curr_group] == curr_max_tables)[0])\n",
    "                    if curr_num_max_tables < min_num_max_tables:\n",
    "                        min_idx = j\n",
    "                    elif lengths[curr_group] < lengths[min_group]:\n",
    "                        min_idx = j\n",
    "            optimal_order[i], optimal_order[min_idx] = optimal_order[min_idx], optimal_order[i]\n",
    "        return optimal_order, tables_involved, relevant_keys\n",
    "\n",
    "    def get_cardinality_bound(self, query_str):\n",
    "        tables_all, table_queries, join_cond, join_keys = self.parse_query_simple(query_str)\n",
    "        equivalent_group = get_join_hyper_graph(join_keys, self.equivalent_keys)\n",
    "        conditional_factors = self.get_all_id_conidtional_distribution(table_queries, join_keys, equivalent_group)\n",
    "        optimal_order, tables_involved, relevant_keys = self.get_optimal_elimination_order(equivalent_group, join_keys,\n",
    "                                                                            conditional_factors)\n",
    "\n",
    "        for key_group in optimal_order:\n",
    "            tables = tables_involved[key_group]\n",
    "            res = self.eliminate_one_key_group(tables, key_group, conditional_factors, relevant_keys)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056a9aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/Users/ziniuw/Desktop/research/Learned_QO/CC_model/CE_scheme_models/stats/model_200.pkl\"\n",
    "with open(model_path, \"rb\") as f:\n",
    "    new_BE = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de13cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_str = \"SELECT COUNT(*) FROM users as u, comments as c, posts as p WHERE p.OwnerUserId = u.Id AND p.Id = c.PostId AND u.UpVotes>=0 AND u.CreationDate>='2010-08-21 21:27:38'::timestamp AND c.CreationDate>='2010-07-21 11:05:37'::timestamp AND c.CreationDate<='2014-08-25 17:59:25'::timestamp\"\n",
    "t = time.time()\n",
    "res = new_BE.get_cardinality_bound(query57)\n",
    "print(time.time() - t)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ff26f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_file = \"/Users/ziniuw/Desktop/past_research/End-to-End-CardEst-Benchmark/workloads/stats_CEB/sub_plan_queries/stats_CEB_sub_queries.sql\"\n",
    "with open(query_file, \"r\") as f:\n",
    "    queries = f.readlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef76e71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "qerror = []\n",
    "latency = []\n",
    "pred = []\n",
    "for i, query_str in enumerate(queries):\n",
    "    query = query_str.split(\"||\")[0][:-1]\n",
    "    print(\"========================\")\n",
    "    true_card = int(query_str.split(\"||\")[-1])\n",
    "    t = time.time()\n",
    "    res = new_BE.get_cardinality_bound(query)\n",
    "    pred.append(res)\n",
    "    latency.append(time.time() - t)\n",
    "    qerror.append(res/true_card)\n",
    "    print(f\"estimating query {i}: predicted {res}, true_card {true_card}, qerror {res/true_card}, latency {time.time() - t}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e595aff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "qerror = np.asarray(qerror)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e6d0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_qerror = copy.deepcopy(qerror)\n",
    "temp_qerror[temp_qerror < 1] = 1/temp_qerror[temp_qerror < 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73add80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [50, 90, 95, 99, 100]:\n",
    "    print(f\"q-error {i}% percentile is {np.percentile(temp_qerror, i)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f81d047",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"stats_CEB_sub_queries_CE_scheme.txt\", \"w\") as f:\n",
    "    for p in pred:\n",
    "        f.write(str(p)+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301c046f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"stats_CEB_exec.sql\", \"r\") as f:\n",
    "    queries = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e23ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"stats_CEB_exec.sql\", \"w\") as f:\n",
    "    for q in queries:\n",
    "        q = q.split(\"||\")[-1]\n",
    "        f.write(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38019fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
