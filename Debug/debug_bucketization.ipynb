{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5d492b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"/home/ubuntu/CE_scheme/\")\n",
    "from Schemas.imdb.schema import gen_imdb_schema\n",
    "from Join_scheme.data_prepare import read_table_csv\n",
    "from Join_scheme.bound import Bound_ensemble\n",
    "from Join_scheme.join_graph import parse_query_all_join, get_join_hyper_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1922eeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import logging\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from Schemas.imdb.schema import gen_imdb_schema\n",
    "from Join_scheme.binning import identify_key_values, sub_optimal_bucketize, Table_bucket\n",
    "from Join_scheme.binning import apply_binning_to_data_value_count\n",
    "from Join_scheme.bound import Factor\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def timestamp_transorform(time_string, start_date=\"2010-07-19 00:00:00\"):\n",
    "    start_date_int = time.strptime(start_date, \"%Y-%m-%d %H:%M:%S\")\n",
    "    time_array = time.strptime(time_string, \"%Y-%m-%d %H:%M:%S\")\n",
    "    return int(time.mktime(time_array)) - int(time.mktime(start_date_int))\n",
    "\n",
    "\n",
    "def read_table_hdf(table_obj):\n",
    "    \"\"\"\n",
    "    Reads hdf from path, renames columns and drops unnecessary columns\n",
    "    \"\"\"\n",
    "    df_rows = pd.read_hdf(table_obj.csv_file_location)\n",
    "    df_rows.columns = [table_obj.table_name + '.' + attr for attr in table_obj.attributes]\n",
    "\n",
    "    for attribute in table_obj.irrelevant_attributes:\n",
    "        df_rows = df_rows.drop(table_obj.table_name + '.' + attribute, axis=1)\n",
    "\n",
    "    return df_rows.apply(pd.to_numeric, errors=\"ignore\")\n",
    "\n",
    "\n",
    "def read_table_csv(table_obj, csv_seperator=',', stats=True):\n",
    "    \"\"\"\n",
    "    Reads csv from path, renames columns and drops unnecessary columns\n",
    "    \"\"\"\n",
    "    if stats:\n",
    "        df_rows = pd.read_csv(table_obj.csv_file_location)\n",
    "    else:\n",
    "        df_rows = pd.read_csv(table_obj.csv_file_location, header=None, escapechar='\\\\', encoding='utf-8',\n",
    "                              quotechar='\"',\n",
    "                              sep=csv_seperator)\n",
    "    df_rows.columns = [table_obj.table_name + '.' + attr for attr in table_obj.attributes]\n",
    "\n",
    "    for attribute in table_obj.irrelevant_attributes:\n",
    "        df_rows = df_rows.drop(table_obj.table_name + '.' + attribute, axis=1)\n",
    "\n",
    "    return df_rows.apply(pd.to_numeric, errors=\"ignore\")\n",
    "\n",
    "\n",
    "def make_sample(np_data, nrows=1000000, seed=0):\n",
    "    np.random.seed(seed)\n",
    "    samp_data = np_data[np_data != -1]\n",
    "    if len(samp_data) <= nrows:\n",
    "        return samp_data, 1.0\n",
    "    else:\n",
    "        selected = np.random.choice(len(samp_data), size=nrows, replace=False)\n",
    "        return samp_data[selected], nrows/len(samp_data)\n",
    "\n",
    "\n",
    "def stats_analysis(sample, data, sample_rate, show=10):\n",
    "    n, c = np.unique(sample, return_counts=True)\n",
    "    idx = np.argsort(c)[::-1]\n",
    "    for i in range(min(show, len(idx))):\n",
    "        print(c[idx[i]], c[idx[i]]/sample_rate, len(data[data == n[idx[i]]]))\n",
    "\n",
    "\n",
    "def get_ground_truth_no_filter(equivalent_keys, data, bins, table_lens, na_values):\n",
    "    all_factor_pdfs = dict()\n",
    "    for PK in equivalent_keys:\n",
    "        bin_value = bins[PK]\n",
    "        for key in equivalent_keys[PK]:\n",
    "            table = key.split(\".\")[0]\n",
    "            temp = apply_binning_to_data_value_count(bin_value, data[key])\n",
    "            if table not in all_factor_pdfs:\n",
    "                all_factor_pdfs[table] = dict()\n",
    "            all_factor_pdfs[table][key] = temp / np.sum(temp)\n",
    "\n",
    "    all_factors = dict()\n",
    "    for table in all_factor_pdfs:\n",
    "        all_factors[table] = Factor(table, table_lens[table], list(all_factor_pdfs[table].keys()),\n",
    "                                    all_factor_pdfs[table], na_values[table])\n",
    "    return all_factors\n",
    "\n",
    "\n",
    "def process_imdb_data(data_path, model_folder, n_bins, sample_size=100000, save_bucket_bins=False):\n",
    "    schema = gen_imdb_schema(data_path)\n",
    "    all_keys, equivalent_keys = identify_key_values(schema)\n",
    "    data = dict()\n",
    "    table_lens = dict()\n",
    "    na_values = dict()\n",
    "    primary_keys = []\n",
    "    for table_obj in schema.tables:\n",
    "        df_rows = pd.read_csv(table_obj.csv_file_location, header=None, escapechar='\\\\', encoding='utf-8',\n",
    "                              quotechar='\"',\n",
    "                              sep=\",\")\n",
    "\n",
    "        df_rows.columns = [table_obj.table_name + '.' + attr for attr in table_obj.attributes]\n",
    "        for attribute in table_obj.irrelevant_attributes:\n",
    "            df_rows = df_rows.drop(table_obj.table_name + '.' + attribute, axis=1)\n",
    "\n",
    "        df_rows.apply(pd.to_numeric, errors=\"ignore\")\n",
    "        table_lens[table_obj.table_name] = len(df_rows)\n",
    "        if table_obj.table_name not in na_values:\n",
    "            na_values[table_obj.table_name] = dict()\n",
    "        for attr in df_rows.columns:\n",
    "            if attr in all_keys:\n",
    "                data[attr] = df_rows[attr].values\n",
    "                data[attr][np.isnan(data[attr])] = -1\n",
    "                data[attr][data[attr] < 0] = -1\n",
    "                na_values[table_obj.table_name][attr] = len(data[attr][data[attr] != -1]) / table_lens[\n",
    "                    table_obj.table_name]\n",
    "                data[attr] = copy.deepcopy(data[attr])[data[attr] >= 0]\n",
    "                if len(np.unique(data[attr])) >= len(data[attr]) - 10:\n",
    "                    primary_keys.append(attr)\n",
    "\n",
    "    sample_rate = dict()\n",
    "    sampled_data = dict()\n",
    "    for k in data:\n",
    "        temp = make_sample(data[k], 1000000)\n",
    "        sampled_data[k] = temp[0]\n",
    "        sample_rate[k] = temp[1]\n",
    "\n",
    "    optimal_buckets = dict()\n",
    "    bin_size = dict()\n",
    "    all_bin_modes = dict()\n",
    "    for PK in equivalent_keys:\n",
    "        # if PK != 'kind_type.id':\n",
    "        #   continue\n",
    "        group_data = {}\n",
    "        group_sample_rate = {}\n",
    "        for K in equivalent_keys[PK]:\n",
    "            group_data[K] = sampled_data[K]\n",
    "            group_sample_rate[K] = sample_rate[K]\n",
    "        _, optimal_bucket = sub_optimal_bucketize(group_data, group_sample_rate, n_bins=n_bins[PK], primary_keys=primary_keys)\n",
    "        optimal_buckets[PK] = optimal_bucket\n",
    "        for K in equivalent_keys[PK]:\n",
    "            temp_table_name = K.split(\".\")[0]\n",
    "            if temp_table_name not in bin_size:\n",
    "                bin_size[temp_table_name] = dict()\n",
    "                all_bin_modes[temp_table_name] = dict()\n",
    "            bin_size[temp_table_name][K] = len(optimal_bucket.bins)\n",
    "            all_bin_modes[temp_table_name][K] = optimal_bucket.buckets[K].bin_modes\n",
    "\n",
    "    table_buckets = dict()\n",
    "    for table_name in bin_size:\n",
    "        table_buckets[table_name] = Table_bucket(table_name, list(bin_size[table_name].keys()), bin_size[table_name],\n",
    "                                                 all_bin_modes[table_name])\n",
    "    \n",
    "    all_bins = dict()\n",
    "    for key in optimal_buckets:\n",
    "        all_bins[key] = optimal_buckets[key].bins\n",
    "\n",
    "    ground_truth_factors_no_filter = get_ground_truth_no_filter(equivalent_keys, data, all_bins, table_lens, na_values)\n",
    "\n",
    "    if save_bucket_bins:\n",
    "        with open(model_folder + f\"/imdb_buckets.pkl\") as f:\n",
    "            pickle.dump(optimal_buckets, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    return schema, table_buckets, ground_truth_factors_no_filter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e095e348",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/bayescard/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3364: DtypeWarning: Columns (5,10) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-0af32f1706ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mmodel_folder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/home/ubuntu/data_CE/CE_scheme_models/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m schema, table_buckets, ground_truth_factors_no_filter = process_imdb_data(data_path, model_folder, n_bins,\n\u001b[0;32m---> 17\u001b[0;31m                                                                               False)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-3e034a8dc042>\u001b[0m in \u001b[0;36mprocess_imdb_data\u001b[0;34m(data_path, model_folder, n_bins, sample_size, save_bucket_bins)\u001b[0m\n\u001b[1;32m     96\u001b[0m         df_rows = pd.read_csv(table_obj.csv_file_location, header=None, escapechar='\\\\', encoding='utf-8',\n\u001b[1;32m     97\u001b[0m                               \u001b[0mquotechar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\"'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m                               sep=\",\")\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mdf_rows\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtable_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtable_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mattr\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mattr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtable_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattributes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bayescard/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    684\u001b[0m     )\n\u001b[1;32m    685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bayescard/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bayescard/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1209\u001b[0m             \u001b[0mnew_rows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1211\u001b[0;31m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1213\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_currow\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnew_rows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bayescard/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m             \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bayescard/lib/python3.7/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36minit_dict\u001b[0;34m(data, index, columns, dtype)\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0marr\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_datetime64tz_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m         ]\n\u001b[0;32m--> 283\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bayescard/lib/python3.7/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, arr_names, index, columns, dtype, verify_integrity)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;31m# don't force copy because getting jammed in an ndarray anyway\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_homogenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bayescard/lib/python3.7/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m_homogenize\u001b[0;34m(data, index, dtype)\u001b[0m\n\u001b[1;32m    350\u001b[0m                 \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfast_multiget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m             val = sanitize_array(\n\u001b[0;32m--> 352\u001b[0;31m                 \u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_cast_failure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m             )\n\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bayescard/lib/python3.7/site-packages/pandas/core/construction.py\u001b[0m in \u001b[0;36msanitize_array\u001b[0;34m(data, index, dtype, copy, raise_cast_failure)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_object_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubarr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_object_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 513\u001b[0;31m             \u001b[0minferred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    514\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minferred\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"interval\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"period\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m                 \u001b[0msubarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_bins = {\n",
    "    'title.id': 800,\n",
    "    'info_type.id': 100,\n",
    "    'keyword.id': 100,\n",
    "    'company_name.id': 100,\n",
    "    'name.id': 100,\n",
    "    'company_type.id': 100,\n",
    "    'comp_cast_type.id': 50,\n",
    "    'kind_type.id': 50,\n",
    "    'char_name.id': 50,\n",
    "    'role_type.id': 50,\n",
    "    'link_type.id': 50\n",
    "}\n",
    "data_path = \"/home/ubuntu/data_CE/imdb/{}.csv\"\n",
    "model_folder = \"/home/ubuntu/data_CE/CE_scheme_models/\"\n",
    "schema, table_buckets, ground_truth_factors_no_filter = process_imdb_data(data_path, model_folder, n_bins,\n",
    "                                                                              False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0359f926",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "from Join_scheme.join_graph import get_join_hyper_graph, parse_query_all_join\n",
    "from Join_scheme.data_prepare import identify_key_values\n",
    "#from Sampling.load_sample import load_sample_imdb_one_query\n",
    "\n",
    "\n",
    "class Factor:\n",
    "    \"\"\"\n",
    "    This the class defines a multidimensional conditional probability on one table.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, table, table_len, variables, pdfs, na_values=None):\n",
    "        self.table = table\n",
    "        self.table_len = table_len\n",
    "        self.variables = variables\n",
    "        self.pdfs = pdfs\n",
    "        self.na_values = na_values  # this is the percentage of data, which is not nan.\n",
    "\n",
    "\n",
    "class Group_Factor:\n",
    "    \"\"\"\n",
    "        This the class defines a multidimensional conditional probability on a group of tables.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tables, tables_size, variables, pdfs, bin_modes, equivalent_groups=None, \n",
    "                 table_key_equivalent_group=None, na_values=None, join_cond=None):\n",
    "        self.table = tables\n",
    "        self.tables_size = tables_size\n",
    "        self.variables = variables\n",
    "        self.pdfs = pdfs\n",
    "        self.bin_modes = bin_modes\n",
    "        self.equivalent_groups = equivalent_groups\n",
    "        self.table_key_equivalent_group = table_key_equivalent_group\n",
    "        self.na_values = na_values\n",
    "        self.join_cond = join_cond\n",
    "\n",
    "\n",
    "class Bound_ensemble:\n",
    "    \"\"\"\n",
    "    This the class where we store all the trained models and perform inference on the bound.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, table_buckets, schema, ground_truth_factors_no_filter=None):\n",
    "        self.table_buckets = table_buckets\n",
    "        self.schema = schema\n",
    "        self.all_keys, self.equivalent_keys = identify_key_values(schema)\n",
    "        self.all_join_conds = None\n",
    "        self.ground_truth_factors_no_filter = ground_truth_factors_no_filter\n",
    "        # self.reverse_table_alias = None\n",
    "\n",
    "    def parse_query_simple(self, query):\n",
    "        \"\"\"\n",
    "        If your selection query contains no aggregation and nested sub-queries, you can use this function to parse a\n",
    "        join query. Otherwise, use parse_query function.\n",
    "        \"\"\"\n",
    "        tables_all, join_cond, join_keys = parse_query_all_join(query)\n",
    "        # TODO: implement functions on parsing filter conditions.\n",
    "        table_filters = dict()\n",
    "        return tables_all, table_filters, join_cond, join_keys\n",
    "\n",
    "    def get_all_id_conidtional_distribution(self, query_file_name, tables_alias, join_keys):\n",
    "        # TODO: make it work on query-driven and sampling based\n",
    "        return load_sample_imdb_one_query(self.table_buckets, tables_alias, query_file_name, join_keys,\n",
    "                                          self.ground_truth_factors_no_filter)\n",
    "\n",
    "    def eliminate_one_key_group(self, tables, key_group, factors, relevant_keys):\n",
    "        \"\"\"This version only supports 2D distributions (i.e. the distribution learned with tree-structured PGM)\"\"\"\n",
    "        rest_group = None\n",
    "        rest_group_cardinalty = 0\n",
    "        eliminated_tables = []\n",
    "        rest_group_tables = []\n",
    "        for table in tables:\n",
    "            assert key_group in factors[table].equivalent_variables\n",
    "            temp = copy.deepcopy(factors[table].equivalent_variables)\n",
    "            temp.remove(key_group)\n",
    "            if len(temp) == 0:\n",
    "                eliminated_tables.append(table)\n",
    "            for key in temp:\n",
    "                if rest_group:\n",
    "                    assert factors[table].cardinalities[key] == rest_group_cardinalty\n",
    "                    rest_group_tables.append(table)\n",
    "                else:\n",
    "                    rest_group = key\n",
    "                    rest_group_cardinalty = factors[table].cardinalities[key]\n",
    "                    rest_group_tables = [table]\n",
    "\n",
    "        all_probs_eliminated = []\n",
    "        all_modes_eliminated = []\n",
    "        for table in eliminated_tables:\n",
    "            bin_modes = self.table_buckets[table].oned_bin_modes[relevant_keys[key_group][table]]\n",
    "            all_probs_eliminated.append(factors[table].pdfs)\n",
    "            all_modes_eliminated.append(np.minimum(bin_modes, factors[table].pdfs))\n",
    "\n",
    "        if rest_group:\n",
    "            new_factor_pdf = np.zeros(rest_group_cardinalty)\n",
    "        else:\n",
    "            return self.compute_bound_oned(all_probs_eliminated, all_modes_eliminated)\n",
    "\n",
    "        for i in range(rest_group_cardinalty):\n",
    "            for table in rest_group_tables:\n",
    "                idx_f = factors[table].equivalent_variables.index(key_group)\n",
    "                idx_b = self.table_buckets[table].id_attributes.index(relevant_keys[key_group][table])\n",
    "                bin_modes = self.table_buckets[table].twod_bin_modes[relevant_keys[key_group][table]]\n",
    "                if idx_f == 0 and idx_b == 0:\n",
    "                    all_probs_eliminated.append(factors[table].pdfs[:, i])\n",
    "                    all_modes_eliminated.append(np.minimum(bin_modes[:, i], factors[table].pdfs[:, i]))\n",
    "                elif idx_f == 0 and idx_b == 1:\n",
    "                    all_probs_eliminated.append(factors[table].pdfs[:, i])\n",
    "                    all_modes_eliminated.append(np.minimum(bin_modes[i, :], factors[table].pdfs[:, i]))\n",
    "                elif idx_f == 1 and idx_b == 0:\n",
    "                    all_probs_eliminated.append(factors[table].pdfs[i, :])\n",
    "                    all_modes_eliminated.append(np.minimum(bin_modes[:, i], factors[table].pdfs[i, :]))\n",
    "                else:\n",
    "                    all_probs_eliminated.append(factors[table].pdfs[i, :])\n",
    "                    all_modes_eliminated.append(np.minimum(bin_modes[i, :], factors[table].pdfs[i, :]))\n",
    "            new_factor_pdf[i] = self.compute_bound_oned(all_probs_eliminated, all_modes_eliminated)\n",
    "\n",
    "        for table in rest_group_tables:\n",
    "            factors[table] = Factor([rest_group], new_factor_pdf, [rest_group])\n",
    "\n",
    "        return None\n",
    "\n",
    "    def compute_bound_oned(self, all_probs, all_modes, return_factor=False):\n",
    "        temp_all_modes = []\n",
    "        for i in range(len(all_modes)):\n",
    "            temp_all_modes.append(np.minimum(all_probs[i], all_modes[i]))\n",
    "        all_probs = np.stack(all_probs, axis=0)\n",
    "        temp_all_modes = np.stack(temp_all_modes, axis=0)\n",
    "        multiplier = np.prod(temp_all_modes, axis=0)\n",
    "        non_zero_idx = np.where(multiplier != 0)[0]\n",
    "        min_number = np.amin(all_probs[:, non_zero_idx] / temp_all_modes[:, non_zero_idx], axis=0)\n",
    "        # print(min_number, multiplier[non_zero_idx])\n",
    "        if return_factor:\n",
    "            new_probs = np.zeros(multiplier.shape)\n",
    "            new_probs[non_zero_idx] = multiplier[non_zero_idx] * min_number\n",
    "            return new_probs, multiplier\n",
    "        else:\n",
    "            multiplier[non_zero_idx] = multiplier[non_zero_idx] * min_number\n",
    "            return np.sum(multiplier)\n",
    "\n",
    "    def get_optimal_elimination_order(self, equivalent_group, join_keys, factors):\n",
    "        \"\"\"\n",
    "        This function determines the optimial elimination order for each key group\n",
    "        \"\"\"\n",
    "        cardinalities = dict()\n",
    "        lengths = dict()\n",
    "        tables_involved = dict()\n",
    "        relevant_keys = dict()\n",
    "        for group in equivalent_group:\n",
    "            relevant_keys[group] = dict()\n",
    "            lengths[group] = len(equivalent_group[group])\n",
    "            cardinalities[group] = []\n",
    "            tables_involved[group] = set([])\n",
    "            for keys in equivalent_group[group]:\n",
    "                for table in join_keys:\n",
    "                    if keys in join_keys[table]:\n",
    "                        cardinalities[group].append(len(join_keys[table]))\n",
    "                        tables_involved[group].add(table)\n",
    "                        variables = factors[table].variables\n",
    "                        variables[variables.index(keys)] = group\n",
    "                        factors[table].variables = variables\n",
    "                        relevant_keys[group][table] = keys\n",
    "                        break\n",
    "            cardinalities[group] = np.asarray(cardinalities[group])\n",
    "\n",
    "        optimal_order = list(equivalent_group.keys())\n",
    "        for i in range(len(optimal_order)):\n",
    "            min_idx = i\n",
    "            for j in range(i + 1, len(optimal_order)):\n",
    "                min_group = optimal_order[min_idx]\n",
    "                curr_group = optimal_order[j]\n",
    "                if np.max(cardinalities[curr_group]) < np.max(cardinalities[min_group]):\n",
    "                    min_idx = j\n",
    "                else:\n",
    "                    min_max_tables = np.max(cardinalities[min_group])\n",
    "                    min_num_max_tables = len(np.where(cardinalities[min_group] == min_max_tables)[0])\n",
    "                    curr_max_tables = np.max(cardinalities[curr_group])\n",
    "                    curr_num_max_tables = len(np.where(cardinalities[curr_group] == curr_max_tables)[0])\n",
    "                    if curr_num_max_tables < min_num_max_tables:\n",
    "                        min_idx = j\n",
    "                    elif lengths[curr_group] < lengths[min_group]:\n",
    "                        min_idx = j\n",
    "            optimal_order[i], optimal_order[min_idx] = optimal_order[min_idx], optimal_order[i]\n",
    "        return optimal_order, tables_involved, relevant_keys\n",
    "\n",
    "    def get_cardinality_bound_one(self, query_str):\n",
    "        tables_all, table_queries, join_cond, join_keys = self.parse_query_simple(query_str)\n",
    "        equivalent_group = get_join_hyper_graph(join_keys, self.equivalent_keys)\n",
    "        conditional_factors = self.get_all_id_conidtional_distribution(table_queries, join_keys, equivalent_group)\n",
    "        optimal_order, tables_involved, relevant_keys = self.get_optimal_elimination_order(equivalent_group, join_keys,\n",
    "                                                                                           conditional_factors)\n",
    "\n",
    "        for key_group in optimal_order:\n",
    "            tables = tables_involved[key_group]\n",
    "            res = self.eliminate_one_key_group(tables, key_group, conditional_factors, relevant_keys)\n",
    "        return res\n",
    "\n",
    "    def get_sub_plan_queries_sql(self, query_str, sub_plan_query_str_all, query_name=None):\n",
    "        tables_all, table_queries, join_cond, join_keys = self.parse_query_simple(query_str)\n",
    "        equivalent_group, table_equivalent_group, table_key_equivalent_group = get_join_hyper_graph(join_keys,\n",
    "                                                                                                    self.equivalent_keys)\n",
    "        cached_sub_queries_sql = dict()\n",
    "        cached_union_key_group = dict()\n",
    "        res_sql = []\n",
    "        for (left_tables, right_tables) in sub_plan_query_str_all:\n",
    "            assert \" \" not in left_tables, f\"{left_tables} contains more than one tables, violating left deep plan\"\n",
    "            sub_plan_query_list = right_tables.split(\" \") + [left_tables]\n",
    "            sub_plan_query_list.sort()\n",
    "            sub_plan_query_str = \" \".join(sub_plan_query_list)  # get the string name of the sub plan query\n",
    "            sql_header = \"SELECT COUNT(*) FROM \"\n",
    "            for alias in sub_plan_query_list:\n",
    "                sql_header += (tables_all[alias] + \" AS \" + alias + \", \")\n",
    "            sql_header = sql_header[:-2] + \" WHERE \"\n",
    "            if \" \" in right_tables:\n",
    "                assert right_tables in cached_sub_queries_sql, f\"{right_tables} not in cache, input is not ordered\"\n",
    "                right_sql = cached_sub_queries_sql[right_tables]\n",
    "                right_union_key_group = cached_union_key_group[right_tables]\n",
    "                if left_tables in table_queries:\n",
    "                    left_sql = table_queries[left_tables]\n",
    "                    curr_sql = right_sql + \" AND (\" + left_sql + \")\"\n",
    "                else:\n",
    "                    curr_sql = right_sql\n",
    "                additional_joins, union_key_group = self.get_additional_join_with_table_group(left_tables,\n",
    "                                                                                              right_union_key_group,\n",
    "                                                                                              table_equivalent_group,\n",
    "                                                                                              table_key_equivalent_group)\n",
    "                for join in additional_joins:\n",
    "                    curr_sql = curr_sql + \" AND \" + join\n",
    "            else:\n",
    "                curr_sql = \"\"\n",
    "                if left_tables in table_queries:\n",
    "                    curr_sql += (\"(\" + table_queries[left_tables] + \")\")\n",
    "                if right_tables in table_queries:\n",
    "                    if curr_sql != \"\":\n",
    "                        curr_sql += \" AND \"\n",
    "                    curr_sql += (\"(\" + table_queries[right_tables] + \")\")\n",
    "\n",
    "                additional_joins, union_key_group = self.get_additional_joins_two_tables(left_tables, right_tables,\n",
    "                                                                                         table_equivalent_group,\n",
    "                                                                                         table_key_equivalent_group)\n",
    "                for join in additional_joins:\n",
    "                    if curr_sql == \"\":\n",
    "                        curr_sql += join\n",
    "                    else:\n",
    "                        curr_sql = curr_sql + \" AND \" + join\n",
    "            cached_sub_queries_sql[sub_plan_query_str] = curr_sql\n",
    "            cached_union_key_group[sub_plan_query_str] = union_key_group\n",
    "            res_sql.append(sql_header + curr_sql + \";\")\n",
    "        return res_sql\n",
    "\n",
    "    def get_cardinality_bound_all(self, query_str, sub_plan_query_str_all, query_name=None, debug=False,\n",
    "                                  true_card=None):\n",
    "        \"\"\"\n",
    "        Get the cardinality bounds for all sub_plan_queires of a query.\n",
    "        Note: Due to efficiency, this current version only support left_deep plans (like the one generated by postgres),\n",
    "              but it can easily support right deep or bushy plans.\n",
    "        :param query_str: the target query\n",
    "        :param sub_plan_query_str_all: all sub_plan_queries of the target query,\n",
    "               it should be sorted by number of the tables in the sub_plan_query\n",
    "        \"\"\"\n",
    "        tables_all, table_queries, join_cond, join_keys = self.parse_query_simple(query_str)\n",
    "        #print(join_cond)\n",
    "        # print(join_keys)\n",
    "        equivalent_group, table_equivalent_group, table_key_equivalent_group, table_key_group_map = \\\n",
    "            get_join_hyper_graph(join_keys, self.equivalent_keys)\n",
    "        conditional_factors = self.get_all_id_conidtional_distribution(query_name, tables_all, join_keys)\n",
    "        # self.reverse_table_alias = {v: k for k, v in tables_all.items()}\n",
    "        cached_sub_queries = dict()\n",
    "        cardinality_bounds = []\n",
    "        for i, (left_tables, right_tables) in enumerate(sub_plan_query_str_all):\n",
    "            assert \" \" not in left_tables, f\"{left_tables} contains more than one tables, violating left deep plan\"\n",
    "            sub_plan_query_list = right_tables.split(\" \") + [left_tables]\n",
    "            sub_plan_query_list.sort()\n",
    "            sub_plan_query_str = \" \".join(sub_plan_query_list)  # get the string name of the sub plan query\n",
    "            # print(sub_plan_query_str)\n",
    "            #print(sub_plan_query_str, \"=========================================\")\n",
    "            if \" \" in right_tables:\n",
    "                assert right_tables in cached_sub_queries, f\"{right_tables} not in cache, input is not ordered\"\n",
    "                right_bound_factor = cached_sub_queries[right_tables]\n",
    "                curr_bound_factor, res = self.join_with_one_table(sub_plan_query_str,\n",
    "                                                                  left_tables,\n",
    "                                                                  tables_all,\n",
    "                                                                  right_bound_factor,\n",
    "                                                                  conditional_factors[left_tables],\n",
    "                                                                  table_equivalent_group,\n",
    "                                                                  table_key_equivalent_group,\n",
    "                                                                  table_key_group_map,\n",
    "                                                                  join_cond)\n",
    "            else:\n",
    "                curr_bound_factor, res = self.join_two_tables(sub_plan_query_str,\n",
    "                                                              left_tables,\n",
    "                                                              right_tables,\n",
    "                                                              tables_all,\n",
    "                                                              conditional_factors,\n",
    "                                                              join_keys,\n",
    "                                                              table_equivalent_group,\n",
    "                                                              table_key_equivalent_group,\n",
    "                                                              table_key_group_map,\n",
    "                                                              join_cond)\n",
    "            cached_sub_queries[sub_plan_query_str] = curr_bound_factor\n",
    "            res = max(res, 1)\n",
    "            if debug:\n",
    "                if true_card[i] == -1:\n",
    "                    error = \"NA\"\n",
    "                else:\n",
    "                    error = max(res / true_card[i], true_card[i] / res)\n",
    "                #print(f\"{left_tables}, {right_tables}|| estimate: {res}, true: {true_card[i]}, error: {error}\")\n",
    "            cardinality_bounds.append(res)\n",
    "        return cardinality_bounds\n",
    "\n",
    "    def join_with_one_table(self, sub_plan_query_str, left_table, tables_all, right_bound_factor, cond_factor_left,\n",
    "                            table_equivalent_group, table_key_equivalent_group, table_key_group_map, join_cond):\n",
    "        \"\"\"\n",
    "        Get the cardinality bound by joining the left_table with the seen right_tables\n",
    "        :param left_table:\n",
    "        :param right_tables:\n",
    "        \"\"\"\n",
    "        equivalent_key_group, union_key_group_set, union_key_group, new_join_cond = \\\n",
    "            self.get_join_keys_with_table_group(left_table, right_bound_factor, tables_all, table_equivalent_group,\n",
    "                                                table_key_equivalent_group, table_key_group_map, join_cond)\n",
    "        bin_mode_left = self.table_buckets[tables_all[left_table]].oned_bin_modes\n",
    "        bin_mode_right = right_bound_factor.bin_modes\n",
    "        key_group_pdf = dict()\n",
    "        key_group_bin_mode = dict()\n",
    "        new_union_key_group = dict()\n",
    "        new_na_values = dict()\n",
    "        right_variables = right_bound_factor.variables\n",
    "        new_variables = copy.deepcopy(right_variables)\n",
    "        res = right_bound_factor.tables_size\n",
    "        #print(\"\\n\")\n",
    "        #print(union_key_group_set)\n",
    "        #print(union_key_group)\n",
    "        for key_group in equivalent_key_group:\n",
    "            #print(key_group, equivalent_key_group[key_group], res)\n",
    "            # print(cond_factor_left.na_values)\n",
    "            # print(right_bound_factor.na_values)\n",
    "            #print(cond_factor_left.pdfs.keys())\n",
    "            #print(right_bound_factor.pdfs.keys())\n",
    "            all_pdfs = [cond_factor_left.pdfs[key] * cond_factor_left.table_len * cond_factor_left.na_values[key]\n",
    "                        for key in equivalent_key_group[key_group][\"left\"]]\n",
    "            all_bin_modes = [bin_mode_left[key] for key in equivalent_key_group[key_group][\"left\"]]\n",
    "            for key in equivalent_key_group[key_group][\"left\"]:\n",
    "                new_variables[key] = key_group\n",
    "            for key in equivalent_key_group[key_group][\"right\"]:\n",
    "                if key in right_bound_factor.pdfs:\n",
    "                    new_variables[key] = key_group\n",
    "                    all_pdfs.append(right_bound_factor.pdfs[key] * res * right_bound_factor.na_values[key])\n",
    "                    all_bin_modes.append(bin_mode_right[key])\n",
    "                else:\n",
    "                    key = right_variables[key]\n",
    "                    all_pdfs.append(right_bound_factor.pdfs[key] * res * right_bound_factor.na_values[key])\n",
    "                    all_bin_modes.append(bin_mode_right[key])\n",
    "\n",
    "            new_pdf, new_bin_mode = self.compute_bound_oned(all_pdfs, all_bin_modes, return_factor=True)\n",
    "            res = np.sum(new_pdf)\n",
    "            if res == 0:\n",
    "                res = 10.0\n",
    "                new_pdf[-1] = 1\n",
    "                key_group_pdf[key_group] = new_pdf\n",
    "            else:\n",
    "                key_group_pdf[key_group] = new_pdf / res\n",
    "            key_group_bin_mode[key_group] = new_bin_mode\n",
    "            new_union_key_group[key_group] = [key_group]\n",
    "            new_na_values[key_group] = 1\n",
    "        \n",
    "        for group in union_key_group:\n",
    "            if group not in new_union_key_group:\n",
    "                new_union_key_group[group] = []\n",
    "            for table, keys in union_key_group[group]:\n",
    "                for key in keys:\n",
    "                    new_union_key_group[group].append(key)\n",
    "                    if table == \"left\":\n",
    "                        key_group_pdf[key] = cond_factor_left.pdfs[key]\n",
    "                        key_group_bin_mode[key] = self.table_buckets[tables_all[left_table]].oned_bin_modes[key]\n",
    "                        new_na_values[key] = cond_factor_left.na_values[key]\n",
    "                    else:\n",
    "                        key_group_pdf[key] = right_bound_factor.pdfs[key]\n",
    "                        key_group_bin_mode[key] = right_bound_factor.bin_modes[key]\n",
    "                        new_na_values[key] = right_bound_factor.na_values[key]\n",
    "        \n",
    "        #print(\"****\", key_group_pdf.keys())\n",
    "        new_factor = Group_Factor(sub_plan_query_str, res, new_variables, key_group_pdf, key_group_bin_mode,\n",
    "                                  union_key_group_set, new_union_key_group, new_na_values, new_join_cond)\n",
    "        return new_factor, res\n",
    "\n",
    "    def get_join_keys_with_table_group(self, left_table, right_bound_factor, tables_all, table_equivalent_group,\n",
    "                                       table_key_equivalent_group, table_key_group_map, join_cond):\n",
    "        \"\"\"\n",
    "            Get the join keys between two tables\n",
    "        \"\"\"\n",
    "\n",
    "        actual_join_cond = []\n",
    "        for cond in join_cond[left_table]:\n",
    "            if cond in right_bound_factor.join_cond:\n",
    "                actual_join_cond.append(cond)\n",
    "        #print(join_cond[left_table], right_bound_factor.join_cond)\n",
    "        #print(actual_join_cond)\n",
    "        equivalent_key_group = dict()\n",
    "        union_key_group_set = table_equivalent_group[left_table].union(right_bound_factor.equivalent_groups)\n",
    "        union_key_group = dict()\n",
    "        new_join_cond = right_bound_factor.join_cond.union(join_cond[left_table])\n",
    "        if len(actual_join_cond) != 0:\n",
    "            for cond in actual_join_cond:\n",
    "                key1 = cond.split(\"=\")[0].strip()\n",
    "                key2 = cond.split(\"=\")[1].strip()\n",
    "                if key1.split(\".\")[0] == left_table:\n",
    "                    key_left = tables_all[left_table] + \".\" + key1.split(\".\")[-1]\n",
    "                    key_group = table_key_group_map[left_table][key_left]\n",
    "                    if key_group not in equivalent_key_group:\n",
    "                        equivalent_key_group[key_group] = dict()\n",
    "                    if left_table in equivalent_key_group[key_group]:\n",
    "                        equivalent_key_group[key_group][\"left\"].append(key_left)\n",
    "                    else:\n",
    "                        equivalent_key_group[key_group][\"left\"] = [key_left]\n",
    "                    right_table = key2.split(\".\")[0]\n",
    "                    key_right = tables_all[right_table] + \".\" + key2.split(\".\")[-1]\n",
    "                    key_group_t = table_key_group_map[right_table][key_right]\n",
    "                    assert key_group_t == key_group, f\"key group mismatch for join {cond}\"\n",
    "                    if \"right\" in equivalent_key_group[key_group]:\n",
    "                        equivalent_key_group[key_group][\"right\"].append(key_right)\n",
    "                    else:\n",
    "                        equivalent_key_group[key_group][\"right\"] = [key_right]\n",
    "                else:\n",
    "                    assert key2.split(\".\")[0] == left_table, f\"unrecognized table alias\"\n",
    "                    key_left = tables_all[left_table] + \".\" + key2.split(\".\")[-1]\n",
    "                    key_group = table_key_group_map[left_table][key_left]\n",
    "                    if key_group not in equivalent_key_group:\n",
    "                        equivalent_key_group[key_group] = dict()\n",
    "                    if left_table in equivalent_key_group[key_group]:\n",
    "                        equivalent_key_group[key_group][\"left\"].append(key_left)\n",
    "                    else:\n",
    "                        equivalent_key_group[key_group][\"left\"] = [key_left]\n",
    "                    right_table = key1.split(\".\")[0]\n",
    "                    key_right = tables_all[right_table] + \".\" + key1.split(\".\")[-1]\n",
    "                    key_group_t = table_key_group_map[right_table][key_right]\n",
    "                    assert key_group_t == key_group, f\"key group mismatch for join {cond}\"\n",
    "                    if \"right\" in equivalent_key_group[key_group]:\n",
    "                        equivalent_key_group[key_group][\"right\"].append(key_right)\n",
    "                    else:\n",
    "                        equivalent_key_group[key_group][\"right\"] = [key_right]\n",
    "            \n",
    "            for group in union_key_group_set:\n",
    "                if group in equivalent_key_group:\n",
    "                    new_left_key = []\n",
    "                    for key in table_key_equivalent_group[left_table][group]:\n",
    "                        if key not in equivalent_key_group[group][\"left\"]:\n",
    "                            new_left_key.append(key)\n",
    "                    if len(new_left_key) != 0:\n",
    "                        union_key_group[group] = [(\"left\", new_left_key)]\n",
    "                    new_right_key = []\n",
    "                    for key in right_bound_factor.table_key_equivalent_group[group]:\n",
    "                        if key not in equivalent_key_group[group][\"right\"]:\n",
    "                            new_right_key.append(key)\n",
    "                    if len(new_right_key) != 0:\n",
    "                        if group in union_key_group:\n",
    "                            union_key_group[group].append((\"right\", new_right_key))\n",
    "                        else:\n",
    "                            union_key_group[group] = [(\"right\", new_right_key)]\n",
    "                else:\n",
    "                    if group in table_key_equivalent_group[left_table]:\n",
    "                        if group in union_key_group:\n",
    "                            union_key_group[group].append((\"left\", table_key_equivalent_group[left_table][group]))\n",
    "                        else:\n",
    "                            union_key_group[group] = [(\"left\", table_key_equivalent_group[left_table][group])]\n",
    "                    if group in right_bound_factor.table_key_equivalent_group:\n",
    "                        if group in union_key_group:\n",
    "                            union_key_group[group].append((\"right\", right_bound_factor.table_key_equivalent_group[group]))\n",
    "                        else:\n",
    "                            union_key_group[group] = [(\"right\", right_bound_factor.table_key_equivalent_group[group])]\n",
    "                        \n",
    "        else:\n",
    "            common_key_group = table_equivalent_group[left_table].intersection(right_bound_factor.equivalent_groups)\n",
    "            common_key_group = list(common_key_group)[0]\n",
    "            for group in union_key_group_set:\n",
    "                if group == common_key_group:\n",
    "                    equivalent_key_group[group] = dict()\n",
    "                    equivalent_key_group[group][\"left\"] = table_key_equivalent_group[left_table][group]\n",
    "                    equivalent_key_group[group][\"right\"] = right_bound_factor.table_key_equivalent_group[group]\n",
    "                else:\n",
    "                    if group in table_key_equivalent_group[left_table]:\n",
    "                        if group in union_key_group:\n",
    "                            union_key_group[group].append((\"left\", table_key_equivalent_group[left_table][group]))\n",
    "                        else:\n",
    "                            union_key_group[group] = [(\"left\", table_key_equivalent_group[left_table][group])]\n",
    "                    if group in right_bound_factor.table_key_equivalent_group:\n",
    "                        if group in union_key_group:\n",
    "                            union_key_group[group].append((\"right\", right_bound_factor.table_key_equivalent_group[group]))\n",
    "                        else:\n",
    "                            union_key_group[group] = [(\"right\", right_bound_factor.table_key_equivalent_group[group])]\n",
    "\n",
    "        return equivalent_key_group, union_key_group_set, union_key_group, new_join_cond\n",
    "\n",
    "    def get_additional_join_with_table_group(self, left_table, right_union_key_group, table_equivalent_group,\n",
    "                                             table_key_equivalent_group):\n",
    "        common_key_group = table_equivalent_group[left_table].intersection(set(right_union_key_group.keys()))\n",
    "        union_key_group_set = table_equivalent_group[left_table].union(set(right_union_key_group.keys()))\n",
    "        union_key_group = copy.deepcopy(right_union_key_group)\n",
    "        all_join_predicates = []\n",
    "        for group in union_key_group_set:\n",
    "            if group in common_key_group:\n",
    "                left_key = table_key_equivalent_group[left_table][group][0]\n",
    "                left_key = left_table + \".\" + left_key.split(\".\")[-1]\n",
    "                right_key = right_union_key_group[group]\n",
    "                join_predicate = left_key + \" = \" + right_key\n",
    "                all_join_predicates.append(join_predicate)\n",
    "            if group not in union_key_group:\n",
    "                assert group in table_key_equivalent_group[left_table]\n",
    "                left_key = table_key_equivalent_group[left_table][group][0]\n",
    "                left_key = left_table + \".\" + left_key.split(\".\")[-1]\n",
    "                union_key_group[group] = left_key\n",
    "\n",
    "        return all_join_predicates, union_key_group\n",
    "\n",
    "    def join_two_tables(self, sub_plan_query_str, left_table, right_table, tables_all, conditional_factors, join_keys,\n",
    "                        table_equivalent_group, table_key_equivalent_group, table_key_group_map, join_cond):\n",
    "        \"\"\"\n",
    "            Get the cardinality bound by joining the left_table with the right_table\n",
    "            :param left_table:\n",
    "            :param right_table:\n",
    "        \"\"\"\n",
    "        equivalent_key_group, union_key_group_set, union_key_group, new_join_cond = \\\n",
    "            self.get_join_keys_two_tables(left_table, right_table, table_equivalent_group, table_key_equivalent_group,\n",
    "                                          table_key_group_map, join_cond, join_keys, tables_all)\n",
    "        # print(left_table, right_table)\n",
    "        # print(equivalent_key_group)\n",
    "        # print(union_key_group)\n",
    "        # print(conditional_factors.keys())\n",
    "        cond_factor_left = conditional_factors[left_table]\n",
    "        cond_factor_right = conditional_factors[right_table]\n",
    "        bin_mode_left = self.table_buckets[tables_all[left_table]].oned_bin_modes\n",
    "        bin_mode_right = self.table_buckets[tables_all[right_table]].oned_bin_modes\n",
    "        key_group_pdf = dict()\n",
    "        key_group_bin_mode = dict()\n",
    "        new_union_key_group = dict()\n",
    "        res = cond_factor_right.table_len\n",
    "        new_na_values = dict()\n",
    "        new_variables = dict()\n",
    "        # print(equivalent_key_group)\n",
    "        for key_group in equivalent_key_group:\n",
    "            # print(key_group)\n",
    "            # print(\"========================\")\n",
    "            #print(equivalent_key_group[key_group][left_table])\n",
    "            #print(bin_mode_left)\n",
    "            # print(\"==========================================\")\n",
    "            #print(equivalent_key_group[key_group][right_table])\n",
    "            #print(bin_mode_right)\n",
    "            if len(equivalent_key_group[key_group][left_table]) > 1:\n",
    "                print(len(equivalent_key_group[key_group][left_table]), sub_plan_query_str)\n",
    "            if len(equivalent_key_group[key_group][right_table]) > 1:\n",
    "                print(len(equivalent_key_group[key_group][right_table]), sub_plan_query_str)\n",
    "            all_pdfs = [cond_factor_left.pdfs[key] * cond_factor_left.table_len * cond_factor_left.na_values[key]\n",
    "                        for key in equivalent_key_group[key_group][left_table]] + \\\n",
    "                       [cond_factor_right.pdfs[key] * res * cond_factor_right.na_values[key]\n",
    "                        for key in equivalent_key_group[key_group][right_table]]\n",
    "            all_bin_modes = [bin_mode_left[key] for key in equivalent_key_group[key_group][left_table]] + \\\n",
    "                            [bin_mode_right[key] for key in equivalent_key_group[key_group][right_table]]\n",
    "            # print(\"====================================================\")\n",
    "            # print(equivalent_key_group[key_group][left_table])\n",
    "            # print(equivalent_key_group[key_group][right_table])\n",
    "            for key in equivalent_key_group[key_group][left_table] + equivalent_key_group[key_group][right_table]:\n",
    "                new_variables[key] = key_group\n",
    "            new_pdf, new_bin_mode = self.compute_bound_oned(all_pdfs, all_bin_modes, return_factor=True)\n",
    "            res = np.sum(new_pdf)\n",
    "            key_group_pdf[key_group] = new_pdf / res\n",
    "            key_group_bin_mode[key_group] = new_bin_mode\n",
    "            new_union_key_group[key_group] = [key_group]\n",
    "            new_na_values[key_group] = 1.0\n",
    "\n",
    "        for group in union_key_group:\n",
    "            if group not in new_union_key_group:\n",
    "                new_union_key_group[group] = []\n",
    "            for table, keys in union_key_group[group]:\n",
    "                for key in keys:\n",
    "                    new_union_key_group[group].append(key)\n",
    "                    key_group_pdf[key] = conditional_factors[table].pdfs[key]\n",
    "                    key_group_bin_mode[key] = self.table_buckets[tables_all[table]].oned_bin_modes[key]\n",
    "                    new_na_values[key] = conditional_factors[table].na_values[key]\n",
    "        \n",
    "        #print(\"!!!!!!!\", union_key_group)\n",
    "        #print(new_union_key_group)\n",
    "        new_factor = Group_Factor(sub_plan_query_str, res, new_variables, key_group_pdf, key_group_bin_mode,\n",
    "                                  union_key_group_set, new_union_key_group, new_na_values, new_join_cond)\n",
    "        return new_factor, res\n",
    "\n",
    "    def get_join_keys_two_tables(self, left_table, right_table, table_equivalent_group, table_key_equivalent_group,\n",
    "                                 table_key_group_map, join_cond, join_keys, tables_all):\n",
    "        \"\"\"\n",
    "            Get the join keys between two tables\n",
    "        \"\"\"\n",
    "        actual_join_cond = []\n",
    "        for cond in join_cond[left_table]:\n",
    "            if cond in join_cond[right_table]:\n",
    "                actual_join_cond.append(cond)\n",
    "        equivalent_key_group = dict()\n",
    "        union_key_group_set = table_equivalent_group[left_table].union(table_equivalent_group[right_table])\n",
    "        union_key_group = dict()\n",
    "        new_join_cond = join_cond[left_table].union(join_cond[right_table])\n",
    "        if len(actual_join_cond) != 0:\n",
    "            for cond in actual_join_cond:\n",
    "                key1 = cond.split(\"=\")[0].strip()\n",
    "                key2 = cond.split(\"=\")[1].strip()\n",
    "                if key1.split(\".\")[0] == left_table:\n",
    "                    key_left = tables_all[left_table] + \".\" + key1.split(\".\")[-1]\n",
    "                    key_group = table_key_group_map[left_table][key_left]\n",
    "                    if key_group not in equivalent_key_group:\n",
    "                        equivalent_key_group[key_group] = dict()\n",
    "                    if left_table in equivalent_key_group[key_group]:\n",
    "                        equivalent_key_group[key_group][left_table].append(key_left)\n",
    "                    else:\n",
    "                        equivalent_key_group[key_group][left_table] = [key_left]\n",
    "                    assert key2.split(\".\")[0] == right_table, f\"unrecognized table alias\"\n",
    "                    key_right = tables_all[right_table] + \".\" + key2.split(\".\")[-1]\n",
    "                    key_group_t = table_key_group_map[right_table][key_right]\n",
    "                    assert key_group_t == key_group, f\"key group mismatch for join {cond}\"\n",
    "                    if right_table in equivalent_key_group[key_group]:\n",
    "                        equivalent_key_group[key_group][right_table].append(key_right)\n",
    "                    else:\n",
    "                        equivalent_key_group[key_group][right_table] = [key_right]\n",
    "                else:\n",
    "                    assert key2.split(\".\")[0] == left_table, f\"unrecognized table alias\"\n",
    "                    key_left = tables_all[left_table] + \".\" + key2.split(\".\")[-1]\n",
    "                    key_group = table_key_group_map[left_table][key_left]\n",
    "                    if key_group not in equivalent_key_group:\n",
    "                        equivalent_key_group[key_group] = dict()\n",
    "                    if left_table in equivalent_key_group[key_group]:\n",
    "                        equivalent_key_group[key_group][left_table].append(key_left)\n",
    "                    else:\n",
    "                        equivalent_key_group[key_group][left_table] = [key_left]\n",
    "                    assert key1.split(\".\")[0] == right_table, f\"unrecognized table alias\"\n",
    "                    key_right = tables_all[right_table] + \".\" + key1.split(\".\")[-1]\n",
    "                    key_group_t = table_key_group_map[right_table][key_right]\n",
    "                    assert key_group_t == key_group, f\"key group mismatch for join {cond}\"\n",
    "                    if right_table in equivalent_key_group[key_group]:\n",
    "                        equivalent_key_group[key_group][right_table].append(key_right)\n",
    "                    else:\n",
    "                        equivalent_key_group[key_group][right_table] = [key_right]\n",
    "\n",
    "            for group in union_key_group_set:\n",
    "                if group in equivalent_key_group:\n",
    "                    new_left_key = []\n",
    "                    for key in table_key_equivalent_group[left_table][group]:\n",
    "                        if key not in equivalent_key_group[group][left_table]:\n",
    "                            new_left_key.append(key)\n",
    "                    if len(new_left_key) != 0:\n",
    "                        union_key_group[group] = [(left_table, new_left_key)]\n",
    "                    new_right_key = []\n",
    "                    for key in table_key_equivalent_group[right_table][group]:\n",
    "                        if key not in equivalent_key_group[group][right_table]:\n",
    "                            new_right_key.append(key)\n",
    "                    if len(new_right_key) != 0:\n",
    "                        if group in union_key_group:\n",
    "                            union_key_group[group].append((right_table, new_right_key))\n",
    "                        else:\n",
    "                            union_key_group[group] = [(right_table, new_right_key)]\n",
    "                else:\n",
    "                    if group in table_key_equivalent_group[left_table]:\n",
    "                        if group in union_key_group:\n",
    "                            union_key_group[group].append((left_table, table_key_equivalent_group[left_table][group]))\n",
    "                        else:\n",
    "                            union_key_group[group] = [(left_table, table_key_equivalent_group[left_table][group])]\n",
    "                    if group in table_key_equivalent_group[right_table]:\n",
    "                        if group in union_key_group:\n",
    "                            union_key_group[group].append((right_table, table_key_equivalent_group[right_table][group]))\n",
    "                        else:\n",
    "                            union_key_group[group] = [(right_table, table_key_equivalent_group[right_table][group])]\n",
    "\n",
    "        else:\n",
    "            common_key_group = table_equivalent_group[left_table].intersection(table_equivalent_group[right_table])\n",
    "            common_key_group = list(common_key_group)[0]\n",
    "            for group in union_key_group_set:\n",
    "                if group == common_key_group:\n",
    "                    equivalent_key_group[group] = dict()\n",
    "                    equivalent_key_group[group][left_table] = table_key_equivalent_group[left_table][group]\n",
    "                    equivalent_key_group[group][right_table] = table_key_equivalent_group[right_table][group]\n",
    "                elif group in table_key_equivalent_group[left_table]:\n",
    "                    if group in union_key_group:\n",
    "                        union_key_group[group].append((left_table, table_key_equivalent_group[left_table][group]))\n",
    "                    else:\n",
    "                        union_key_group[group] = [(left_table, table_key_equivalent_group[left_table][group])]\n",
    "                else:\n",
    "                    if group in union_key_group:\n",
    "                        union_key_group[group].append((right_table, table_key_equivalent_group[right_table][group]))\n",
    "                    else:\n",
    "                        union_key_group[group] = [(right_table, table_key_equivalent_group[right_table][group])]\n",
    "\n",
    "        return equivalent_key_group, union_key_group_set, union_key_group, new_join_cond\n",
    "\n",
    "    def get_additional_joins_two_tables(self, left_table, right_table, table_equivalent_group,\n",
    "                                        table_key_equivalent_group):\n",
    "        common_key_group = table_equivalent_group[left_table].intersection(table_equivalent_group[right_table])\n",
    "        union_key_group_set = table_equivalent_group[left_table].union(table_equivalent_group[right_table])\n",
    "        union_key_group = dict()\n",
    "        all_join_predicates = []\n",
    "        for group in union_key_group_set:\n",
    "            if group in common_key_group:\n",
    "                left_key = table_key_equivalent_group[left_table][group][0]\n",
    "                left_key = left_table + \".\" + left_key.split(\".\")[-1]\n",
    "                right_key = table_key_equivalent_group[right_table][group][0]\n",
    "                right_key = right_table + \".\" + right_key.split(\".\")[-1]\n",
    "                join_predicate = left_key + \" = \" + right_key\n",
    "                all_join_predicates.append(join_predicate)\n",
    "            if group in table_key_equivalent_group[left_table]:\n",
    "                left_key = table_key_equivalent_group[left_table][group][0]\n",
    "                left_key = left_table + \".\" + left_key.split(\".\")[-1]\n",
    "                union_key_group[group] = left_key\n",
    "            else:\n",
    "                right_key = table_key_equivalent_group[right_table][group][0]\n",
    "                right_key = right_table + \".\" + right_key.split(\".\")[-1]\n",
    "                union_key_group[group] = right_key\n",
    "\n",
    "        return all_join_predicates, union_key_group\n",
    "\n",
    "    def get_sub_plan_join_key(self, sub_plan_query, join_cond):\n",
    "        # returning a subset of join_keys covered by the tables in sub_plan_query\n",
    "        touched_join_cond = set()\n",
    "        untouched_join_cond = set()\n",
    "        for tab in join_cond:\n",
    "            if tab in sub_plan_query:\n",
    "                touched_join_cond = touched_join_cond.union(join_cond[tab])\n",
    "            else:\n",
    "                untouched_join_cond = untouched_join_cond.union(join_cond[tab])\n",
    "        touched_join_cond -= untouched_join_cond\n",
    "\n",
    "        join_keys = dict()\n",
    "        for cond in touched_join_cond:\n",
    "            key1 = cond.split(\"=\")[0].strip()\n",
    "            table1 = key1.split(\".\")[0].strip()\n",
    "            if table1 not in join_keys:\n",
    "                join_keys[table1] = set([key1])\n",
    "            else:\n",
    "                join_keys[table1].add(key1)\n",
    "\n",
    "            key2 = cond.split(\"=\")[1].strip()\n",
    "            table2 = key2.split(\".\")[0].strip()\n",
    "            if table2 not in join_keys:\n",
    "                join_keys[table2] = set([key2])\n",
    "            else:\n",
    "                join_keys[table2].add(key2)\n",
    "\n",
    "        return join_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "336bbda5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models save at /home/ubuntu/data_CE/CE_scheme_models/model_imdb_default.pkl\n"
     ]
    }
   ],
   "source": [
    "be = Bound_ensemble(table_buckets, schema, ground_truth_factors_no_filter)\n",
    "model_path = model_folder + f\"model_imdb_default.pkl\"\n",
    "pickle.dump(be, open(model_path, 'wb'), pickle.HIGHEST_PROTOCOL)\n",
    "print(f\"models save at {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c24d7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_path = \"/home/ubuntu/data_CE/job/\"\n",
    "queries = []\n",
    "q_file_names = []\n",
    "for query_no in range(1, 34):\n",
    "    for suffix in ['a', 'b', 'c', 'd', 'e', 'f', 'g']:\n",
    "        file = f\"{query_no}{suffix}.sql\"\n",
    "        if file in os.listdir(query_path):\n",
    "            q_file_names.append(file.split(\".sql\")[0])\n",
    "            with open(query_path+file, \"r\") as f:\n",
    "                q = f.readline()\n",
    "                queries.append(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5fc8f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_key_values(schema):\n",
    "    \"\"\"\n",
    "    identify all the key attributes from the schema of a DB, currently we assume all possible joins are known\n",
    "    It is also easy to support unseen joins, which we left as a future work.\n",
    "    :param schema: the schema of a DB\n",
    "    :return: a dict of all keys, {table: [keys]};\n",
    "             a dict of set, each indicating which keys on different tables are considered the same key.\n",
    "    \"\"\"\n",
    "    all_keys = set()\n",
    "    equivalent_keys = dict()\n",
    "    for i, join in enumerate(schema.relationships):\n",
    "        keys = join.identifier.split(\" = \")\n",
    "        all_keys.add(keys[0])\n",
    "        all_keys.add(keys[1])\n",
    "        seen = False\n",
    "        for k in equivalent_keys:\n",
    "            if keys[0] in equivalent_keys[k]:\n",
    "                equivalent_keys[k].add(keys[1])\n",
    "                seen = True\n",
    "                break\n",
    "            elif keys[1] in equivalent_keys[k]:\n",
    "                equivalent_keys[k].add(keys[0])\n",
    "                seen = True\n",
    "                break\n",
    "        if not seen:\n",
    "            # set the keys[-1] as the identifier of this equivalent join key group for convenience.\n",
    "            equivalent_keys[keys[-1]] = set(keys)\n",
    "\n",
    "    assert len(all_keys) == sum([len(equivalent_keys[k]) for k in equivalent_keys])\n",
    "    return all_keys, equivalent_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10939d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'kind_type.id': {'aka_title.kind_id', 'kind_type.id', 'title.kind_id'}, 'info_type.id': {'movie_info_idx.info_type_id', 'info_type.id', 'movie_info.info_type_id', 'person_info.info_type_id'}, 'title.id': {'title.id', 'movie_keyword.movie_id', 'movie_companies.movie_id', 'cast_info.movie_id', 'complete_cast.movie_id', 'aka_title.movie_id', 'movie_link.movie_id', 'movie_info.movie_id', 'movie_info_idx.movie_id', 'movie_link.linked_movie_id'}, 'name.id': {'name.id', 'aka_name.person_id', 'cast_info.person_id', 'person_info.person_id'}, 'char_name.id': {'cast_info.person_role_id', 'char_name.id'}, 'role_type.id': {'cast_info.role_id', 'role_type.id'}, 'comp_cast_type.id': {'comp_cast_type.id', 'complete_cast.subject_id', 'complete_cast.status_id'}, 'link_type.id': {'link_type.id', 'movie_link.link_type_id'}, 'keyword.id': {'keyword.id', 'movie_keyword.keyword_id'}, 'company_name.id': {'company_name.id', 'movie_companies.company_id'}, 'company_type.id': {'company_type.id', 'movie_companies.company_type_id'}}\n",
      "11\n",
      "36\n"
     ]
    }
   ],
   "source": [
    "all_keys, equivalent_keys = identify_key_values(schema)\n",
    "print(equivalent_keys)\n",
    "print(len(equivalent_keys))\n",
    "print(len(all_keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe3f6875",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_join_key_appearance(queries, equivalent_keys):\n",
    "    \"\"\"\n",
    "    analyze the workload and count how many times each join key group appears\n",
    "    \"\"\"\n",
    "    all_join_keys_stats = dict()\n",
    "    total_num_appearance = 0\n",
    "    for q in queries:\n",
    "        res = parse_query_all_join(q)\n",
    "        for table in res[-1]:\n",
    "            for join_key in list(res[-1][table]):\n",
    "                for PK in equivalent_keys:\n",
    "                    if join_key in equivalent_keys[PK]:\n",
    "                        total_num_appearance += 1\n",
    "                        if PK in all_join_keys_stats:\n",
    "                            all_join_keys_stats[PK] += 1\n",
    "                        else:\n",
    "                            all_join_keys_stats[PK] = 1\n",
    "                        break\n",
    "    return all_join_keys_stats, total_num_appearance\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ad7fc57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'company_type.id': 82,\n",
       " 'title.id': 481,\n",
       " 'info_type.id': 198,\n",
       " 'company_name.id': 148,\n",
       " 'keyword.id': 150,\n",
       " 'name.id': 138,\n",
       " 'link_type.id': 36,\n",
       " 'role_type.id': 40,\n",
       " 'char_name.id': 44,\n",
       " 'kind_type.id': 58,\n",
       " 'comp_cast_type.id': 78}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_join_keys_stats, total_num_appearance = count_join_key_appearance(queries, equivalent_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "06d8546e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bins = {\n",
    "    'title.id': 800,\n",
    "    'info_type.id': 100,\n",
    "    'keyword.id': 100,\n",
    "    'company_name.id': 100,\n",
    "    'name.id': 100,\n",
    "    'company_type.id': 100,\n",
    "    'comp_cast_type.id': 50,\n",
    "    'kind_type.id': 50,\n",
    "    'char_name.id': 50,\n",
    "    'role_type.id': 50,\n",
    "    'link_type.id': 50\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d65952d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sample(np_data, nrows=1000000, seed=0):\n",
    "    np.random.seed(seed)\n",
    "    samp_data = np_data[np_data != -1]\n",
    "    if len(samp_data) <= nrows:\n",
    "        return samp_data, 1.0\n",
    "    else:\n",
    "        selected = np.random.choice(len(samp_data), size=nrows, replace=False)\n",
    "        return samp_data[selected], nrows/len(samp_data)\n",
    "\n",
    "def stats_analysis(sample, data, sample_rate, show=10):\n",
    "    n, c = np.unique(sample, return_counts=True)\n",
    "    idx = np.argsort(c)[::-1]\n",
    "    for i in range(min(show, len(idx))):\n",
    "        print(c[idx[i]], c[idx[i]]/sample_rate, len(data[data == n[idx[i]]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8b86b246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title.id\n",
      "0\n",
      "2528312 1.0\n",
      "0\n",
      "title.kind_id\n",
      "0\n",
      "2528312 1.0\n",
      "0\n",
      "movie_info_idx.movie_id\n",
      "0\n",
      "1380035 1.0\n",
      "0\n",
      "movie_info_idx.info_type_id\n",
      "0\n",
      "1380035 1.0\n",
      "0\n",
      "movie_info.movie_id\n",
      "0\n",
      "14835720 1.0\n",
      "0\n",
      "movie_info.info_type_id\n",
      "0\n",
      "14835720 1.0\n",
      "0\n",
      "info_type.id\n",
      "0\n",
      "113 1.0\n",
      "0\n",
      "cast_info.person_id\n",
      "0\n",
      "36244344 1.0\n",
      "0\n",
      "cast_info.movie_id\n",
      "0\n",
      "36244344 1.0\n",
      "0\n",
      "cast_info.person_role_id\n",
      "18672825\n",
      "36244344 0.48480720191818066\n",
      "0\n",
      "cast_info.role_id\n",
      "0\n",
      "36244344 1.0\n",
      "0\n",
      "char_name.id\n",
      "0\n",
      "3140339 1.0\n",
      "0\n",
      "role_type.id\n",
      "0\n",
      "12 1.0\n",
      "0\n",
      "complete_cast.movie_id\n",
      "0\n",
      "135086 1.0\n",
      "0\n",
      "complete_cast.subject_id\n",
      "0\n",
      "135086 1.0\n",
      "0\n",
      "complete_cast.status_id\n",
      "0\n",
      "135086 1.0\n",
      "0\n",
      "comp_cast_type.id\n",
      "0\n",
      "4 1.0\n",
      "0\n",
      "name.id\n",
      "0\n",
      "4167491 1.0\n",
      "0\n",
      "aka_name.person_id\n",
      "0\n",
      "901343 1.0\n",
      "0\n",
      "movie_link.movie_id\n",
      "0\n",
      "29997 1.0\n",
      "0\n",
      "movie_link.linked_movie_id\n",
      "0\n",
      "29997 1.0\n",
      "0\n",
      "movie_link.link_type_id\n",
      "0\n",
      "29997 1.0\n",
      "0\n",
      "link_type.id\n",
      "0\n",
      "18 1.0\n",
      "0\n",
      "movie_keyword.movie_id\n",
      "0\n",
      "4523930 1.0\n",
      "0\n",
      "movie_keyword.keyword_id\n",
      "0\n",
      "4523930 1.0\n",
      "0\n",
      "keyword.id\n",
      "0\n",
      "134170 1.0\n",
      "0\n",
      "person_info.person_id\n",
      "0\n",
      "2963664 1.0\n",
      "0\n",
      "person_info.info_type_id\n",
      "0\n",
      "2963664 1.0\n",
      "0\n",
      "movie_companies.movie_id\n",
      "0\n",
      "2609129 1.0\n",
      "0\n",
      "movie_companies.company_id\n",
      "0\n",
      "2609129 1.0\n",
      "0\n",
      "movie_companies.company_type_id\n",
      "0\n",
      "2609129 1.0\n",
      "0\n",
      "company_name.id\n",
      "0\n",
      "234997 1.0\n",
      "0\n",
      "company_type.id\n",
      "0\n",
      "4 1.0\n",
      "0\n",
      "aka_title.movie_id\n",
      "0\n",
      "361472 1.0\n",
      "0\n",
      "aka_title.kind_id\n",
      "0\n",
      "361472 1.0\n",
      "0\n",
      "kind_type.id\n",
      "0\n",
      "7 1.0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "data = dict()\n",
    "table_len = dict()\n",
    "na_values = dict()\n",
    "sample_rate = dict()\n",
    "primary_keys = []\n",
    "for table_obj in schema.tables:\n",
    "    df_rows = pd.read_csv(table_obj.csv_file_location, header=None, escapechar='\\\\', encoding='utf-8', quotechar='\"',\n",
    "                          sep=\",\")\n",
    "    \n",
    "    df_rows.columns = [table_obj.table_name + '.' + attr for attr in table_obj.attributes]\n",
    "\n",
    "    for attribute in table_obj.irrelevant_attributes:\n",
    "        df_rows = df_rows.drop(table_obj.table_name + '.' + attribute, axis=1)\n",
    "\n",
    "    df_rows.apply(pd.to_numeric, errors=\"ignore\")\n",
    "    table_len[table_obj.table_name] = len(df_rows)\n",
    "    if table_obj.table_name not in na_values:\n",
    "        na_values[table_obj.table_name] = dict()\n",
    "    for attr in df_rows.columns:\n",
    "        if attr in all_keys:\n",
    "            print(attr)\n",
    "            print(np.sum(np.isnan(df_rows[attr].values)))\n",
    "            data[attr] = df_rows[attr].values\n",
    "            data[attr][np.isnan(data[attr])] = -1\n",
    "            data[attr][data[attr] < 0] = -1\n",
    "            na_values[table_obj.table_name][attr] = len(data[attr][data[attr] != -1])/table_len[table_obj.table_name]\n",
    "            print(len(data[attr]), na_values[table_obj.table_name][attr])\n",
    "            data[attr] = copy.deepcopy(data[attr])[data[attr]>=0]\n",
    "            if len(np.unique(data[attr])) >= len(data[attr]) - 10:\n",
    "                primary_keys.append(attr)\n",
    "            print(np.sum(np.isnan(df_rows[attr].values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1294f672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title.id\n",
      "1 2.528312 1\n",
      "1 2.528312 1\n",
      "1 2.528312 1\n",
      "1 2.528312 1\n",
      "1 2.528312 1\n",
      "1 2.528312 1\n",
      "1 2.528312 1\n",
      "1 2.528312 1\n",
      "1 2.528312 1\n",
      "1 2.528312 1\n",
      "title.kind_id\n",
      "609674 1541446.090288 1543264\n",
      "262591 663911.976392 662824\n",
      "46890 118552.54968 118234\n",
      "39888 100849.309056 100537\n",
      "35940 90867.53328 90852\n",
      "5016 12682.012992 12600\n",
      "1 2.528312 1\n",
      "movie_info_idx.movie_id\n",
      "4 5.52014 4\n",
      "4 5.52014 4\n",
      "4 5.52014 4\n",
      "4 5.52014 4\n",
      "4 5.52014 4\n",
      "4 5.52014 4\n",
      "4 5.52014 4\n",
      "4 5.52014 4\n",
      "4 5.52014 4\n",
      "4 5.52014 4\n",
      "movie_info_idx.info_type_id\n",
      "333438 460156.11033 459925\n",
      "333295 459958.765325 459925\n",
      "333076 459656.53766 459925\n",
      "185 255.30647499999998 250\n",
      "6 8.28021 10\n",
      "movie_info.movie_id\n",
      "194 2878.12968 2937\n",
      "118 1750.61496 1584\n",
      "109 1617.09348 1638\n",
      "97 1439.06484 1344\n",
      "92 1364.88624 1281\n",
      "89 1320.37908 1081\n",
      "83 1231.36476 1219\n",
      "82 1216.5290400000001 1160\n",
      "77 1142.35044 1189\n",
      "77 1142.35044 1207\n",
      "movie_info.info_type_id\n",
      "204500 3033904.74 3036719\n",
      "103748 1539176.27856 1533909\n",
      "94821 1406737.80612 1401902\n",
      "89252 1324117.68144 1325361\n",
      "87478 1297799.11416 1298989\n",
      "87209 1293808.30548 1288928\n",
      "54058 801989.35176 802140\n",
      "44377 658364.74644 660923\n",
      "40308 597998.20176 598457\n",
      "32495 482086.72140000004 486554\n",
      "info_type.id\n",
      "1 1.0 1\n",
      "1 1.0 1\n",
      "1 1.0 1\n",
      "1 1.0 1\n",
      "1 1.0 1\n",
      "1 1.0 1\n",
      "1 1.0 1\n",
      "1 1.0 1\n",
      "1 1.0 1\n",
      "1 1.0 1\n",
      "cast_info.person_id\n",
      "279 10112.171976 10249\n",
      "236 8553.665184 8242\n",
      "225 8154.9774 7725\n",
      "219 7937.511336 7684\n",
      "187 6777.692328 6034\n",
      "186 6741.447984 6912\n",
      "180 6523.98192 6243\n",
      "178 6451.493232 5980\n",
      "166 6016.561104 5310\n",
      "162 5871.583728 6516\n",
      "cast_info.movie_id\n",
      "54 1957.194576 1741\n",
      "48 1739.728512 1531\n",
      "37 1341.040728 1572\n",
      "35 1268.55204 728\n",
      "27 978.597288 1027\n",
      "25 906.1086 911\n",
      "24 869.864256 987\n",
      "23 833.619912 790\n",
      "22 797.375568 667\n",
      "21 761.131224 660\n",
      "cast_info.person_role_id\n",
      "75296 1323065.094624 1322872\n",
      "37827 664677.8492129999 662713\n",
      "16303 286468.47425699997 286435\n",
      "5099 89597.175381 90715\n",
      "3665 64399.617135 63110\n",
      "3571 62747.894348999995 62610\n",
      "2419 42505.504461 42872\n",
      "2278 40027.920282 40445\n",
      "2266 39817.062054 40768\n",
      "2239 39342.631041 37837\n",
      "cast_info.role_id\n",
      "350218 12693421.666992 12670688\n",
      "205047 7431794.004168 7451973\n",
      "118622 4299376.573968 4323018\n",
      "110600 4008624.4464 4008037\n",
      "75521 2737209.103224 2728943\n",
      "46984 1702904.258496 1703543\n",
      "30167 1093383.125448 1093558\n",
      "24993 905854.889592 898389\n",
      "21455 777622.40052 773674\n",
      "8693 315072.082392 316118\n",
      "char_name.id\n",
      "1 3.140339 1\n",
      "1 3.140339 1\n",
      "1 3.140339 1\n",
      "1 3.140339 1\n",
      "1 3.140339 1\n",
      "1 3.140339 1\n",
      "1 3.140339 1\n",
      "1 3.140339 1\n",
      "1 3.140339 1\n",
      "1 3.140339 1\n",
      "role_type.id\n",
      "1 1.0 1\n",
      "1 1.0 1\n",
      "1 1.0 1\n",
      "1 1.0 1\n",
      "1 1.0 1\n",
      "1 1.0 1\n",
      "1 1.0 1\n",
      "1 1.0 1\n",
      "1 1.0 1\n",
      "1 1.0 1\n",
      "complete_cast.movie_id\n",
      "2 2.0 2\n",
      "2 2.0 2\n",
      "2 2.0 2\n",
      "2 2.0 2\n",
      "2 2.0 2\n",
      "2 2.0 2\n",
      "2 2.0 2\n",
      "2 2.0 2\n",
      "2 2.0 2\n",
      "2 2.0 2\n",
      "complete_cast.subject_id\n",
      "85941 85941.0 85941\n",
      "49145 49145.0 49145\n",
      "complete_cast.status_id\n",
      "110494 110494.0 110494\n",
      "24592 24592.0 24592\n",
      "comp_cast_type.id\n",
      "1 1.0 1\n",
      "1 1.0 1\n",
      "1 1.0 1\n",
      "1 1.0 1\n",
      "name.id\n",
      "1 4.167491 1\n",
      "1 4.167491 1\n",
      "1 4.167491 1\n",
      "1 4.167491 1\n",
      "1 4.167491 1\n",
      "1 4.167491 1\n",
      "1 4.167491 1\n",
      "1 4.167491 1\n",
      "1 4.167491 1\n",
      "1 4.167491 1\n",
      "aka_name.person_id\n",
      "76 76.0 76\n",
      "72 72.0 72\n",
      "63 63.0 63\n",
      "51 51.0 51\n",
      "51 51.0 51\n",
      "41 41.0 41\n",
      "39 39.0 39\n",
      "37 37.0 37\n",
      "37 37.0 37\n",
      "36 36.0 36\n",
      "movie_link.movie_id\n",
      "569 569.0 569\n",
      "517 517.0 517\n",
      "276 276.0 276\n",
      "267 267.0 267\n",
      "196 196.0 196\n",
      "170 170.0 170\n",
      "169 169.0 169\n",
      "161 161.0 161\n",
      "153 153.0 153\n",
      "146 146.0 146\n",
      "movie_link.linked_movie_id\n",
      "131 131.0 131\n",
      "72 72.0 72\n",
      "70 70.0 70\n",
      "64 64.0 64\n",
      "57 57.0 57\n",
      "50 50.0 50\n",
      "39 39.0 39\n",
      "39 39.0 39\n",
      "37 37.0 37\n",
      "37 37.0 37\n",
      "movie_link.link_type_id\n",
      "8593 8593.0 8593\n",
      "5503 5503.0 5503\n",
      "5186 5186.0 5186\n",
      "3341 3341.0 3341\n",
      "2223 2223.0 2223\n",
      "1158 1158.0 1158\n",
      "1157 1157.0 1157\n",
      "781 781.0 781\n",
      "625 625.0 625\n",
      "278 278.0 278\n",
      "link_type.id\n",
      "1 1.0 1\n",
      "1 1.0 1\n",
      "1 1.0 1\n",
      "1 1.0 1\n",
      "1 1.0 1\n",
      "1 1.0 1\n",
      "1 1.0 1\n",
      "1 1.0 1\n",
      "1 1.0 1\n",
      "1 1.0 1\n",
      "movie_keyword.movie_id\n",
      "118 533.82374 459\n",
      "115 520.2519500000001 540\n",
      "112 506.68016000000006 455\n",
      "111 502.15623000000005 462\n",
      "104 470.48872 503\n",
      "102 461.44086000000004 444\n",
      "101 456.91693000000004 378\n",
      "99 447.86907 413\n",
      "97 438.82121 394\n",
      "96 434.29728 435\n",
      "movie_keyword.keyword_id\n",
      "16093 72803.60549 72496\n",
      "12846 58114.404780000004 58448\n",
      "9257 41878.02001 41840\n",
      "7827 35408.800110000004 34710\n",
      "6387 28894.340910000003 29440\n",
      "4451 20136.012430000002 20522\n",
      "4360 19724.3348 19528\n",
      "3247 14689.200710000001 14364\n",
      "3075 13911.08475 13753\n",
      "3069 13883.94117 14120\n",
      "keyword.id\n",
      "1 1.0 1\n",
      "1 1.0 1\n",
      "1 1.0 1\n",
      "1 1.0 1\n",
      "1 1.0 1\n",
      "1 1.0 1\n",
      "1 1.0 1\n",
      "1 1.0 1\n",
      "1 1.0 1\n",
      "1 1.0 1\n",
      "person_info.person_id\n",
      "479 1419.595056 1479\n",
      "422 1250.666208 1209\n",
      "384 1138.046976 1178\n",
      "301 892.0628640000001 864\n",
      "286 847.6079040000001 835\n",
      "272 806.116608 836\n",
      "254 752.770656 696\n",
      "236 699.424704 705\n",
      "232 687.570048 652\n",
      "228 675.7153920000001 648\n",
      "person_info.info_type_id\n",
      "208893 619088.6639520001 620526\n",
      "141278 418700.522592 419654\n",
      "119451 354012.628464 353282\n",
      "111521 330510.772944 329504\n",
      "48204 142860.459456 142345\n",
      "45427 134630.364528 134574\n",
      "42743 126675.890352 125700\n",
      "41184 122055.538176 122779\n",
      "38399 113801.733936 113543\n",
      "33618 99632.45635200001 100269\n",
      "movie_companies.movie_id\n",
      "49 127.847321 94\n",
      "33 86.10125699999999 56\n",
      "32 83.492128 87\n",
      "31 80.882999 75\n",
      "30 78.27387 60\n",
      "30 78.27387 58\n",
      "29 75.66474099999999 72\n",
      "29 75.66474099999999 70\n",
      "28 73.055612 59\n",
      "28 73.055612 70\n",
      "movie_companies.company_id\n",
      "23824 62159.889295999994 62075\n",
      "17937 46799.946873 47252\n",
      "13826 36073.817554 36136\n",
      "10555 27539.356594999997 27303\n",
      "6130 15993.96077 16220\n",
      "4963 12949.107226999999 12948\n",
      "4956 12930.843324 12894\n",
      "4499 11738.471371 11817\n",
      "4116 10739.174964 10667\n",
      "3874 10107.765746 10160\n",
      "movie_companies.company_type_id\n",
      "511780 1335300.03962 1334883\n",
      "488220 1273828.9603799998 1274246\n",
      "company_name.id\n",
      "1 1.0 1\n",
      "1 1.0 1\n",
      "1 1.0 1\n",
      "1 1.0 1\n",
      "1 1.0 1\n",
      "1 1.0 1\n",
      "1 1.0 1\n",
      "1 1.0 1\n",
      "1 1.0 1\n",
      "1 1.0 1\n",
      "company_type.id\n",
      "1 1.0 1\n",
      "1 1.0 1\n",
      "1 1.0 1\n",
      "1 1.0 1\n",
      "aka_title.movie_id\n",
      "93 93.0 93\n",
      "43 43.0 43\n",
      "43 43.0 43\n",
      "27 27.0 27\n",
      "26 26.0 26\n",
      "22 22.0 22\n",
      "20 20.0 20\n",
      "19 19.0 19\n",
      "18 18.0 18\n",
      "16 16.0 16\n",
      "aka_title.kind_id\n",
      "293275 293275.0 293275\n",
      "26939 26939.0 26939\n",
      "20320 20320.0 20320\n",
      "13497 13497.0 13497\n",
      "4565 4565.0 4565\n",
      "2876 2876.0 2876\n",
      "kind_type.id\n",
      "1 1.0 1\n",
      "1 1.0 1\n",
      "1 1.0 1\n",
      "1 1.0 1\n",
      "1 1.0 1\n",
      "1 1.0 1\n",
      "1 1.0 1\n"
     ]
    }
   ],
   "source": [
    "sample_rate = dict()\n",
    "sampled_data = dict()\n",
    "for k in data:\n",
    "    print(k)\n",
    "    temp = make_sample(data[k], 1000000)\n",
    "    stats_analysis(temp[0], data[k], temp[1])\n",
    "    sampled_data[k] = temp[0]\n",
    "    sample_rate[k] = temp[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d3302651",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "\n",
    "class Bucket:\n",
    "    \"\"\"\n",
    "    The class of bucketization of a key attribute\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name, bins=[], bin_modes=[], bin_vars=[], bin_means=[], rest_bins_remaining=None):\n",
    "        self.name = name\n",
    "        self.bins = bins\n",
    "        self.bin_modes = bin_modes\n",
    "        self.bin_vars = bin_vars\n",
    "        self.bin_means = bin_means\n",
    "        self.rest_bins_remaining = rest_bins_remaining\n",
    "        if len(bins) != 0:\n",
    "            assert len(bins) == len(bin_modes)\n",
    "\n",
    "\n",
    "class Table_bucket:\n",
    "    \"\"\"\n",
    "    The class of bucketization for all key attributes in a table.\n",
    "    Supporting more than three dimensional bin modes requires simplifying the causal structure\n",
    "    \"\"\"\n",
    "    def __init__(self, table_name, id_attributes, bin_sizes, oned_bin_modes=None):\n",
    "        self.table_name = table_name\n",
    "        self.id_attributes = id_attributes\n",
    "        self.bin_sizes = bin_sizes\n",
    "        if oned_bin_modes:\n",
    "            self.oned_bin_modes = oned_bin_modes\n",
    "        else:\n",
    "            self.oned_bin_modes = dict()\n",
    "        self.twod_bin_modes = dict()\n",
    "\n",
    "\n",
    "class Bucket_group:\n",
    "    \"\"\"\n",
    "    The class of bucketization for a group of equivalent join keys\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, buckets, start_key, sample_rate, bins=None, primary_keys=[]):\n",
    "        self.buckets = buckets\n",
    "        self.start_key = start_key\n",
    "        self.sample_rate = sample_rate\n",
    "        self.bins = bins\n",
    "        self.primary_keys = primary_keys\n",
    "\n",
    "    def bucketize(self, data):\n",
    "        \"\"\"\n",
    "        Discretize data based on the bucket\n",
    "        \"\"\"\n",
    "        res = dict()\n",
    "        seen_remain_key = np.array([])\n",
    "        cumulative_bin = copy.deepcopy(self.buckets[self.start_key].bins)\n",
    "        start_means = np.asarray(self.buckets[self.start_key].bin_means)\n",
    "\n",
    "        for key in data:\n",
    "            if key in self.primary_keys:\n",
    "                continue\n",
    "            res[key] = copy.deepcopy(data[key])\n",
    "            if key != self.start_key:\n",
    "                unique_remain = np.setdiff1d(self.buckets[key].rest_bins_remaining, seen_remain_key)\n",
    "                assert sum([np.sum(np.isin(unique_remain, b) == 1) for b in cumulative_bin]) == 0\n",
    "\n",
    "                if len(unique_remain) != 0:\n",
    "                    remaining_data = data[key][np.isin(data[key], unique_remain)]\n",
    "                    unique_remain, count_remain = np.unique(remaining_data, return_counts=True)\n",
    "                    unique_counts = np.unique(count_remain)\n",
    "                    for u in unique_counts:\n",
    "                        temp_idx = np.searchsorted(start_means, u)\n",
    "                        if temp_idx == len(cumulative_bin):\n",
    "                            idx = -1\n",
    "                            if u > self.buckets[key].bin_modes[-1]:\n",
    "                                self.buckets[key].bin_modes[-1] = u\n",
    "                        elif temp_idx == 0:\n",
    "                            idx = 0\n",
    "                        else:\n",
    "                            if (u - start_means[temp_idx - 1]) >= (start_means[temp_idx] - u):\n",
    "                                idx = temp_idx - 1\n",
    "                            else:\n",
    "                                idx = temp_idx\n",
    "                        temp_unique = unique_remain[count_remain == u]\n",
    "                        cumulative_bin[idx] = np.concatenate((cumulative_bin[idx], temp_unique))\n",
    "                        seen_remain_key = np.concatenate((seen_remain_key, temp_unique))\n",
    "                        if u > self.buckets[key].bin_modes[idx]:\n",
    "                            self.buckets[key].bin_modes[idx] = u\n",
    "            res[key] = copy.deepcopy(data[key])\n",
    "            count = 0\n",
    "            for i, b in enumerate(cumulative_bin):\n",
    "                count += len(data[key][np.isin(data[key], b)])\n",
    "                res[key][np.isin(data[key], b)] = i\n",
    "\n",
    "        self.bins = cumulative_bin\n",
    "\n",
    "        for key in data:\n",
    "            if key in self.primary_keys:\n",
    "                res[key] = self.bucketize_PK(data[key])\n",
    "                self.buckets[key] = Bucket(key, bin_modes=np.ones(len(self.bins)))\n",
    "        \n",
    "        for key in data:\n",
    "            if key in self.primary_keys:\n",
    "                continue\n",
    "            if self.sample_rate[key] < 1.0:\n",
    "                bin_modes = np.asarray(self.buckets[key].bin_modes)\n",
    "                bin_modes[bin_modes != 1] = bin_modes[bin_modes != 1] / self.sample_rate[key]\n",
    "                self.buckets[key].bin_modes = bin_modes\n",
    "        \n",
    "        return res\n",
    "\n",
    "    def bucketize_PK(self, data):\n",
    "        res = copy.deepcopy(data)\n",
    "        remaining_data = np.unique(data)\n",
    "        for i, b in enumerate(self.bins):\n",
    "            res[np.isin(data, b)] = i\n",
    "            remaining_data = np.setdiff1d(remaining_data, b)\n",
    "        if len(remaining_data) != 0:\n",
    "            self.bins.append(list(remaining_data))\n",
    "            for key in self.buckets:\n",
    "                if key not in self.primary_keys:\n",
    "                    self.buckets[key].bin_modes = np.append(self.buckets[key].bin_modes, 0)\n",
    "        res[np.isin(data, remaining_data)] = len(self.bins)\n",
    "        return res\n",
    "\n",
    "\n",
    "\n",
    "def identify_key_values(schema):\n",
    "    \"\"\"\n",
    "    identify all the key attributes from the schema of a DB, currently we assume all possible joins are known\n",
    "    It is also easy to support unseen joins, which we left as a future work.\n",
    "    :param schema: the schema of a DB\n",
    "    :return: a dict of all keys, {table: [keys]};\n",
    "             a dict of set, each indicating which keys on different tables are considered the same key.\n",
    "    \"\"\"\n",
    "    all_keys = set()\n",
    "    equivalent_keys = dict()\n",
    "    for i, join in enumerate(schema.relationships):\n",
    "        keys = join.identifier.split(\" = \")\n",
    "        all_keys.add(keys[0])\n",
    "        all_keys.add(keys[1])\n",
    "        seen = False\n",
    "        for k in equivalent_keys:\n",
    "            if keys[0] in equivalent_keys[k]:\n",
    "                equivalent_keys[k].add(keys[1])\n",
    "                seen = True\n",
    "                break\n",
    "            elif keys[1] in equivalent_keys[k]:\n",
    "                equivalent_keys[k].add(keys[0])\n",
    "                seen = True\n",
    "                break\n",
    "        if not seen:\n",
    "            # set the keys[-1] as the identifier of this equivalent join key group for convenience.\n",
    "            equivalent_keys[keys[-1]] = set(keys)\n",
    "\n",
    "    assert len(all_keys) == sum([len(equivalent_keys[k]) for k in equivalent_keys])\n",
    "    return all_keys, equivalent_keys\n",
    "\n",
    "\n",
    "def equal_freq_binning(name, data, n_bins, data_len):\n",
    "    uniques, counts = data\n",
    "    if len(uniques) <= n_bins:\n",
    "        bins = []\n",
    "        bin_modes = []\n",
    "        bin_vars = []\n",
    "        bin_means = []\n",
    "        \n",
    "        for i, uni in enumerate(uniques):\n",
    "            bins.append([uni])\n",
    "            bin_modes.append(counts[i])\n",
    "            bin_vars.append(0)\n",
    "            bin_means.append(counts[i])\n",
    "        return Bucket(name, bins, bin_modes, bin_vars, bin_means)\n",
    "    \n",
    "    unique_counts, count_counts = np.unique(counts, return_counts=True)\n",
    "    idx = np.argsort(unique_counts)\n",
    "    unique_counts = unique_counts[idx]\n",
    "    count_counts = count_counts[idx]\n",
    "\n",
    "    bins = []\n",
    "    bin_modes = []\n",
    "    bin_vars = []\n",
    "    bin_means = []\n",
    "\n",
    "    bin_freq = data_len / n_bins\n",
    "    cur_freq = 0\n",
    "    cur_bin = []\n",
    "    cur_bin_count = []\n",
    "    for i, uni_c in enumerate(unique_counts):\n",
    "        cur_freq += count_counts[i] * uni_c\n",
    "        cur_bin.append(uniques[np.where(counts == uni_c)[0]])\n",
    "        cur_bin_count.extend([uni_c] * count_counts[i])\n",
    "        if (cur_freq > bin_freq) or (i == (len(unique_counts) - 1)):\n",
    "            bins.append(np.concatenate(cur_bin))\n",
    "            cur_bin_count = np.asarray(cur_bin_count)\n",
    "            bin_modes.append(uni_c)\n",
    "            bin_means.append(np.mean(cur_bin_count))\n",
    "            bin_vars.append(np.var(cur_bin_count))\n",
    "            cur_freq = 0\n",
    "            cur_bin = []\n",
    "            cur_bin_count = []\n",
    "    assert len(uniques) == sum([len(b) for b in bins]), f\"some unique values missed or duplicated\"\n",
    "    return Bucket(name, bins, bin_modes, bin_vars, bin_means)\n",
    "\n",
    "\n",
    "def compute_variance_score(buckets):\n",
    "    \"\"\"\n",
    "    compute the variance of products of random variables\n",
    "    \"\"\"\n",
    "    all_mean = np.asarray([buckets[k].bin_means for k in buckets])\n",
    "    all_var = np.asarray([buckets[k].bin_vars for k in buckets])\n",
    "    return np.sum(np.prod(all_var + all_mean ** 2, axis=0) - np.prod(all_mean, axis=0) ** 2)\n",
    "\n",
    "\n",
    "def sub_optimal_bucketize(data, sample_rate, n_bins=30, primary_keys=[]):\n",
    "    \"\"\"\n",
    "    Perform sub-optimal bucketization on a group of equivalent join keys.\n",
    "    :param data: a dict of (potentially sampled) table data of the keys\n",
    "                 the keys of this dict are one group of equivalent join keys\n",
    "    :param sample_rate: the sampling rate the data, could be all 1 if no sampling is performed\n",
    "    :param n_bins: how many bins can we allocate\n",
    "    :param primary_keys: the primary keys in the equivalent group since we don't need to bucketize PK.\n",
    "    :return: new data, where the keys are bucketized\n",
    "             the mode of each bucket\n",
    "    \"\"\"\n",
    "    unique_values = dict()\n",
    "    for key in data:\n",
    "        if key not in primary_keys:\n",
    "            unique_values[key] = np.unique(data[key], return_counts=True)\n",
    "\n",
    "    best_variance_score = np.infty\n",
    "    best_bin_len = 0\n",
    "    best_start_key = None\n",
    "    best_buckets = None\n",
    "    for start_key in data:\n",
    "        if start_key in primary_keys:\n",
    "            continue\n",
    "        start_bucket = equal_freq_binning(start_key, unique_values[start_key], n_bins, len(data[start_key]))\n",
    "        rest_buckets = dict()\n",
    "        for key in data:\n",
    "            if key == start_key or key in primary_keys:\n",
    "                continue\n",
    "            uniques = unique_values[key][0]\n",
    "            counts = unique_values[key][1]\n",
    "            rest_buckets[key] = Bucket(key, [], [0] * len(start_bucket.bins), [0] * len(start_bucket.bins),\n",
    "                                       [0] * len(start_bucket.bins), uniques)\n",
    "            for i, bin in enumerate(start_bucket.bins):\n",
    "                idx = np.where(np.isin(uniques, bin) == 1)[0]\n",
    "                if len(idx) != 0:\n",
    "                    bin_count = counts[idx]\n",
    "                    unique_bin_keys = uniques[idx]\n",
    "                    # unique_bin_count = np.unique(bin_count)\n",
    "                    # bin_count = np.concatenate([counts[counts == j] for j in unique_bin_count])\n",
    "                    # unique_bin_keys = np.concatenate([uniques[counts == j] for j in unique_bin_count])\n",
    "                    rest_buckets[key].rest_bins_remaining = np.setdiff1d(rest_buckets[key].rest_bins_remaining,\n",
    "                                                                         unique_bin_keys)\n",
    "                    rest_buckets[key].bin_modes[i] = np.max(bin_count)\n",
    "                    rest_buckets[key].bin_vars[i] = np.var(bin_count)\n",
    "                    rest_buckets[key].bin_means[i] = np.mean(bin_count)\n",
    "\n",
    "        rest_buckets[start_key] = start_bucket\n",
    "        var_score = compute_variance_score(rest_buckets)\n",
    "        if len(start_bucket.bins) > best_bin_len:\n",
    "            best_variance_score = var_score\n",
    "            best_start_key = start_key\n",
    "            best_buckets = rest_buckets\n",
    "            best_bin_len = len(start_bucket.bins)\n",
    "        elif len(start_bucket.bins) >= best_bin_len * 0.9 and var_score < best_variance_score:\n",
    "            best_variance_score = var_score\n",
    "            best_start_key = start_key\n",
    "            best_buckets = rest_buckets\n",
    "            best_bin_len = len(start_bucket.bins)\n",
    "    \n",
    "    best_buckets = Bucket_group(best_buckets, best_start_key, sample_rate, primary_keys=primary_keys)\n",
    "    new_data = best_buckets.bucketize(data)\n",
    "    return new_data, best_buckets\n",
    "\n",
    "\n",
    "def fixed_start_key_bucketize(start_key, data, sample_rate, n_bins=30, primary_keys=[]):\n",
    "    \"\"\"\n",
    "    Perform sub-optimal bucketization on a group of equivalent join keys based on the pre-defined start_key.\n",
    "    :param data: a dict of (potentially sampled) table data of the keys\n",
    "                 the keys of this dict are one group of equivalent join keys\n",
    "    :param sample_rate: the sampling rate the data, could be all 1 if no sampling is performed\n",
    "    :param n_bins: how many bins can we allocate\n",
    "    :param primary_keys: the primary keys in the equivalent group since we don't need to bucketize PK.\n",
    "    :return: new data, where the keys are bucketized\n",
    "             the mode of each bucket\n",
    "    \"\"\"\n",
    "    unique_values = dict()\n",
    "    for key in data:\n",
    "        if key not in primary_keys:\n",
    "            unique_values[key] = np.unique(data[key], return_counts=True)\n",
    "\n",
    "    start_bucket = equal_freq_binning(start_key, unique_values[start_key], n_bins, len(data[start_key]))\n",
    "    rest_buckets = dict()\n",
    "    for key in data:\n",
    "        if key == start_key or key in primary_keys:\n",
    "            continue\n",
    "        uniques = unique_values[key][0]\n",
    "        counts = unique_values[key][1]\n",
    "        rest_buckets[key] = Bucket(key, [], [0] * len(start_bucket.bins), [0] * len(start_bucket.bins),\n",
    "                                   [0] * len(start_bucket.bins), uniques)\n",
    "        for i, bin in enumerate(start_bucket.bins):\n",
    "            idx = np.where(np.isin(uniques, bin) == 1)[0]\n",
    "            if len(idx) != 0:\n",
    "                bin_count = counts[idx]\n",
    "                unique_bin_keys = uniques[idx]\n",
    "                rest_buckets[key].rest_bins_remaining = np.setdiff1d(rest_buckets[key].rest_bins_remaining,\n",
    "                                                                     unique_bin_keys)\n",
    "                rest_buckets[key].bin_modes[i] = np.max(bin_count)\n",
    "                rest_buckets[key].bin_means[i] = np.mean(bin_count)\n",
    "\n",
    "    best_buckets = Bucket_group(rest_buckets, start_key, sample_rate, primary_keys=primary_keys)\n",
    "    new_data = best_buckets.bucketize(data)\n",
    "    return new_data, best_buckets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3349d510",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = gen_imdb_schema(data_path)\n",
    "all_keys, equivalent_keys = identify_key_values(schema)\n",
    "data = dict()\n",
    "primary_keys = []\n",
    "for table_obj in schema.tables:\n",
    "    df_rows = pd.read_csv(table_obj.csv_file_location, header=None, escapechar='\\\\', encoding='utf-8',\n",
    "                          quotechar='\"',\n",
    "                          sep=\",\")\n",
    "\n",
    "    df_rows.columns = [table_obj.table_name + '.' + attr for attr in table_obj.attributes]\n",
    "\n",
    "    for attribute in table_obj.irrelevant_attributes:\n",
    "        df_rows = df_rows.drop(table_obj.table_name + '.' + attribute, axis=1)\n",
    "\n",
    "    df_rows.apply(pd.to_numeric, errors=\"ignore\")\n",
    "    for attr in df_rows.columns:\n",
    "        if attr in all_keys:\n",
    "            data[attr] = df_rows[attr].values\n",
    "            data[attr][np.isnan(data[attr])] = -1\n",
    "            data[attr][data[attr] < 0] = -1\n",
    "            data[attr] = copy.deepcopy(data[attr])[data[attr] >= 0]\n",
    "            if len(np.unique(data[attr])) >= len(data[attr]) - 10:\n",
    "                primary_keys.append(attr)\n",
    "\n",
    "sample_rate = dict()\n",
    "sampled_data = dict()\n",
    "for k in data:\n",
    "    temp = make_sample(data[k], 1000000)\n",
    "    sampled_data[k] = temp[0]\n",
    "    sample_rate[k] = temp[1]\n",
    "\n",
    "optimal_buckets = dict()\n",
    "bin_size = dict()\n",
    "all_bin_modes = dict()\n",
    "for PK in equivalent_keys:\n",
    "    # if PK != 'kind_type.id':\n",
    "    #   continue\n",
    "    group_data = {}\n",
    "    group_sample_rate = {}\n",
    "    for K in equivalent_keys[PK]:\n",
    "        group_data[K] = sampled_data[K]\n",
    "        group_sample_rate[K] = sample_rate[K]\n",
    "    _, optimal_bucket = sub_optimal_bucketize(group_data, group_sample_rate, n_bins=n_bins[PK], primary_keys=primary_keys)\n",
    "    optimal_buckets[PK] = optimal_bucket\n",
    "    for K in equivalent_keys[PK]:\n",
    "        temp_table_name = K.split(\".\")[0]\n",
    "        if temp_table_name not in bin_size:\n",
    "            bin_size[temp_table_name] = dict()\n",
    "            all_bin_modes[temp_table_name] = dict()\n",
    "        bin_size[temp_table_name][K] = len(optimal_bucket.bins)\n",
    "        all_bin_modes[temp_table_name][K] = optimal_bucket.buckets[K].bin_modes\n",
    "\n",
    "table_buckets = dict()\n",
    "for table_name in bin_size:\n",
    "    table_buckets[table_name] = Table_bucket(table_name, list(bin_size[table_name].keys()), bin_size[table_name],\n",
    "                                             all_bin_modes[table_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "32238d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_bins = dict()\n",
    "for key in optimal_buckets:\n",
    "    temp_bins[key] = optimal_buckets[key].bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6ffcdc29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['kind_type.id', 'info_type.id', 'title.id', 'name.id', 'char_name.id', 'role_type.id', 'comp_cast_type.id', 'link_type.id', 'keyword.id', 'company_name.id', 'company_type.id'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_bins.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dfea0d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"table_buckets.pkl\", \"rb\") as f:\n",
    "    old_table_buckets = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "29fb6982",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_table_buckets = dict()\n",
    "for key in table_buckets:\n",
    "    if key in old_table_buckets:\n",
    "        new_table_buckets[key] = old_table_buckets[key]\n",
    "    else:\n",
    "        new_table_buckets[key] = table_buckets[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9131967f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': <__main__.Table_bucket at 0x7fc0923dda50>,\n",
       " 'aka_title': <__main__.Table_bucket at 0x7fc09d1a4b50>,\n",
       " 'kind_type': <__main__.Table_bucket at 0x7fc09d1a42d0>,\n",
       " 'info_type': <__main__.Table_bucket at 0x7fc09d1a4b10>,\n",
       " 'movie_info': <__main__.Table_bucket at 0x7fc09d1a45d0>,\n",
       " 'movie_info_idx': <__main__.Table_bucket at 0x7fc09d1a4790>,\n",
       " 'person_info': <__main__.Table_bucket at 0x7fc09d1a4950>,\n",
       " 'movie_companies': <__main__.Table_bucket at 0x7fc09d1a4c10>,\n",
       " 'movie_keyword': <__main__.Table_bucket at 0x7fc09c7fd890>,\n",
       " 'cast_info': <__main__.Table_bucket at 0x7fc09c7fd050>,\n",
       " 'complete_cast': <__main__.Table_bucket at 0x7fc09c7fdd50>,\n",
       " 'name': <__main__.Table_bucket at 0x7fc09c7fd350>,\n",
       " 'aka_name': <__main__.Table_bucket at 0x7fc09c7fd2d0>,\n",
       " 'char_name': <__main__.Table_bucket at 0x7fc09c7fd290>,\n",
       " 'role_type': <__main__.Table_bucket at 0x7fc09c7fdb90>,\n",
       " 'comp_cast_type': <__main__.Table_bucket at 0x7fc09c7fd0d0>,\n",
       " 'keyword': <__main__.Table_bucket at 0x7fc09c7fd110>,\n",
       " 'company_name': <__main__.Table_bucket at 0x7fc09c7fd310>,\n",
       " 'company_type': <__main__.Table_bucket at 0x7fc09c7fd410>}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_table_buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4f3bcc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_binning_to_data_value_count(bins, data):\n",
    "    res = np.zeros(len(bins))\n",
    "    bin_mode = np.zeros(len(bins))\n",
    "    unique_remain = np.unique(data)\n",
    "    for i, bin in enumerate(bins):\n",
    "        data_bin = data[np.isin(data, bin)]\n",
    "        res[i] = np.sum(np.isin(data, bin))\n",
    "        unique_remain = np.setdiff1d(unique_remain, bin)\n",
    "        _, counts = np.unique(data_bin, return_counts=True)\n",
    "        if len(counts) == 0:\n",
    "            bin_mode[i] = 0\n",
    "        else:\n",
    "            bin_mode[i] = np.max(counts)\n",
    "    res[0] += np.sum(np.isin(data, unique_remain))\n",
    "    return bin_mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "bcd2642a",
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = apply_binning_to_data_value_count(bins['title.id'], data['movie_link.linked_movie_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f2479afe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "974ed8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_table_buckets['movie_link'].oned_bin_modes['movie_link.linked_movie_id'] = t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "12ced593",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'movie_link.linked_movie_id': array([ 30.,  33.,  23.,  26.,  23.,  28.,  28.,  72.,  12.,  30.,  17.,\n",
       "         19.,  26.,  23.,  16.,  26.,  22.,  39.,  21.,  16.,  15.,  34.,\n",
       "         57.,  15.,  23.,  18.,  36.,  19.,  12.,  35.,  39.,  13.,  19.,\n",
       "         18.,  37.,  22.,  14.,  14.,  33.,  27., 131.,   8.,  29.,  64.,\n",
       "         15.,  23.,   5.,  15.,  25.,   7.,  14.,  29.,   9.,  50.,   4.,\n",
       "         11.,   6.,   6.,   3.,   5.,  16.,  14.,  11.,  20.,  14.,  15.,\n",
       "          4.,  27.,  19.,  37.,   6.,  12.,   3.,  19.]),\n",
       " 'movie_link.movie_id': array([267., 517., 117., 161., 276., 146., 143.,  46., 170.,  50.,  32.,\n",
       "         44.,  40.,  36.,  11., 569.,  67.,  81.,  11., 196.,   8.,   0.,\n",
       "          2.,  21.,   4.,   0.,   0.,   1., 101.,   0.,   0.,  42.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,  42.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "          0.,   0.,   0.,   0.,   0.,   0.,   0.,  72.]),\n",
       " 'movie_link.link_type_id': array([1158, 1157,  207,  199, 8593, 5503,  625,  781, 5186, 2223,  204,\n",
       "         247, 3341,  211,  278,   84,    0])}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_table_buckets['movie_link'].oned_bin_modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1ab05830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title kind_type.id\n",
      "2528312.0\n",
      "[6.628240e+05 9.085200e+04 1.005370e+05 1.182340e+05 1.260000e+04\n",
      " 1.543264e+06 1.000000e+00 0.000000e+00]\n",
      "kind_type kind_type.id\n",
      "7.0\n",
      "[1. 1. 1. 1. 1. 1. 0. 1.]\n",
      "aka_title kind_type.id\n",
      "361472.0\n",
      "[293275.  20320.  26939.  13497.   4565.   2876.      0.      0.]\n",
      "person_info info_type.id\n",
      "2963664.0\n",
      "[      0.       0.       0.       0.       0.       0.       0.       0.\n",
      "       0.       0.       0.       0.   75258.       0.  620526.       0.\n",
      "   49990.       0.       0.       0.       0.       0.       0.       0.\n",
      "       0.       0.       0.       0.       0.       0.       0.       0.\n",
      "       0.       0.       0.       0.       0.       0.       0.       0.\n",
      "       0.       0.       0.       0.       0.       0.       0.       0.\n",
      "       0.       0.       0.       0.       0.       0.       0.       0.\n",
      "       0.       0.       0.       0.       0.       0.       0.   12109.\n",
      "   40462.   10589.       0.       0.       0.       0. 2154730.       0.]\n",
      "info_type info_type.id\n",
      "113.0\n",
      "[ 3.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  3.  3.  2.  1.  1.  1.  1. 18. 18.]\n",
      "movie_info_idx info_type.id\n",
      "1380035.0\n",
      "[2.600000e+02 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      " 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      " 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      " 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      " 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      " 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      " 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      " 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      " 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      " 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      " 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      " 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      " 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      " 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00\n",
      " 1.379775e+06 0.000000e+00]\n",
      "movie_info info_type.id\n",
      "14835720.0\n",
      "[8.021400e+05 1.288928e+06 1.533909e+06 1.298989e+06 4.865540e+05\n",
      " 4.744430e+05 1.401902e+06 1.325361e+06 1.400650e+05 1.820500e+04\n",
      " 1.608600e+04 1.801120e+05 5.984570e+05 3.036719e+06 3.682180e+05\n",
      " 6.609230e+05 4.339000e+03 6.598000e+03 6.614000e+03 6.509000e+03\n",
      " 6.595000e+03 1.424000e+03 6.142000e+03 6.586000e+03 1.500000e+01\n",
      " 5.576000e+03 6.594000e+03 6.482000e+03 6.034000e+03 6.616000e+03\n",
      " 6.613000e+03 6.592000e+03 6.440000e+03 6.620000e+03 6.536000e+03\n",
      " 6.272000e+03 6.346000e+03 6.590000e+03 2.522000e+03 1.444000e+03\n",
      " 6.620000e+03 6.527000e+03 6.578000e+03 6.569000e+03 6.435000e+03\n",
      " 6.620000e+03 4.700000e+02 6.575000e+03 6.621000e+03 6.422000e+03\n",
      " 6.160000e+03 6.621000e+03 3.130000e+03 1.853900e+04 3.084000e+03\n",
      " 5.162000e+03 2.112000e+03 6.802500e+04 4.797000e+03 9.621000e+03\n",
      " 1.369800e+04 3.288570e+05 1.660300e+04 6.112800e+04 3.698500e+04\n",
      " 1.218630e+05 1.090340e+05 1.474870e+05 1.821400e+04 1.873000e+03\n",
      " 3.018000e+04 0.000000e+00]\n",
      "movie_companies title.id\n",
      "2609129.0\n",
      "[1.71660e+06 2.47205e+05 1.05941e+05 6.28350e+04 4.51230e+04 3.57920e+04\n",
      " 2.96900e+04 2.51350e+04 2.30450e+04 2.01530e+04 1.90140e+04 1.66010e+04\n",
      " 1.38600e+04 1.25910e+04 1.03790e+04 1.04190e+04 8.15600e+03 8.23200e+03\n",
      " 7.71200e+03 6.62500e+03 5.77800e+03 6.38500e+03 5.31000e+03 5.28800e+03\n",
      " 5.47100e+03 4.39800e+03 4.82900e+03 3.99300e+03 3.81200e+03 3.83200e+03\n",
      " 3.21800e+03 3.79900e+03 2.84900e+03 3.27500e+03 2.75000e+03 2.82500e+03\n",
      " 2.53500e+03 2.31700e+03 2.23500e+03 1.89500e+03 1.94400e+03 1.78700e+03\n",
      " 2.01100e+03 1.45000e+03 1.46600e+03 1.85900e+03 1.52000e+03 1.24700e+03\n",
      " 1.19600e+03 9.89000e+02 1.11100e+03 1.45400e+03 7.82000e+02 1.14400e+03\n",
      " 6.02000e+02 1.30400e+03 6.26000e+02 7.84000e+02 5.72000e+02 5.27000e+02\n",
      " 6.75000e+02 6.40000e+02 8.80000e+02 7.07000e+02 6.86000e+02 5.71000e+02\n",
      " 5.07000e+02 7.23000e+02 5.81000e+02 5.37000e+02 3.97000e+02 3.79000e+02\n",
      " 3.46000e+02 7.52230e+04]\n",
      "cast_info title.id\n",
      "36244344.0\n",
      "[2.5883188e+07 3.2638690e+06 1.0663170e+06 5.7018300e+05 3.7732200e+05\n",
      " 2.7253000e+05 2.1975700e+05 1.8288000e+05 1.5557000e+05 1.3438000e+05\n",
      " 1.2080800e+05 1.0051200e+05 8.4783000e+04 7.6793000e+04 5.9688000e+04\n",
      " 6.0270000e+04 4.5665000e+04 4.2752000e+04 4.0533000e+04 3.6170000e+04\n",
      " 3.0571000e+04 3.4413000e+04 2.8089000e+04 2.8586000e+04 2.9505000e+04\n",
      " 2.4226000e+04 2.3272000e+04 2.0495000e+04 2.1336000e+04 2.0379000e+04\n",
      " 1.7238000e+04 1.7742000e+04 1.5171000e+04 1.7571000e+04 1.5938000e+04\n",
      " 1.3390000e+04 1.3173000e+04 1.2599000e+04 1.0964000e+04 9.1780000e+03\n",
      " 9.9540000e+03 9.1900000e+03 9.3530000e+03 7.8000000e+03 7.7390000e+03\n",
      " 9.0390000e+03 7.9150000e+03 6.9900000e+03 7.2300000e+03 4.7300000e+03\n",
      " 6.3690000e+03 7.5960000e+03 4.7670000e+03 5.8050000e+03 3.2300000e+03\n",
      " 5.9220000e+03 3.8250000e+03 3.9410000e+03 2.4850000e+03 3.0690000e+03\n",
      " 3.9210000e+03 2.9550000e+03 4.8980000e+03 3.9970000e+03 3.6960000e+03\n",
      " 4.6810000e+03 2.7610000e+03 2.9830000e+03 2.8450000e+03 3.0930000e+03\n",
      " 1.6630000e+03 2.5420000e+03 1.5340000e+03 2.8780200e+06]\n",
      "movie_info_idx title.id\n",
      "1380035.0\n",
      "[9.10258e+05 2.37807e+05 5.89230e+04 3.37970e+04 2.33740e+04 1.74970e+04\n",
      " 1.37820e+04 1.13150e+04 9.73800e+03 7.88400e+03 7.04700e+03 5.85000e+03\n",
      " 4.76900e+03 4.00200e+03 3.15600e+03 2.89300e+03 2.24200e+03 2.07600e+03\n",
      " 1.80500e+03 1.61000e+03 1.36000e+03 1.37400e+03 1.08900e+03 1.04700e+03\n",
      " 1.06400e+03 9.37000e+02 8.95000e+02 7.64000e+02 7.34000e+02 6.35000e+02\n",
      " 5.88000e+02 6.10000e+02 4.89000e+02 5.30000e+02 4.75000e+02 4.13000e+02\n",
      " 4.52000e+02 3.99000e+02 3.64000e+02 3.00000e+02 2.93000e+02 2.80000e+02\n",
      " 2.83000e+02 2.55000e+02 2.48000e+02 2.66000e+02 2.24000e+02 1.67000e+02\n",
      " 1.84000e+02 1.45000e+02 1.50000e+02 2.16000e+02 1.16000e+02 1.77000e+02\n",
      " 1.05000e+02 1.59000e+02 8.80000e+01 1.07000e+02 8.20000e+01 9.40000e+01\n",
      " 7.50000e+01 7.70000e+01 1.08000e+02 9.90000e+01 7.90000e+01 7.70000e+01\n",
      " 6.80000e+01 7.70000e+01 7.30000e+01 5.60000e+01 5.40000e+01 4.40000e+01\n",
      " 3.70000e+01 1.12800e+03]\n",
      "movie_link title.id\n",
      "29997.0\n",
      "[1.1812e+04 2.9660e+03 1.3930e+03 8.9700e+02 7.5600e+02 5.8400e+02\n",
      " 5.3800e+02 6.2500e+02 4.2800e+02 4.1600e+02 4.5300e+02 4.3200e+02\n",
      " 3.7300e+02 3.1300e+02 2.9600e+02 3.7600e+02 3.4400e+02 2.9800e+02\n",
      " 2.6000e+02 2.1900e+02 2.5000e+02 2.9400e+02 2.4500e+02 2.1200e+02\n",
      " 2.7200e+02 1.6100e+02 1.9700e+02 1.8900e+02 1.8700e+02 2.0600e+02\n",
      " 2.1300e+02 1.6200e+02 1.8300e+02 1.7000e+02 1.7000e+02 1.1400e+02\n",
      " 1.4400e+02 1.2700e+02 1.7500e+02 1.4200e+02 2.0000e+02 7.3000e+01\n",
      " 1.2100e+02 9.9000e+01 7.8000e+01 1.1300e+02 5.1000e+01 9.2000e+01\n",
      " 6.2000e+01 3.2000e+01 4.7000e+01 9.2000e+01 2.8000e+01 1.0400e+02\n",
      " 1.3000e+01 5.6000e+01 4.1000e+01 1.9000e+01 6.0000e+00 1.7000e+01\n",
      " 4.6000e+01 4.0000e+01 4.0000e+01 5.4000e+01 3.6000e+01 3.7000e+01\n",
      " 1.5000e+01 5.0000e+01 3.9000e+01 1.3800e+02 1.2000e+01 2.6000e+01\n",
      " 7.0000e+00 5.2100e+02]\n",
      "aka_title title.id\n",
      "361472.0\n",
      "[2.00758e+05 4.32230e+04 2.03880e+04 1.33130e+04 9.91400e+03 8.17400e+03\n",
      " 7.10700e+03 6.01800e+03 5.58200e+03 4.53700e+03 4.22700e+03 3.75000e+03\n",
      " 3.05700e+03 2.74600e+03 2.32100e+03 2.19700e+03 1.71900e+03 1.72400e+03\n",
      " 1.56500e+03 1.37700e+03 1.14000e+03 1.20200e+03 9.12000e+02 8.77000e+02\n",
      " 9.88000e+02 8.33000e+02 8.44000e+02 6.81000e+02 6.45000e+02 5.68000e+02\n",
      " 5.23000e+02 6.04000e+02 4.86000e+02 5.26000e+02 4.63000e+02 3.53000e+02\n",
      " 4.49000e+02 3.95000e+02 3.77000e+02 3.08000e+02 2.99000e+02 2.76000e+02\n",
      " 2.66000e+02 2.70000e+02 2.48000e+02 2.59000e+02 2.29000e+02 1.79000e+02\n",
      " 1.70000e+02 1.42000e+02 1.47000e+02 2.12000e+02 1.10000e+02 1.69000e+02\n",
      " 1.07000e+02 1.57000e+02 7.90000e+01 9.90000e+01 7.60000e+01 9.20000e+01\n",
      " 6.80000e+01 5.90000e+01 1.08000e+02 1.13000e+02 5.50000e+01 6.40000e+01\n",
      " 5.20000e+01 1.01000e+02 7.30000e+01 6.80000e+01 7.30000e+01 4.90000e+01\n",
      " 1.32000e+02 0.00000e+00]\n",
      "movie_link title.id\n",
      "29997.0\n",
      "[1.7297e+04 4.2060e+03 1.8400e+03 1.1180e+03 6.7700e+02 6.7000e+02\n",
      " 3.2300e+02 1.7800e+02 5.0700e+02 1.7600e+02 1.3300e+02 6.4000e+01\n",
      " 1.0200e+02 1.3000e+02 1.6000e+01 5.8200e+02 1.0200e+02 1.1200e+02\n",
      " 1.1000e+01 2.0200e+02 1.9000e+01 0.0000e+00 2.0000e+00 4.2000e+01\n",
      " 4.0000e+00 0.0000e+00 0.0000e+00 1.0000e+00 1.0100e+02 0.0000e+00\n",
      " 0.0000e+00 4.3000e+01 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00\n",
      " 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00\n",
      " 0.0000e+00 0.0000e+00 0.0000e+00 4.2000e+01 0.0000e+00 0.0000e+00\n",
      " 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00\n",
      " 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00\n",
      " 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00\n",
      " 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00\n",
      " 0.0000e+00 1.2970e+03]\n",
      "title title.id\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2528312.0\n",
      "[1.881396e+06 1.312220e+05 3.459500e+04 1.693000e+04 1.052400e+04\n",
      " 7.475000e+03 5.756000e+03 4.737000e+03 3.935000e+03 3.183000e+03\n",
      " 2.811000e+03 2.285000e+03 1.802000e+03 1.481000e+03 1.161000e+03\n",
      " 1.024000e+03 7.940000e+02 7.210000e+02 6.270000e+02 5.510000e+02\n",
      " 4.690000e+02 4.670000e+02 3.700000e+02 3.530000e+02 3.590000e+02\n",
      " 3.150000e+02 2.990000e+02 2.560000e+02 2.470000e+02 2.120000e+02\n",
      " 1.960000e+02 2.020000e+02 1.610000e+02 1.750000e+02 1.570000e+02\n",
      " 1.390000e+02 1.490000e+02 1.310000e+02 1.190000e+02 9.900000e+01\n",
      " 9.800000e+01 9.300000e+01 9.400000e+01 8.400000e+01 8.100000e+01\n",
      " 8.800000e+01 7.400000e+01 5.500000e+01 6.100000e+01 4.900000e+01\n",
      " 4.900000e+01 7.200000e+01 3.900000e+01 5.800000e+01 3.500000e+01\n",
      " 5.200000e+01 3.000000e+01 3.500000e+01 2.700000e+01 3.100000e+01\n",
      " 2.600000e+01 2.500000e+01 3.600000e+01 3.200000e+01 2.600000e+01\n",
      " 2.500000e+01 2.200000e+01 2.600000e+01 2.300000e+01 1.700000e+01\n",
      " 1.800000e+01 1.400000e+01 1.200000e+01 4.089200e+05]\n",
      "movie_keyword title.id\n",
      "4523930.0\n",
      "[869557. 401702. 287443. 226712. 200361. 183740. 169562. 159732. 149915.\n",
      " 134945. 127838. 112825.  96206.  84101.  70460.  67519.  55559.  54585.\n",
      "  50399.  47640.  42600.  44598.  37262.  37422.  39705.  35949.  35302.\n",
      "  31168.  31612.  27750.  26943.  28839.  23951.  26219.  24253.  21997.\n",
      "  23811.  21785.  20040.  17575.  17623.  16961.  17911.  15951.  15752.\n",
      "  17641.  15192.  11538.  13193.  10880.  10792.  16402.   8769.  13532.\n",
      "   8140.  12482.   7405.   8816.   6761.   7997.   6977.   6739.   9858.\n",
      "   8766.   7301.   7295.   6469.   8099.   7347.   5882.   6547.   5235.\n",
      "   5287.  28808.]\n",
      "movie_info title.id\n",
      "14835720.0\n",
      "[9.461333e+06 1.230028e+06 4.915710e+05 2.996620e+05 2.235580e+05\n",
      " 1.801550e+05 1.554590e+05 1.367970e+05 1.219700e+05 1.086810e+05\n",
      " 1.040990e+05 9.104500e+04 8.050500e+04 7.326700e+04 6.170300e+04\n",
      " 6.649400e+04 5.631700e+04 5.428200e+04 5.147800e+04 4.601400e+04\n",
      " 3.865500e+04 4.501100e+04 3.598000e+04 3.644100e+04 4.105700e+04\n",
      " 3.258600e+04 3.576400e+04 2.952300e+04 3.063500e+04 3.133400e+04\n",
      " 2.659400e+04 2.760400e+04 2.472300e+04 2.570000e+04 2.440800e+04\n",
      " 2.203700e+04 2.040100e+04 2.161800e+04 1.865800e+04 1.635100e+04\n",
      " 1.526500e+04 1.341300e+04 1.570400e+04 1.173900e+04 1.251700e+04\n",
      " 1.509200e+04 1.170200e+04 1.102700e+04 9.913000e+03 8.104000e+03\n",
      " 8.648000e+03 1.128000e+04 6.020000e+03 8.282000e+03 4.438000e+03\n",
      " 9.695000e+03 5.581000e+03 5.548000e+03 3.958000e+03 3.985000e+03\n",
      " 6.081000e+03 4.840000e+03 7.337000e+03 6.395000e+03 5.392000e+03\n",
      " 5.386000e+03 4.144000e+03 5.776000e+03 4.573000e+03 6.326000e+03\n",
      " 2.282000e+03 3.390000e+03 2.397000e+03 8.999920e+05]\n",
      "complete_cast title.id\n",
      "135086.0\n",
      "[7.9735e+04 1.5233e+04 7.0750e+03 4.4450e+03 3.5230e+03 3.0180e+03\n",
      " 2.4080e+03 2.1770e+03 2.0000e+03 1.6250e+03 1.5080e+03 1.3320e+03\n",
      " 1.0450e+03 8.9500e+02 7.5200e+02 7.3800e+02 5.7900e+02 5.3400e+02\n",
      " 5.2000e+02 4.1900e+02 3.5500e+02 3.7400e+02 2.9400e+02 2.8000e+02\n",
      " 2.9300e+02 2.4400e+02 2.8700e+02 2.0200e+02 2.1000e+02 1.9800e+02\n",
      " 1.7400e+02 1.7300e+02 1.5200e+02 1.5200e+02 1.5300e+02 1.1800e+02\n",
      " 1.3000e+02 1.2500e+02 1.0700e+02 8.4000e+01 8.8000e+01 8.3000e+01\n",
      " 8.7000e+01 7.0000e+01 7.8000e+01 8.9000e+01 7.3000e+01 6.3000e+01\n",
      " 5.2000e+01 4.9000e+01 4.5000e+01 6.4000e+01 3.6000e+01 5.7000e+01\n",
      " 2.7000e+01 5.4000e+01 3.2000e+01 3.2000e+01 2.4000e+01 3.2000e+01\n",
      " 3.2000e+01 2.1000e+01 3.2000e+01 2.8000e+01 2.1000e+01 2.2000e+01\n",
      " 2.1000e+01 3.0000e+01 2.2000e+01 2.2000e+01 1.3000e+01 1.4000e+01\n",
      " 7.0000e+00 0.0000e+00]\n",
      "name name.id\n",
      "4167491.0\n",
      "[3.156119e+06 1.212440e+05 5.590100e+04 2.776900e+04 1.563900e+04\n",
      " 9.779000e+03 6.407000e+03 4.548000e+03 3.224000e+03 2.563000e+03\n",
      " 1.966000e+03 1.574000e+03 1.256000e+03 9.840000e+02 1.257000e+03\n",
      " 1.071000e+03 8.400000e+02 7.860000e+02 6.500000e+02 5.270000e+02\n",
      " 4.270000e+02 3.850000e+02 2.810000e+02 2.320000e+02 1.860000e+02\n",
      " 1.320000e+02 7.800000e+01 1.800000e+01 7.516480e+05]\n",
      "person_info name.id\n",
      "2963664.0\n",
      "[823136. 458315. 306418. 206016. 148587. 112835.  88453.  73138.  60318.\n",
      "  51953.  44536.  39668.  34991.  29635.  47445.  38237.  34761.  38977.\n",
      "  32058.  33514.  31060.  32449.  29057.  29783.  30630.  29156.  29280.\n",
      "  12745.  36513.]\n",
      "aka_name name.id\n",
      "901343.0\n",
      "[5.51447e+05 1.33435e+05 7.41230e+04 4.20090e+04 2.53490e+04 1.68970e+04\n",
      " 1.12790e+04 8.35100e+03 5.92500e+03 4.90800e+03 3.51900e+03 3.07700e+03\n",
      " 2.61700e+03 1.89000e+03 2.81500e+03 2.24800e+03 1.85100e+03 1.68900e+03\n",
      " 1.44700e+03 1.30700e+03 1.07900e+03 1.02400e+03 8.80000e+02 7.23000e+02\n",
      " 5.85000e+02 4.58000e+02 3.19000e+02 9.20000e+01 0.00000e+00]\n",
      "cast_info name.id\n",
      "36244344.0\n",
      "[16966220.  4497279.  2821807.  1860859.  1352828.  1015160.   762076.\n",
      "   603445.   499444.   419294.   336571.   300819.   252718.   202077.\n",
      "   273075.   296175.   262221.   237326.   255113.   194175.   210241.\n",
      "   195891.   179451.   135167.   143436.   107293.    39093.    17485.\n",
      "  1807605.]\n",
      "char_name char_name.id\n",
      "3140339.0\n",
      "[2.171765e+06 3.237100e+04 1.349100e+04 7.673000e+03 4.913000e+03\n",
      " 3.523000e+03 4.649000e+03 2.936000e+03 1.997000e+03 1.914000e+03\n",
      " 1.239000e+03 1.020000e+03 8.280000e+02 7.300000e+02 5.160000e+02\n",
      " 4.050000e+02 3.270000e+02 2.400000e+02 1.740000e+02 1.240000e+02\n",
      " 8.700000e+01 5.600000e+01 3.300000e+01 1.400000e+01 7.000000e+00\n",
      " 1.000000e+00 1.000000e+00 8.893050e+05]\n",
      "cast_info char_name.id\n",
      "17571519.0\n",
      "[4381971.  736191.  545231.  446582.  368766.  329405.  545576.  441938.\n",
      "  369255.  434972.  340988.  338029.  336615.  381235.  351465.  348045.\n",
      "  362783.  359717.  347767.  351776.  355553.  352681.  356402.  359176.\n",
      "  626955.  662713. 1322872. 1416860.]\n",
      "cast_info role_type.id\n",
      "36244344.0\n",
      "[12670688.  7451973.  4008037.  2728943.   898389.   773674.   276403.\n",
      "  1703543.  1093558.  4323018.   316118.        0.]\n",
      "role_type role_type.id\n",
      "12.0\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "comp_cast_type comp_cast_type.id\n",
      "4.0\n",
      "[2. 2.]\n",
      "complete_cast comp_cast_type.id\n",
      "135086.0\n",
      "[ 24592. 110494.]\n",
      "complete_cast comp_cast_type.id\n",
      "135086.0\n",
      "[85941. 49145.]\n",
      "movie_link link_type.id\n",
      "29997.0\n",
      "[1158. 1157.  207.  199. 8593. 5503.  625.  781. 5186. 2223.  204.  247.\n",
      " 3341.  211.  278.   84.    0.]\n",
      "link_type link_type.id\n",
      "18.0\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2.]\n",
      "keyword keyword.id\n",
      "134170.0\n",
      "[3.1359e+04 1.1032e+04 5.8530e+03 3.8120e+03 2.7610e+03 2.1010e+03\n",
      " 1.6530e+03 1.3140e+03 2.0670e+03 1.4870e+03 1.1550e+03 9.1400e+02\n",
      " 7.2500e+02 6.2000e+02 4.8600e+02 4.3000e+02 5.1600e+02 4.4700e+02\n",
      " 3.5400e+02 3.7200e+02 2.8900e+02 2.4500e+02 2.4900e+02 2.4100e+02\n",
      " 2.1700e+02 1.8300e+02 2.1600e+02 1.5800e+02 1.6200e+02 1.5700e+02\n",
      " 1.3000e+02 1.2300e+02 1.1000e+02 1.0500e+02 9.6000e+01 9.0000e+01\n",
      " 9.1000e+01 7.9000e+01 7.7000e+01 7.7000e+01 7.0000e+01 6.1000e+01\n",
      " 5.9000e+01 6.1000e+01 5.0000e+01 4.9000e+01 4.7000e+01 4.5000e+01\n",
      " 4.3000e+01 4.0000e+01 3.5000e+01 3.3000e+01 3.2000e+01 2.9000e+01\n",
      " 2.8000e+01 2.6000e+01 2.7000e+01 2.3000e+01 2.2000e+01 2.1000e+01\n",
      " 1.9000e+01 1.8000e+01 1.7000e+01 1.7000e+01 1.5000e+01 1.5000e+01\n",
      " 1.4000e+01 1.3000e+01 1.2000e+01 1.1000e+01 1.1000e+01 1.0000e+01\n",
      " 9.0000e+00 8.0000e+00 7.0000e+00 7.0000e+00 6.0000e+00 6.0000e+00\n",
      " 5.0000e+00 4.0000e+00 3.0000e+00 2.0000e+00 2.0000e+00 1.0000e+00\n",
      " 1.0000e+00 6.0583e+04]\n",
      "movie_keyword keyword.id\n",
      "4523930.0\n",
      "[107838.  85363.  70187.  62348.  57879.  53432.  49430.  44632.  84137.\n",
      "  74028.  68192.  61684.  55810.  52761.  46820.  44260.  58859.  58769.\n",
      "  50550.  58268.  48879.  45282.  48925.  51478.  50373.  45150.  58536.\n",
      "  45800.  49277.  52574.  46537.  48223.  46290.  47630.  45825.  45108.\n",
      "  48761.  44828.  47864.  50685.  48547.  45572.  46820.  52507.  45110.\n",
      "  47238.  47511.  47751.  48509.  48735.  45762.  46352.  47672.  44864.\n",
      "  46440.  45055.  51689.  45689.  46660.  47401.  45345.  46192.  46853.\n",
      "  49422.  46123.  48656.  49080.  48725.  48414.  46330.  48036.  46276.\n",
      "  45974.  46216.  45405.  48014.  45756.  49902.  48347.  48108.  47645.\n",
      "  49962.  76550.  58448.  72496. 112499.]\n",
      "movie_companies company_name.id\n",
      "2609129.0\n",
      "[147149.  87752.  61631.  49610.  41145.  35842.  32303.  28288.  25873.\n",
      "  45555.  41603.  35282.  32027.  29387.  37221.  37345.  31457.  30995.\n",
      "  28555.  26642.  31759.  29641.  27931.  31968.  26160.  29016.  26216.\n",
      "  30639.  31068.  29431.  29478.  28093.  26527.  28003.  27919.  27061.\n",
      "  27893.  26912.  28027.  26936.  27826.  27903.  26300.  27181.  27482.\n",
      "  26244.  26537.  27694.  26672.  27096.  26009.  25943.  27475.  27226.\n",
      "  27495.  27746.  27910.  27621.  26072.  28851.  28059.  26590.  28703.\n",
      "  27932.  26235.  32463.  33522.  28859.  32644.  42062.  27303.  36136.\n",
      "  47252.  62075. 129671.]\n",
      "company_name company_name.id\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "234997.0\n",
      "[8.0873e+04 2.1082e+04 9.1760e+03 5.3470e+03 3.4680e+03 2.4830e+03\n",
      " 1.8890e+03 1.4290e+03 1.1650e+03 1.7360e+03 1.3280e+03 9.6800e+02\n",
      " 7.7600e+02 6.2100e+02 7.0000e+02 6.1300e+02 4.5700e+02 4.0400e+02\n",
      " 3.3700e+02 2.9200e+02 3.1300e+02 2.6500e+02 2.3000e+02 2.3900e+02\n",
      " 1.7900e+02 1.8300e+02 1.5400e+02 1.6700e+02 1.5400e+02 1.3500e+02\n",
      " 1.2300e+02 1.0800e+02 9.4000e+01 9.0000e+01 8.3000e+01 7.5000e+01\n",
      " 7.0000e+01 6.3000e+01 6.0000e+01 5.3000e+01 5.1000e+01 4.7000e+01\n",
      " 4.1000e+01 3.9000e+01 3.6000e+01 3.2000e+01 2.9000e+01 2.8000e+01\n",
      " 2.5000e+01 2.3000e+01 2.0000e+01 1.8000e+01 1.7000e+01 1.5000e+01\n",
      " 1.4000e+01 1.3000e+01 1.2000e+01 1.1000e+01 9.0000e+00 9.0000e+00\n",
      " 8.0000e+00 7.0000e+00 7.0000e+00 6.0000e+00 5.0000e+00 5.0000e+00\n",
      " 4.0000e+00 3.0000e+00 3.0000e+00 3.0000e+00 1.0000e+00 1.0000e+00\n",
      " 1.0000e+00 1.0000e+00 9.6471e+04]\n",
      "company_type company_type.id\n",
      "4.0\n",
      "[1. 1. 2.]\n",
      "movie_companies company_type.id\n",
      "2609129.0\n",
      "[1274246. 1334883.       0.]\n"
     ]
    }
   ],
   "source": [
    "def apply_binning_to_data_value_count(bins, data):\n",
    "    res = np.zeros(len(bins))\n",
    "    unique_remain = np.unique(data)\n",
    "    for i, bin in enumerate(bins):\n",
    "        res[i] = np.sum(np.isin(data, bin))\n",
    "        unique_remain = np.setdiff1d(unique_remain, bin)\n",
    "\n",
    "    res[0] += np.sum(np.isin(data, unique_remain))\n",
    "    return res\n",
    "\n",
    "class Factor:\n",
    "    \"\"\"\n",
    "    This the class defines a multidimensional conditional probability on one table.\n",
    "    \"\"\"\n",
    "    def __init__(self, table, table_len, variables, pdfs, na_values=None):\n",
    "        self.table = table\n",
    "        self.table_len = table_len\n",
    "        self.variables = variables\n",
    "        self.pdfs = pdfs\n",
    "        self.na_values = na_values  # this is the percentage of data, which is not nan.\n",
    "\n",
    "all_factor_pdfs = dict()\n",
    "for PK in equivalent_keys:\n",
    "    if PK in bins:\n",
    "        bin_value = bins[PK]\n",
    "    else:\n",
    "        bin_value = temp_bins[PK]\n",
    "    for key in equivalent_keys[PK]:\n",
    "        table = key.split(\".\")[0]\n",
    "        print(table, PK)\n",
    "        temp = apply_binning_to_data_value_count(bin_value, data[key])\n",
    "        print(np.sum(temp))\n",
    "        print(temp)\n",
    "        if table not in all_factor_pdfs:\n",
    "            all_factor_pdfs[table] = dict()\n",
    "        all_factor_pdfs[table][key] = temp / np.sum(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "566dfdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_factors = dict()\n",
    "for table in all_factor_pdfs:\n",
    "    all_factors[table] = Factor(table, table_len[table], list(all_factor_pdfs[table].keys()),\n",
    "                                all_factor_pdfs[table], na_values[table])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "462f2dd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['title', 'kind_type', 'aka_title', 'person_info', 'info_type', 'movie_info_idx', 'movie_info', 'movie_companies', 'cast_info', 'movie_link', 'movie_keyword', 'complete_cast', 'name', 'aka_name', 'char_name', 'role_type', 'comp_cast_type', 'link_type', 'keyword', 'company_name', 'company_type'])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_factor_pdfs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "899412f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"new_table_buckets.pkl\", \"wb\") as f:\n",
    "    pickle.dump(new_table_buckets, f, pickle.HIGHEST_PROTOCOL)\n",
    "#with open(\"ground_truth_factors_no_filter.pkl\", \"wb\") as f:\n",
    " #   pickle.dump(all_factors, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d5450365",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"/home/ubuntu/data_CE/saved_models/bins.pkl\", \"rb\") as f:\n",
    "    bins = pickle.load(f)\n",
    "#with open(\"equivalent_keys.pkl\", \"rb\") as f:\n",
    " #   equivalent_keys = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "962f58d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['kind_type.id', 'info_type.id', 'title.id', 'name.id', 'char_name.id', 'role_type.id', 'comp_cast_type.id', 'keyword.id', 'company_name.id', 'company_type.id'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bins.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "28e6256d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kind_type.id 8 8\n",
      "kind_type.id 1 1 okay\n",
      "kind_type.id 1 1 okay\n",
      "kind_type.id 1 1 okay\n",
      "kind_type.id 1 1 okay\n",
      "kind_type.id 1 1 okay\n",
      "kind_type.id 1 1 okay\n",
      "kind_type.id 1 1 okay\n",
      "kind_type.id 1 1 okay\n",
      "info_type.id 72 72\n",
      "info_type.id 3 3 okay\n",
      "info_type.id 1 1 okay\n",
      "info_type.id 1 1 okay\n",
      "info_type.id 1 1 okay\n",
      "info_type.id 1 1 okay\n",
      "info_type.id 1 1 okay\n",
      "info_type.id 1 1 okay\n",
      "info_type.id 1 1 okay\n",
      "info_type.id 1 1 okay\n",
      "info_type.id 1 1 okay\n",
      "info_type.id 1 1 okay\n",
      "info_type.id 1 1 okay\n",
      "info_type.id 1 1 okay\n",
      "info_type.id 1 1 okay\n",
      "info_type.id 1 1 okay\n",
      "info_type.id 1 1 okay\n",
      "info_type.id 1 1 okay\n",
      "info_type.id 1 1 okay\n",
      "info_type.id 1 1 okay\n",
      "info_type.id 1 1 okay\n",
      "info_type.id 1 1 okay\n",
      "info_type.id 1 1 okay\n",
      "info_type.id 1 1 okay\n",
      "info_type.id 1 1 okay\n",
      "info_type.id 1 1 okay\n",
      "info_type.id 1 1 okay\n",
      "info_type.id 1 1 okay\n",
      "info_type.id 1 1 okay\n",
      "info_type.id 1 1 okay\n",
      "info_type.id 1 1 okay\n",
      "info_type.id 1 1 okay\n",
      "info_type.id 1 1 okay\n",
      "info_type.id 1 1 okay\n",
      "info_type.id 1 1 okay\n",
      "info_type.id 1 1 okay\n",
      "info_type.id 1 1 okay\n",
      "info_type.id 1 1 okay\n",
      "info_type.id 1 1 okay\n",
      "info_type.id 1 1 okay\n",
      "info_type.id 1 1 okay\n",
      "info_type.id 1 1 okay\n",
      "info_type.id 1 1 okay\n",
      "info_type.id 1 1 okay\n",
      "info_type.id 1 1 okay\n",
      "info_type.id 1 1 okay\n",
      "info_type.id 1 1 okay\n",
      "info_type.id 1 1 okay\n",
      "info_type.id 1 1 okay\n",
      "info_type.id 1 1 okay\n",
      "info_type.id 1 1 okay\n",
      "info_type.id 1 1 okay\n",
      "info_type.id 1 1 okay\n",
      "info_type.id 1 1 okay\n",
      "info_type.id 1 1 okay\n",
      "info_type.id 1 1 okay\n",
      "info_type.id 1 1 okay\n",
      "info_type.id 1 1 okay\n",
      "info_type.id 1 1 okay\n",
      "info_type.id 1 1 okay\n",
      "info_type.id 1 1 okay\n",
      "info_type.id 1 1 okay\n",
      "info_type.id 1 1 okay\n",
      "info_type.id 1 1 okay\n",
      "info_type.id 3 3 okay\n",
      "info_type.id 3 3 okay\n",
      "info_type.id 2 2 okay\n",
      "info_type.id 1 1 okay\n",
      "info_type.id 1 1 okay\n",
      "info_type.id 1 1 okay\n",
      "info_type.id 1 1 okay\n",
      "info_type.id 18 18 notokay\n",
      "info_type.id 18 18 okay\n",
      "title.id 88 74\n",
      "title.id 1333786 1255192 notokay\n",
      "title.id 111885 131222 notokay\n",
      "title.id 22656 34595 notokay\n",
      "title.id 9701 16930 notokay\n",
      "title.id 5152 10524 notokay\n",
      "title.id 3152 7475 notokay\n",
      "title.id 2111 5756 notokay\n",
      "title.id 1463 4737 notokay\n",
      "title.id 1156 3935 notokay\n",
      "title.id 914 3183 notokay\n",
      "title.id 680 2811 notokay\n",
      "title.id 516 2285 notokay\n",
      "title.id 387 1802 notokay\n",
      "title.id 276 1481 notokay\n",
      "title.id 200 1161 notokay\n",
      "title.id 173 1024 notokay\n",
      "title.id 137 794 notokay\n",
      "title.id 90 721 notokay\n",
      "title.id 96 627 notokay\n",
      "title.id 71 551 notokay\n",
      "title.id 45 469 notokay\n",
      "title.id 29 467 notokay\n",
      "title.id 22 370 notokay\n",
      "title.id 23 353 notokay\n",
      "title.id 15 359 notokay\n",
      "title.id 14 315 notokay\n",
      "title.id 13 299 notokay\n",
      "title.id 22 256 notokay\n",
      "title.id 17 247 notokay\n",
      "title.id 11 212 notokay\n",
      "title.id 5 196 notokay\n",
      "title.id 11 202 notokay\n",
      "title.id 4 161 notokay\n",
      "title.id 9 175 notokay\n",
      "title.id 3 157 notokay\n",
      "title.id 3 139 notokay\n",
      "title.id 4 149 notokay\n",
      "title.id 4 131 notokay\n",
      "title.id 3 119 notokay\n",
      "title.id 2 99 notokay\n",
      "title.id 3 98 notokay\n",
      "title.id 1 93 notokay\n",
      "title.id 3 94 notokay\n",
      "title.id 6 84 notokay\n",
      "title.id 2 81 notokay\n",
      "title.id 2 88 notokay\n",
      "title.id 1 74 notokay\n",
      "title.id 3 55 notokay\n",
      "title.id 4 61 notokay\n",
      "title.id 2 49 notokay\n",
      "title.id 2 49 notokay\n",
      "title.id 2 72 notokay\n",
      "title.id 2 39 notokay\n",
      "title.id 1 58 notokay\n",
      "title.id 1 35 notokay\n",
      "title.id 1 52 notokay\n",
      "title.id 2 30 notokay\n",
      "title.id 1 35 notokay\n",
      "title.id 2 27 notokay\n",
      "title.id 3 31 notokay\n",
      "title.id 2 26 notokay\n",
      "title.id 1 25 notokay\n",
      "title.id 1 36 notokay\n",
      "title.id 1 32 notokay\n",
      "title.id 1 26 notokay\n",
      "title.id 2 25 notokay\n",
      "title.id 1 22 notokay\n",
      "title.id 1 26 notokay\n",
      "title.id 1 23 notokay\n",
      "title.id 1 17 notokay\n",
      "title.id 1 18 notokay\n",
      "title.id 1 14 notokay\n",
      "title.id 1 13 notokay\n",
      "title.id 1 408920 notokay\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/bayescard/lib/python3.7/site-packages/ipykernel_launcher.py:5: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  \"\"\"\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-9b43d393559e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_bins\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;31m#print(key, len(temp_bins[key][i]), len(bins[key][i]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_bins\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_bins\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbins\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"okay\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "for key in temp_bins:\n",
    "    print(key, len(temp_bins[key]), len(bins[key]))\n",
    "    for i in range(len(temp_bins[key])):\n",
    "        #print(key, len(temp_bins[key][i]), len(bins[key][i]))\n",
    "        if np.all(temp_bins[key][i] == bins[key][i]):\n",
    "            print(key, len(temp_bins[key][i]), len(bins[key][i]), \"okay\")\n",
    "        else:\n",
    "            print(key, len(temp_bins[key][i]), len(bins[key][i]), \"notokay\")\n",
    "        #assert np.all(temp_bins[key][i] == bins[key][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa9bfb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "equivalent_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79350c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "[len(temp_bins[k]) for k in temp_bins]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5cbedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = temp['company_type.id']\n",
    "for k in bucket.buckets:\n",
    "    print(\"==============================================================\")\n",
    "    print(k, len(bucket.buckets[k].bin_modes), len(bucket.bins))\n",
    "    print(bucket.buckets[k].bin_modes)\n",
    "    print([len(b) for b in bucket.buckets[k].bins])\n",
    "    print(bucket.buckets[k].bin_means)\n",
    "    print(bucket.buckets[k].bin_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49973d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/Users/ziniuw/Desktop/past_research/End-to-End-CardEst-Benchmark/datasets/stats_simplified/{}.csv\"\n",
    "model_folder = \"../../CE_scheme_models\"\n",
    "data, null_values, key_attrs, table_buckets, equivalent_keys, schema, bin_size = process_stats_data(data_path,\n",
    "                                                                       model_folder, 200, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6868f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = \"votes\"\n",
    "bucket = table_buckets[table]\n",
    "for attr in bucket.id_attributes:\n",
    "    print(attr)\n",
    "    print(np.sum(bucket.oned_bin_modes[attr]), bucket.oned_bin_modes[attr].shape)\n",
    "    if len(bucket.twod_bin_modes[attr]) != 0:\n",
    "        print(np.sum(bucket.twod_bin_modes[attr]), bucket.twod_bin_modes[attr].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844f0b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = \"postLinks\"\n",
    "bucket = table_buckets[table]\n",
    "for attr in bucket.id_attributes:\n",
    "    print(attr)\n",
    "    print(np.sum(bucket.oned_bin_modes[attr]), bucket.oned_bin_modes[attr].shape)\n",
    "    if len(bucket.twod_bin_modes[attr]) != 0:\n",
    "        print(np.sum(bucket.twod_bin_modes[attr]), bucket.twod_bin_modes[attr].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba052cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bucket.oned_bin_modes[bucket.id_attributes[0]])\n",
    "print(np.sum(bucket.twod_bin_modes[bucket.id_attributes[0]], axis=1))\n",
    "print(bucket.oned_bin_modes[bucket.id_attributes[1]])\n",
    "print(np.sum(bucket.twod_bin_modes[bucket.id_attributes[1]], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22d89f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.arange(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5f6b427",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jenkspy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb6b3a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 1.0, 3.0, 5.0, 7.0, 9.0]\n"
     ]
    }
   ],
   "source": [
    "breaks = jenkspy.jenks_breaks(np.random.randint(0, 10, 200), nb_class=5)\n",
    "print(breaks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c87a86ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "218b16f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.mode(a).count[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f23196",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
