{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d492b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"/home/ubuntu/CE_scheme/\")\n",
    "from Schemas.imdb.schema import gen_imdb_schema\n",
    "from Join_scheme.data_prepare import read_table_csv\n",
    "from Join_scheme.bound import Bound_ensemble\n",
    "from Join_scheme.join_graph import parse_query_all_join, get_join_hyper_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336bbda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/home/ubuntu/data_CE/imdb/{}.csv\"\n",
    "schema = gen_imdb_schema(data_path)\n",
    "#BN = Bound_ensemble(None, None, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c24d7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_path = \"/home/ubuntu/data_CE/job/\"\n",
    "queries = []\n",
    "q_file_names = []\n",
    "for query_no in range(1, 34):\n",
    "    for suffix in ['a', 'b', 'c', 'd', 'e', 'f', 'g']:\n",
    "        file = f\"{query_no}{suffix}.sql\"\n",
    "        if file in os.listdir(query_path):\n",
    "            q_file_names.append(file.split(\".sql\")[0])\n",
    "            with open(query_path+file, \"r\") as f:\n",
    "                q = f.readline()\n",
    "                queries.append(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fc8f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_key_values(schema):\n",
    "    \"\"\"\n",
    "    identify all the key attributes from the schema of a DB, currently we assume all possible joins are known\n",
    "    It is also easy to support unseen joins, which we left as a future work.\n",
    "    :param schema: the schema of a DB\n",
    "    :return: a dict of all keys, {table: [keys]};\n",
    "             a dict of set, each indicating which keys on different tables are considered the same key.\n",
    "    \"\"\"\n",
    "    all_keys = set()\n",
    "    equivalent_keys = dict()\n",
    "    for i, join in enumerate(schema.relationships):\n",
    "        keys = join.identifier.split(\" = \")\n",
    "        all_keys.add(keys[0])\n",
    "        all_keys.add(keys[1])\n",
    "        seen = False\n",
    "        for k in equivalent_keys:\n",
    "            if keys[0] in equivalent_keys[k]:\n",
    "                equivalent_keys[k].add(keys[1])\n",
    "                seen = True\n",
    "                break\n",
    "            elif keys[1] in equivalent_keys[k]:\n",
    "                equivalent_keys[k].add(keys[0])\n",
    "                seen = True\n",
    "                break\n",
    "        if not seen:\n",
    "            # set the keys[-1] as the identifier of this equivalent join key group for convenience.\n",
    "            equivalent_keys[keys[-1]] = set(keys)\n",
    "\n",
    "    assert len(all_keys) == sum([len(equivalent_keys[k]) for k in equivalent_keys])\n",
    "    return all_keys, equivalent_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10939d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_keys, equivalent_keys = identify_key_values(schema)\n",
    "print(equivalent_keys)\n",
    "print(len(equivalent_keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3f6875",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_join_keys_stats = dict()\n",
    "for q in queries:\n",
    "    res = parse_query_all_join(q)\n",
    "    for table in res[-1]:\n",
    "        for join_key in list(res[-1][table]):\n",
    "            for PK in equivalent_keys:\n",
    "                indicator = False\n",
    "                if join_key in equivalent_keys[PK]:\n",
    "                    if PK in all_join_keys_stats:\n",
    "                        all_join_keys_stats[PK] += 1\n",
    "                    else:\n",
    "                        all_join_keys_stats[PK] = 1\n",
    "                    indicator = True\n",
    "                    break\n",
    "            if not indicator:\n",
    "                print(join_key)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad7fc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_join_keys_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d8546e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bins = {\n",
    "    'title.id': 800,\n",
    "    'info_type.id': 100,\n",
    "    'keyword.id': 100,\n",
    "    'company_name.id': 100,\n",
    "    'name.id': 100,\n",
    "    'company_type.id': 100,\n",
    "    'comp_cast_type.id': 50,\n",
    "    'kind_type.id': 50,\n",
    "    'char_name.id': 50,\n",
    "    'role_type.id': 50,\n",
    "    'link_type.id': 50\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d65952d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sample(np_data, nrows=1000000, seed=0):\n",
    "    np.random.seed(seed)\n",
    "    samp_data = np_data[np_data != -1]\n",
    "    if len(samp_data) <= nrows:\n",
    "        return samp_data, 1.0\n",
    "    else:\n",
    "        selected = np.random.choice(len(samp_data), size=nrows, replace=False)\n",
    "        return samp_data[selected], nrows/len(samp_data)\n",
    "\n",
    "def stats_analysis(sample, data, sample_rate, show=10):\n",
    "    n, c = np.unique(sample, return_counts=True)\n",
    "    idx = np.argsort(c)[::-1]\n",
    "    for i in range(min(show, len(idx))):\n",
    "        print(c[idx[i]], c[idx[i]]/sample_rate, len(data[data == n[idx[i]]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b86b246",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dict()\n",
    "table_len = dict()\n",
    "na_values = dict()\n",
    "sample_rate = dict()\n",
    "primary_keys = []\n",
    "for table_obj in schema.tables:\n",
    "    df_rows = pd.read_csv(table_obj.csv_file_location, header=None, escapechar='\\\\', encoding='utf-8', quotechar='\"',\n",
    "                          sep=\",\")\n",
    "    \n",
    "    df_rows.columns = [table_obj.table_name + '.' + attr for attr in table_obj.attributes]\n",
    "\n",
    "    for attribute in table_obj.irrelevant_attributes:\n",
    "        df_rows = df_rows.drop(table_obj.table_name + '.' + attribute, axis=1)\n",
    "\n",
    "    df_rows.apply(pd.to_numeric, errors=\"ignore\")\n",
    "    table_len[table_obj.table_name] = len(df_rows)\n",
    "    if table_obj.table_name not in na_values:\n",
    "        na_values[table_obj.table_name] = dict()\n",
    "    for attr in df_rows.columns:\n",
    "        if attr in all_keys:\n",
    "            print(attr)\n",
    "            print(np.sum(np.isnan(df_rows[attr].values)))\n",
    "            data[attr] = df_rows[attr].values\n",
    "            data[attr][np.isnan(data[attr])] = -1\n",
    "            data[attr][data[attr] < 0] = -1\n",
    "            na_values[table_obj.table_name][attr] = len(data[attr][data[attr] != -1])/table_len[table_obj.table_name]\n",
    "            print(len(data[attr]), na_values[table_obj.table_name][attr])\n",
    "            data[attr] = copy.deepcopy(data[attr])[data[attr]>=0]\n",
    "            if len(np.unique(data[attr])) >= len(data[attr]) - 10:\n",
    "                primary_keys.append(attr)\n",
    "            print(np.sum(np.isnan(df_rows[attr].values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1294f672",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_rate = dict()\n",
    "sampled_data = dict()\n",
    "for k in data:\n",
    "    print(k)\n",
    "    temp = make_sample(data[k], 1000000)\n",
    "    stats_analysis(temp[0], data[k], temp[1])\n",
    "    sampled_data[k] = temp[0]\n",
    "    sample_rate[k] = temp[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3302651",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "from scipy import stats\n",
    "import jenkspy\n",
    "\n",
    "\n",
    "class Bucket:\n",
    "    \"\"\"\n",
    "    The class of bucketization of a key attribute\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name, bins=[], bin_modes=[], bin_vars=[], bin_means=[], rest_bins_remaining=None):\n",
    "        self.name = name\n",
    "        self.bins = bins\n",
    "        self.bin_modes = bin_modes\n",
    "        self.bin_vars = bin_vars\n",
    "        self.bin_means = bin_means\n",
    "        self.rest_bins_remaining = rest_bins_remaining\n",
    "        if len(bins) != 0:\n",
    "            assert len(bins) == len(bin_modes)\n",
    "\n",
    "\n",
    "class Table_bucket:\n",
    "    \"\"\"\n",
    "    The class of bucketization for all key attributes in a table.\n",
    "    Supporting more than three dimensional bin modes requires simplifying the causal structure\n",
    "    \"\"\"\n",
    "    def __init__(self, table_name, id_attributes, bin_sizes, oned_bin_modes=None):\n",
    "        self.table_name = table_name\n",
    "        self.id_attributes = id_attributes\n",
    "        self.bin_sizes = bin_sizes\n",
    "        if oned_bin_modes:\n",
    "            self.oned_bin_modes = oned_bin_modes\n",
    "        else:\n",
    "            self.oned_bin_modes = dict()\n",
    "        self.twod_bin_modes = dict()\n",
    "\n",
    "\n",
    "class Bucket_group:\n",
    "    \"\"\"\n",
    "    The class of bucketization for a group of equivalent join keys\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, buckets, start_key, sample_rate, bins=None, primary_keys=[]):\n",
    "        self.buckets = buckets\n",
    "        self.start_key = start_key\n",
    "        self.sample_rate = sample_rate\n",
    "        self.bins = bins\n",
    "        self.primary_keys = primary_keys\n",
    "\n",
    "    def bucketize(self, data):\n",
    "        \"\"\"\n",
    "        Discretize data based on the bucket\n",
    "        \"\"\"\n",
    "        res = dict()\n",
    "        seen_remain_key = np.array([])\n",
    "        cumulative_bin = copy.deepcopy(self.buckets[self.start_key].bins)\n",
    "        start_means = np.asarray(self.buckets[self.start_key].bin_means)\n",
    "\n",
    "        for key in data:\n",
    "            if key in self.primary_keys:\n",
    "                continue\n",
    "            res[key] = copy.deepcopy(data[key])\n",
    "            if key != self.start_key:\n",
    "                unique_remain = np.setdiff1d(self.buckets[key].rest_bins_remaining, seen_remain_key)\n",
    "                assert sum([np.sum(np.isin(unique_remain, b) == 1) for b in cumulative_bin]) == 0\n",
    "\n",
    "                if len(unique_remain) != 0:\n",
    "                    remaining_data = data[key][np.isin(data[key], unique_remain)]\n",
    "                    unique_remain, count_remain = np.unique(remaining_data, return_counts=True)\n",
    "                    unique_counts = np.unique(count_remain)\n",
    "                    for u in unique_counts:\n",
    "                        temp_idx = np.searchsorted(start_means, u)\n",
    "                        if temp_idx == len(cumulative_bin):\n",
    "                            idx = -1\n",
    "                            if u > self.buckets[key].bin_modes[-1]:\n",
    "                                self.buckets[key].bin_modes[-1] = u\n",
    "                        elif temp_idx == 0:\n",
    "                            idx = 0\n",
    "                        else:\n",
    "                            if (u - start_means[temp_idx - 1]) >= (start_means[temp_idx] - u):\n",
    "                                idx = temp_idx - 1\n",
    "                            else:\n",
    "                                idx = temp_idx\n",
    "                        temp_unique = unique_remain[count_remain == u]\n",
    "                        cumulative_bin[idx] = np.concatenate((cumulative_bin[idx], temp_unique))\n",
    "                        seen_remain_key = np.concatenate((seen_remain_key, temp_unique))\n",
    "                        if u > self.buckets[key].bin_modes[idx]:\n",
    "                            self.buckets[key].bin_modes[idx] = u\n",
    "            res[key] = copy.deepcopy(data[key])\n",
    "            count = 0\n",
    "            for i, b in enumerate(cumulative_bin):\n",
    "                count += len(data[key][np.isin(data[key], b)])\n",
    "                res[key][np.isin(data[key], b)] = i\n",
    "\n",
    "        self.bins = cumulative_bin\n",
    "\n",
    "        for key in data:\n",
    "            if key in self.primary_keys:\n",
    "                res[key] = self.bucketize_PK(data[key])\n",
    "                self.buckets[key] = Bucket(key, bin_modes=np.ones(len(self.bins)))\n",
    "        \n",
    "        for key in data:\n",
    "            if key in self.primary_keys:\n",
    "                continue\n",
    "            if self.sample_rate[key] < 1.0:\n",
    "                bin_modes = np.asarray(self.buckets[key].bin_modes)\n",
    "                bin_modes[bin_modes != 1] = bin_modes[bin_modes != 1] / self.sample_rate[key]\n",
    "                self.buckets[key].bin_modes = bin_modes\n",
    "        \n",
    "        return res\n",
    "\n",
    "    def bucketize_PK(self, data):\n",
    "        res = copy.deepcopy(data)\n",
    "        remaining_data = np.unique(data)\n",
    "        for i, b in enumerate(self.bins):\n",
    "            res[np.isin(data, b)] = i\n",
    "            remaining_data = np.setdiff1d(remaining_data, b)\n",
    "        if len(remaining_data) != 0:\n",
    "            self.bins.append(list(remaining_data))\n",
    "            for key in self.buckets:\n",
    "                if key not in self.primary_keys:\n",
    "                    self.buckets[key].bin_modes = np.append(self.buckets[key].bin_modes, 0)\n",
    "        res[np.isin(data, remaining_data)] = len(self.bins)\n",
    "        return res\n",
    "\n",
    "\n",
    "\n",
    "def identify_key_values(schema):\n",
    "    \"\"\"\n",
    "    identify all the key attributes from the schema of a DB, currently we assume all possible joins are known\n",
    "    It is also easy to support unseen joins, which we left as a future work.\n",
    "    :param schema: the schema of a DB\n",
    "    :return: a dict of all keys, {table: [keys]};\n",
    "             a dict of set, each indicating which keys on different tables are considered the same key.\n",
    "    \"\"\"\n",
    "    all_keys = set()\n",
    "    equivalent_keys = dict()\n",
    "    for i, join in enumerate(schema.relationships):\n",
    "        keys = join.identifier.split(\" = \")\n",
    "        all_keys.add(keys[0])\n",
    "        all_keys.add(keys[1])\n",
    "        seen = False\n",
    "        for k in equivalent_keys:\n",
    "            if keys[0] in equivalent_keys[k]:\n",
    "                equivalent_keys[k].add(keys[1])\n",
    "                seen = True\n",
    "                break\n",
    "            elif keys[1] in equivalent_keys[k]:\n",
    "                equivalent_keys[k].add(keys[0])\n",
    "                seen = True\n",
    "                break\n",
    "        if not seen:\n",
    "            # set the keys[-1] as the identifier of this equivalent join key group for convenience.\n",
    "            equivalent_keys[keys[-1]] = set(keys)\n",
    "\n",
    "    assert len(all_keys) == sum([len(equivalent_keys[k]) for k in equivalent_keys])\n",
    "    return all_keys, equivalent_keys\n",
    "\n",
    "\n",
    "def equal_freq_binning(name, data, n_bins, data_len, return_bucket=True):\n",
    "    uniques, counts = data\n",
    "    unique_counts, count_counts = np.unique(counts, return_counts=True)\n",
    "    idx = np.argsort(unique_counts)\n",
    "    unique_counts = unique_counts[idx]\n",
    "    count_counts = count_counts[idx]\n",
    "\n",
    "    bins = []\n",
    "    bin_modes = []\n",
    "    bin_vars = []\n",
    "    bin_means = []\n",
    "\n",
    "    bin_freq = data_len / n_bins\n",
    "    cur_freq = 0\n",
    "    cur_bin = []\n",
    "    cur_bin_count = []\n",
    "    for i, uni_c in enumerate(unique_counts):\n",
    "        cur_freq += count_counts[i] * uni_c\n",
    "        cur_bin.append(uniques[np.where(counts == uni_c)[0]])\n",
    "        cur_bin_count.extend([uni_c] * count_counts[i])\n",
    "        if (cur_freq > bin_freq) or (i == (len(unique_counts) - 1)):\n",
    "            bins.append(np.concatenate(cur_bin))\n",
    "            cur_bin_count = np.asarray(cur_bin_count)\n",
    "            bin_modes.append(uni_c)\n",
    "            bin_means.append(np.mean(cur_bin_count))\n",
    "            bin_vars.append(np.var(cur_bin_count))\n",
    "            cur_freq = 0\n",
    "            cur_bin = []\n",
    "            cur_bin_count = []\n",
    "    assert len(uniques) == sum([len(b) for b in bins]), f\"some unique values missed or duplicated\"\n",
    "    if return_bucket:\n",
    "        return Bucket(name, bins, bin_modes, bin_vars, bin_means)\n",
    "    else:\n",
    "        return bins, bin_means\n",
    "\n",
    "    \n",
    "def apply_binning_to_data(bins, bin_means, data, start_key_data, n_bins, uniques, counts):\n",
    "    # apply one greedy binning step based on existing bins\n",
    "    unique_remains = np.setdiff1d(uniques, np.concatenate(bins))\n",
    "    if len(unique_remains) != 0:\n",
    "        remaining_data = data[np.isin(data, unique_remains)]\n",
    "        unique_remain, count_remain = np.unique(remaining_data, return_counts=True)\n",
    "        unique_counts = np.unique(count_remain)\n",
    "        for u in unique_counts:\n",
    "            temp_idx = np.searchsorted(bin_means, u)\n",
    "            if temp_idx == len(bins):\n",
    "                idx = -1\n",
    "            elif temp_idx == 0:\n",
    "                idx = 0\n",
    "            else:\n",
    "                if (u - bin_means[temp_idx - 1]) >= (bin_means[temp_idx] - u):\n",
    "                    idx = temp_idx - 1\n",
    "                else:\n",
    "                    idx = temp_idx\n",
    "            temp_unique = unique_remain[count_remain == u]\n",
    "            bins[idx] = np.concatenate((bins[idx], temp_unique))  # modifying bins in place\n",
    "\n",
    "    bin_vars = []\n",
    "    temp_bin_means = []\n",
    "    for i, bin in enumerate(bins):\n",
    "        idx = np.where(np.isin(uniques, bin) == 1)[0]\n",
    "        if len(idx) != 0:\n",
    "            bin_vars.append(np.var(counts[idx]))\n",
    "            temp_bin_means.append(np.mean(counts[idx]))\n",
    "        else:\n",
    "            bin_vars.append(0)\n",
    "            temp_bin_means.append(1)\n",
    "\n",
    "    assign_nbins = assign_bins_by_var(n_bins, bin_vars, temp_bin_means)\n",
    "\n",
    "    new_bins = []\n",
    "    new_bin_means = []\n",
    "    for i, bin in enumerate(bins):\n",
    "        if assign_nbins[i] == 0:\n",
    "            new_bins.append(bin)\n",
    "            new_bin_means.append(bin_means[i])\n",
    "        else:\n",
    "            curr_bin_data = data[np.isin(data, bin)]\n",
    "            curr_start_key_data = start_key_data[np.isin(start_key_data, bin)]\n",
    "            curr_bins, curr_bin_means = divide_bin(bin, curr_bin_data, assign_nbins[i] + 1, curr_start_key_data)\n",
    "            new_bins.extend(curr_bins)\n",
    "            new_bin_means.extend(curr_bin_means)\n",
    "\n",
    "    return new_bins, new_bin_means\n",
    "\n",
    "\n",
    "def assign_bins_by_var(n_bins, bin_vars, bin_means, small_threshold=0.2, large_threshold=2):\n",
    "    assign_nbins = np.zeros(len(bin_vars))\n",
    "    remaining_nbins = n_bins\n",
    "    idx = np.argsort(bin_vars)[::-1]\n",
    "    if bin_vars[idx[0]] / bin_means[idx[0]] <= small_threshold:\n",
    "        return assign_nbins\n",
    "\n",
    "    while remaining_nbins > 0:\n",
    "        for i in range(len(assign_nbins)):\n",
    "            normalized_var = bin_vars[idx[i]] / bin_means[idx[i]]\n",
    "            if normalized_var >= large_threshold:\n",
    "                assign_nbins[i] += min(remaining_nbins, 2)\n",
    "                remaining_nbins -= min(remaining_nbins, 2)\n",
    "            elif normalized_var > small_threshold:\n",
    "                assign_nbins[i] += 1\n",
    "                remaining_nbins -= 1\n",
    "            if remaining_nbins <= 0:\n",
    "                break\n",
    "    return assign_nbins\n",
    "\n",
    "\n",
    "def divide_bin(bin, curr_bin_data, n_bins, start_key_data):\n",
    "    # divide one bin into multiple bins to minimize the variance of curr_bin_data\n",
    "    uniques, counts = np.unique(curr_bin_data, return_counts=True)\n",
    "    if len(uniques) == 0:\n",
    "        return [], []\n",
    "\n",
    "    if len(uniques) <= n_bins:\n",
    "        new_bins = []\n",
    "        bin_means = []\n",
    "        remaining_values = bin\n",
    "\n",
    "        for i, uni in enumerate(uniques):\n",
    "            new_bins.append([uni])\n",
    "            remaining_values = np.setdiff1d(remaining_values, np.asarray([uni]))\n",
    "\n",
    "        # randomly assign the remaining index to some bins\n",
    "        if len(remaining_values) > 0:\n",
    "            assign_idx = np.random.randint(0, len(new_bins), size=len(remaining_values))\n",
    "            for i in range(len(new_bins)):\n",
    "                new_bins[i].extend(list(remaining_values[assign_idx == i]))\n",
    "                new_bins[i] = np.asarray(new_bins[i])\n",
    "\n",
    "        for bin in new_bins:\n",
    "            curr_bin_data = start_key_data[np.isin(start_key_data, bin)]\n",
    "            if len(curr_bin_data) == 0:\n",
    "                bin_means.append(0)\n",
    "            else:\n",
    "                _, count = np.unique(curr_bin_data, return_counts=True)\n",
    "                bin_means.append(np.mean(count))\n",
    "        return new_bins, bin_means\n",
    "\n",
    "    idx = np.argsort(counts)\n",
    "    counts = counts[idx]\n",
    "    uniques = uniques[idx]\n",
    "\n",
    "    # Natural breaks optimization using Fisher-Jenks Algorithms\n",
    "    breaks = jenkspy.jenks_breaks(counts, nb_class=n_bins)\n",
    "    breaks[-1] += 0.01\n",
    "    new_bins = []\n",
    "    bin_means = []\n",
    "    remaining_values = np.asarray(bin)\n",
    "    for i in range(1, len(breaks)):\n",
    "        idx = np.where((breaks[i - 1] <= counts) & (counts < breaks[i]))[0]\n",
    "        new_bins.append(uniques[idx])\n",
    "        remaining_values = np.setdiff1d(remaining_values, uniques[idx])\n",
    "\n",
    "    if len(remaining_values) > 0:\n",
    "        assign_idx = np.random.randint(0, len(new_bins), size=len(remaining_values))\n",
    "        for i in range(len(new_bins)):\n",
    "            new_bins[i] = np.concatenate((new_bins[i], remaining_values[assign_idx == i]))\n",
    "\n",
    "    for bin in new_bins:\n",
    "        curr_bin_data = start_key_data[np.isin(start_key_data, bin)]\n",
    "        if len(curr_bin_data) == 0:\n",
    "            bin_means.append(0)\n",
    "        else:\n",
    "            _, count = np.unique(curr_bin_data, return_counts=True)\n",
    "            bin_means.append(np.mean(count))\n",
    "    return new_bins, bin_means\n",
    "    \n",
    "\n",
    "def compute_variance_score(buckets):\n",
    "    \"\"\"\n",
    "    compute the variance of products of random variables\n",
    "    \"\"\"\n",
    "    all_mean = np.asarray([buckets[k].bin_means for k in buckets])\n",
    "    all_var = np.asarray([buckets[k].bin_vars for k in buckets])\n",
    "    return np.sum(np.prod(all_var + all_mean ** 2, axis=0) - np.prod(all_mean, axis=0) ** 2)\n",
    "\n",
    "\n",
    "def greedy_bucketize(data, sample_rate, n_bins=30, primary_keys=[], return_data=False):\n",
    "    \"\"\"\n",
    "    Perform sub-optimal bucketization on a group of equivalent join keys.\n",
    "    A greedy algorithm that assigns half of the bins to one key at a time.\n",
    "    :param data: a dict of (potentially sampled) table data of the keys\n",
    "                 the keys of this dict are one group of equivalent join keys\n",
    "    :param sample_rate: the sampling rate the data, could be all 1 if no sampling is performed\n",
    "    :param n_bins: how many bins can we allocate\n",
    "    :param primary_keys: the primary keys in the equivalent group since we don't need to bucketize PK.\n",
    "    :return: new data, where the keys are bucketized\n",
    "             the mode of each bucket\n",
    "    \"\"\"\n",
    "    unique_values = dict()\n",
    "    key_orders = []\n",
    "    data_lens = []\n",
    "    curr_pk = []\n",
    "    for key in data:\n",
    "        if key not in primary_keys:\n",
    "            unique_values[key] = np.unique(data[key], return_counts=True)\n",
    "            key_orders.append(key)\n",
    "            data_lens.append(len(data[key]))\n",
    "        else:\n",
    "            curr_pk.append(key)\n",
    "    key_orders = [key_orders[i] for i in np.argsort(data_lens)[::-1]]\n",
    "    print(key_orders)\n",
    "    print(curr_pk)\n",
    "    remaining_bins = n_bins\n",
    "    start_key = key_orders[0]\n",
    "    curr_bins = None\n",
    "    curr_bin_means = None\n",
    "    for key in key_orders:\n",
    "        print(key)\n",
    "        if key == key_orders[-1]:\n",
    "            # least key value use up all remaining bins, otherwise use half of it\n",
    "            assign_bins = remaining_bins\n",
    "        else:\n",
    "            assign_bins = remaining_bins // 2\n",
    "        if key == start_key:\n",
    "            curr_bins, curr_bin_means = equal_freq_binning(key, unique_values[key], assign_bins, len(data[key]), False)\n",
    "        else:\n",
    "            curr_bins, curr_bin_means = apply_binning_to_data(curr_bins, curr_bin_means, data[key],\n",
    "                                                              data[start_key], assign_bins,\n",
    "                                                              unique_values[key][0], unique_values[key][1])\n",
    "        print(len(curr_bins), len(curr_bin_means))\n",
    "        remaining_bins = n_bins - len(curr_bins)\n",
    "\n",
    "    new_data, best_buckets, curr_bins = bin_all_data_with_existing_binning(curr_bins, data, sample_rate, curr_pk,\n",
    "                                                                           return_data)\n",
    "    best_buckets = Bucket_group(best_buckets, start_key, sample_rate, curr_bins, primary_keys=curr_pk)\n",
    "    return new_data, best_buckets\n",
    "\n",
    "\n",
    "\n",
    "def sub_optimal_bucketize(data, sample_rate, n_bins=30, primary_keys=[]):\n",
    "    \"\"\"\n",
    "    Perform sub-optimal bucketization on a group of equivalent join keys.\n",
    "    :param data: a dict of (potentially sampled) table data of the keys\n",
    "                 the keys of this dict are one group of equivalent join keys\n",
    "    :param sample_rate: the sampling rate the data, could be all 1 if no sampling is performed\n",
    "    :param n_bins: how many bins can we allocate\n",
    "    :param primary_keys: the primary keys in the equivalent group since we don't need to bucketize PK.\n",
    "    :return: new data, where the keys are bucketized\n",
    "             the mode of each bucket\n",
    "    \"\"\"\n",
    "    unique_values = dict()\n",
    "    for key in data:\n",
    "        if key not in primary_keys:\n",
    "            unique_values[key] = np.unique(data[key], return_counts=True)\n",
    "\n",
    "    best_variance_score = np.infty\n",
    "    best_bin_len = 0\n",
    "    best_start_key = None\n",
    "    best_buckets = None\n",
    "    for start_key in data:\n",
    "        if start_key in primary_keys:\n",
    "            continue\n",
    "        start_bucket = equal_freq_binning(start_key, unique_values[start_key], n_bins, len(data[start_key]))\n",
    "        rest_buckets = dict()\n",
    "        for key in data:\n",
    "            if key == start_key or key in primary_keys:\n",
    "                continue\n",
    "            uniques = unique_values[key][0]\n",
    "            counts = unique_values[key][1]\n",
    "            rest_buckets[key] = Bucket(key, [], [0] * len(start_bucket.bins), [0] * len(start_bucket.bins),\n",
    "                                       [0] * len(start_bucket.bins), uniques)\n",
    "            for i, bin in enumerate(start_bucket.bins):\n",
    "                idx = np.where(np.isin(uniques, bin) == 1)[0]\n",
    "                if len(idx) != 0:\n",
    "                    bin_count = counts[idx]\n",
    "                    unique_bin_keys = uniques[idx]\n",
    "                    # unique_bin_count = np.unique(bin_count)\n",
    "                    # bin_count = np.concatenate([counts[counts == j] for j in unique_bin_count])\n",
    "                    # unique_bin_keys = np.concatenate([uniques[counts == j] for j in unique_bin_count])\n",
    "                    rest_buckets[key].rest_bins_remaining = np.setdiff1d(rest_buckets[key].rest_bins_remaining,\n",
    "                                                                         unique_bin_keys)\n",
    "                    rest_buckets[key].bin_modes[i] = np.max(bin_count)\n",
    "                    rest_buckets[key].bin_vars[i] = np.var(bin_count)\n",
    "                    rest_buckets[key].bin_means[i] = np.mean(bin_count)\n",
    "\n",
    "        rest_buckets[start_key] = start_bucket\n",
    "        var_score = compute_variance_score(rest_buckets)\n",
    "        if len(start_bucket.bins) > best_bin_len:\n",
    "            best_variance_score = var_score\n",
    "            best_start_key = start_key\n",
    "            best_buckets = rest_buckets\n",
    "            best_bin_len = len(start_bucket.bins)\n",
    "        elif len(start_bucket.bins) >= best_bin_len * 0.9 and var_score < best_variance_score:\n",
    "            best_variance_score = var_score\n",
    "            best_start_key = start_key\n",
    "            best_buckets = rest_buckets\n",
    "            best_bin_len = len(start_bucket.bins)\n",
    "    \n",
    "    best_buckets = Bucket_group(best_buckets, best_start_key, sample_rate, primary_keys=primary_keys)\n",
    "    new_data = best_buckets.bucketize(data)\n",
    "    return new_data, best_buckets\n",
    "\n",
    "\n",
    "def fixed_start_key_bucketize(start_key, data, sample_rate, n_bins=30, primary_keys=[]):\n",
    "    \"\"\"\n",
    "    Perform sub-optimal bucketization on a group of equivalent join keys based on the pre-defined start_key.\n",
    "    :param data: a dict of (potentially sampled) table data of the keys\n",
    "                 the keys of this dict are one group of equivalent join keys\n",
    "    :param sample_rate: the sampling rate the data, could be all 1 if no sampling is performed\n",
    "    :param n_bins: how many bins can we allocate\n",
    "    :param primary_keys: the primary keys in the equivalent group since we don't need to bucketize PK.\n",
    "    :return: new data, where the keys are bucketized\n",
    "             the mode of each bucket\n",
    "    \"\"\"\n",
    "    unique_values = dict()\n",
    "    for key in data:\n",
    "        if key not in primary_keys:\n",
    "            unique_values[key] = np.unique(data[key], return_counts=True)\n",
    "\n",
    "    start_bucket = equal_freq_binning(start_key, unique_values[start_key], n_bins, len(data[start_key]))\n",
    "    rest_buckets = dict()\n",
    "    for key in data:\n",
    "        if key == start_key or key in primary_keys:\n",
    "            continue\n",
    "        uniques = unique_values[key][0]\n",
    "        counts = unique_values[key][1]\n",
    "        rest_buckets[key] = Bucket(key, [], [0] * len(start_bucket.bins), [0] * len(start_bucket.bins),\n",
    "                                   [0] * len(start_bucket.bins), uniques)\n",
    "        for i, bin in enumerate(start_bucket.bins):\n",
    "            idx = np.where(np.isin(uniques, bin) == 1)[0]\n",
    "            if len(idx) != 0:\n",
    "                bin_count = counts[idx]\n",
    "                unique_bin_keys = uniques[idx]\n",
    "                rest_buckets[key].rest_bins_remaining = np.setdiff1d(rest_buckets[key].rest_bins_remaining,\n",
    "                                                                     unique_bin_keys)\n",
    "                rest_buckets[key].bin_modes[i] = np.max(bin_count)\n",
    "                rest_buckets[key].bin_means[i] = np.mean(bin_count)\n",
    "\n",
    "    best_buckets = Bucket_group(rest_buckets, start_key, sample_rate, primary_keys=primary_keys)\n",
    "    new_data = best_buckets.bucketize(data)\n",
    "    return new_data, best_buckets\n",
    "\n",
    "\n",
    "def bin_all_data_with_existing_binning(bins, data, sample_rate, curr_pk, return_data):\n",
    "    buckets = dict()\n",
    "    new_data = dict()\n",
    "    if return_data:\n",
    "        new_data = copy.deepcopy(data)\n",
    "\n",
    "    for key in curr_pk:\n",
    "        bin_modes = [1 for i in range(len(bins))]\n",
    "        remaining_data = np.unique(data[key])\n",
    "        for i, bin in enumerate(bins):\n",
    "            if return_data:\n",
    "                new_data[key][np.isin(data[key], bin)] = i\n",
    "            remaining_data = np.setdiff1d(remaining_data, bin)\n",
    "        if len(remaining_data) != 0:\n",
    "            if return_data:\n",
    "                # assigning all remaining key values to the first bin\n",
    "                new_data[key][np.isin(data[key], remaining_data)] = 0\n",
    "            bins[0] = np.concatenate((bins[0], remaining_data))\n",
    "        buckets[key] = Bucket(key, bin_modes=bin_modes)\n",
    "\n",
    "    for key in data:\n",
    "        bin_modes = []\n",
    "        for i, bin in enumerate(bins):\n",
    "            curr_data = data[key][np.isin(data[key], bin)]\n",
    "            if len(curr_data) == 0:\n",
    "                bin_modes.append(0)\n",
    "            else:\n",
    "                bin_mode = stats.mode(curr_data).count[0]\n",
    "                if bin_mode > 1:\n",
    "                    bin_mode /= sample_rate[key]\n",
    "                bin_modes.append(bin_mode)\n",
    "                if return_data:\n",
    "                    new_data[key][np.isin(data[key], bin)] = i\n",
    "        buckets[key] = Bucket(key, bin_modes=bin_modes)\n",
    "\n",
    "    return new_data, buckets, bins\n",
    "\n",
    "\n",
    "def apply_binning_to_data_value_count(bins, data):\n",
    "    res = np.zeros(len(bins))\n",
    "    unique_remain = np.unique(data)\n",
    "    for i, bin in enumerate(bins):\n",
    "        res[i] = np.sum(np.isin(data, bin))\n",
    "        unique_remain = np.setdiff1d(unique_remain, bin)\n",
    "\n",
    "    res[0] += np.sum(np.isin(data, unique_remain))\n",
    "    return res\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3349d510",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = gen_imdb_schema(data_path)\n",
    "all_keys, equivalent_keys = identify_key_values(schema)\n",
    "data = dict()\n",
    "primary_keys = []\n",
    "for table_obj in schema.tables:\n",
    "    df_rows = pd.read_csv(table_obj.csv_file_location, header=None, escapechar='\\\\', encoding='utf-8',\n",
    "                          quotechar='\"',\n",
    "                          sep=\",\")\n",
    "\n",
    "    df_rows.columns = [table_obj.table_name + '.' + attr for attr in table_obj.attributes]\n",
    "\n",
    "    for attribute in table_obj.irrelevant_attributes:\n",
    "        df_rows = df_rows.drop(table_obj.table_name + '.' + attribute, axis=1)\n",
    "\n",
    "    df_rows.apply(pd.to_numeric, errors=\"ignore\")\n",
    "    for attr in df_rows.columns:\n",
    "        if attr in all_keys:\n",
    "            data[attr] = df_rows[attr].values\n",
    "            data[attr][np.isnan(data[attr])] = -1\n",
    "            data[attr][data[attr] < 0] = -1\n",
    "            data[attr] = copy.deepcopy(data[attr])[data[attr] >= 0]\n",
    "            if len(np.unique(data[attr])) >= len(data[attr]) - 10:\n",
    "                primary_keys.append(attr)\n",
    "\n",
    "sample_rate = dict()\n",
    "sampled_data = dict()\n",
    "for k in data:\n",
    "    temp = make_sample(data[k], 1000000)\n",
    "    sampled_data[k] = temp[0]\n",
    "    sample_rate[k] = temp[1]\n",
    "\n",
    "optimal_buckets = dict()\n",
    "bin_size = dict()\n",
    "all_bin_modes = dict()\n",
    "for PK in equivalent_keys:\n",
    "    # if PK != 'kind_type.id':\n",
    "    #   continue\n",
    "    group_data = {}\n",
    "    group_sample_rate = {}\n",
    "    for K in equivalent_keys[PK]:\n",
    "        group_data[K] = sampled_data[K]\n",
    "        group_sample_rate[K] = sample_rate[K]\n",
    "    _, optimal_bucket = greedy_bucketize(group_data, group_sample_rate, n_bins=n_bins[PK], primary_keys=primary_keys, return_data=False)\n",
    "    optimal_buckets[PK] = optimal_bucket\n",
    "    for K in equivalent_keys[PK]:\n",
    "        temp_table_name = K.split(\".\")[0]\n",
    "        if temp_table_name not in bin_size:\n",
    "            bin_size[temp_table_name] = dict()\n",
    "            all_bin_modes[temp_table_name] = dict()\n",
    "        bin_size[temp_table_name][K] = len(optimal_bucket.bins)\n",
    "        all_bin_modes[temp_table_name][K] = optimal_bucket.buckets[K].bin_modes\n",
    "\n",
    "table_buckets = dict()\n",
    "for table_name in bin_size:\n",
    "    table_buckets[table_name] = Table_bucket(table_name, list(bin_size[table_name].keys()), bin_size[table_name],\n",
    "                                             all_bin_modes[table_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32238d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_bins = dict()\n",
    "for key in optimal_buckets:\n",
    "    temp_bins[key] = optimal_buckets[key].bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffcdc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_bins.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfea0d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"table_buckets.pkl\", \"rb\") as f:\n",
    "    old_table_buckets = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fb6982",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_table_buckets = dict()\n",
    "for key in table_buckets:\n",
    "    if key in old_table_buckets:\n",
    "        new_table_buckets[key] = old_table_buckets[key]\n",
    "    else:\n",
    "        new_table_buckets[key] = table_buckets[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9131967f",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_table_buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3bcc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_binning_to_data_value_count(bins, data):\n",
    "    res = np.zeros(len(bins))\n",
    "    bin_mode = np.zeros(len(bins))\n",
    "    unique_remain = np.unique(data)\n",
    "    for i, bin in enumerate(bins):\n",
    "        data_bin = data[np.isin(data, bin)]\n",
    "        res[i] = np.sum(np.isin(data, bin))\n",
    "        unique_remain = np.setdiff1d(unique_remain, bin)\n",
    "        _, counts = np.unique(data_bin, return_counts=True)\n",
    "        if len(counts) == 0:\n",
    "            bin_mode[i] = 0\n",
    "        else:\n",
    "            bin_mode[i] = np.max(counts)\n",
    "    res[0] += np.sum(np.isin(data, unique_remain))\n",
    "    return bin_mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd2642a",
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = apply_binning_to_data_value_count(bins['title.id'], data['movie_link.linked_movie_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2479afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974ed8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_table_buckets['movie_link'].oned_bin_modes['movie_link.linked_movie_id'] = t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ced593",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_table_buckets['movie_link'].oned_bin_modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab05830",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_binning_to_data_value_count(bins, data):\n",
    "    res = np.zeros(len(bins))\n",
    "    unique_remain = np.unique(data)\n",
    "    for i, bin in enumerate(bins):\n",
    "        res[i] = np.sum(np.isin(data, bin))\n",
    "        unique_remain = np.setdiff1d(unique_remain, bin)\n",
    "\n",
    "    res[0] += np.sum(np.isin(data, unique_remain))\n",
    "    return res\n",
    "\n",
    "class Factor:\n",
    "    \"\"\"\n",
    "    This the class defines a multidimensional conditional probability on one table.\n",
    "    \"\"\"\n",
    "    def __init__(self, table, table_len, variables, pdfs, na_values=None):\n",
    "        self.table = table\n",
    "        self.table_len = table_len\n",
    "        self.variables = variables\n",
    "        self.pdfs = pdfs\n",
    "        self.na_values = na_values  # this is the percentage of data, which is not nan.\n",
    "\n",
    "all_factor_pdfs = dict()\n",
    "for PK in equivalent_keys:\n",
    "    if PK in bins:\n",
    "        bin_value = bins[PK]\n",
    "    else:\n",
    "        bin_value = temp_bins[PK]\n",
    "    for key in equivalent_keys[PK]:\n",
    "        table = key.split(\".\")[0]\n",
    "        print(table, PK)\n",
    "        temp = apply_binning_to_data_value_count(bin_value, data[key])\n",
    "        print(np.sum(temp))\n",
    "        print(temp)\n",
    "        if table not in all_factor_pdfs:\n",
    "            all_factor_pdfs[table] = dict()\n",
    "        all_factor_pdfs[table][key] = temp / np.sum(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566dfdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_factors = dict()\n",
    "for table in all_factor_pdfs:\n",
    "    all_factors[table] = Factor(table, table_len[table], list(all_factor_pdfs[table].keys()),\n",
    "                                all_factor_pdfs[table], na_values[table])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462f2dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_factor_pdfs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899412f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"new_table_buckets.pkl\", \"wb\") as f:\n",
    "    pickle.dump(new_table_buckets, f, pickle.HIGHEST_PROTOCOL)\n",
    "#with open(\"ground_truth_factors_no_filter.pkl\", \"wb\") as f:\n",
    " #   pickle.dump(all_factors, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5450365",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"/home/ubuntu/data_CE/saved_models/bins.pkl\", \"rb\") as f:\n",
    "    bins = pickle.load(f)\n",
    "#with open(\"equivalent_keys.pkl\", \"rb\") as f:\n",
    " #   equivalent_keys = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962f58d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e6256d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in temp_bins:\n",
    "    print(key, len(temp_bins[key]), len(bins[key]))\n",
    "    for i in range(len(temp_bins[key])):\n",
    "        #print(key, len(temp_bins[key][i]), len(bins[key][i]))\n",
    "        if np.all(temp_bins[key][i] == bins[key][i]):\n",
    "            print(key, len(temp_bins[key][i]), len(bins[key][i]), \"okay\")\n",
    "        else:\n",
    "            print(key, len(temp_bins[key][i]), len(bins[key][i]), \"notokay\")\n",
    "        #assert np.all(temp_bins[key][i] == bins[key][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa9bfb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "equivalent_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79350c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "[len(temp_bins[k]) for k in temp_bins]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5cbedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = temp['company_type.id']\n",
    "for k in bucket.buckets:\n",
    "    print(\"==============================================================\")\n",
    "    print(k, len(bucket.buckets[k].bin_modes), len(bucket.bins))\n",
    "    print(bucket.buckets[k].bin_modes)\n",
    "    print([len(b) for b in bucket.buckets[k].bins])\n",
    "    print(bucket.buckets[k].bin_means)\n",
    "    print(bucket.buckets[k].bin_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49973d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/Users/ziniuw/Desktop/past_research/End-to-End-CardEst-Benchmark/datasets/stats_simplified/{}.csv\"\n",
    "model_folder = \"../../CE_scheme_models\"\n",
    "data, null_values, key_attrs, table_buckets, equivalent_keys, schema, bin_size = process_stats_data(data_path,\n",
    "                                                                       model_folder, 200, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6868f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = \"votes\"\n",
    "bucket = table_buckets[table]\n",
    "for attr in bucket.id_attributes:\n",
    "    print(attr)\n",
    "    print(np.sum(bucket.oned_bin_modes[attr]), bucket.oned_bin_modes[attr].shape)\n",
    "    if len(bucket.twod_bin_modes[attr]) != 0:\n",
    "        print(np.sum(bucket.twod_bin_modes[attr]), bucket.twod_bin_modes[attr].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844f0b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = \"postLinks\"\n",
    "bucket = table_buckets[table]\n",
    "for attr in bucket.id_attributes:\n",
    "    print(attr)\n",
    "    print(np.sum(bucket.oned_bin_modes[attr]), bucket.oned_bin_modes[attr].shape)\n",
    "    if len(bucket.twod_bin_modes[attr]) != 0:\n",
    "        print(np.sum(bucket.twod_bin_modes[attr]), bucket.twod_bin_modes[attr].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba052cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bucket.oned_bin_modes[bucket.id_attributes[0]])\n",
    "print(np.sum(bucket.twod_bin_modes[bucket.id_attributes[0]], axis=1))\n",
    "print(bucket.oned_bin_modes[bucket.id_attributes[1]])\n",
    "print(np.sum(bucket.twod_bin_modes[bucket.id_attributes[1]], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d89f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876da09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.concatenate([np.ones(5), np.ones(6), np.ones(7)]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4b69e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argsort([1,2,3,4])[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515f583f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.var([1,2,3,4,5,4,3,4,5,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352fb650",
   "metadata": {},
   "outputs": [],
   "source": [
    "min(3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c271a6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
