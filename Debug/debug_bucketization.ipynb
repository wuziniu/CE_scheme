{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5d492b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"/home/ubuntu/CE_scheme/\")\n",
    "from Schemas.imdb.schema import gen_imdb_schema\n",
    "from Join_scheme.data_prepare import read_table_csv\n",
    "from Join_scheme.bound import Bound_ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "368cd984",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "\n",
    "OPS = {\n",
    "    '>': np.greater,\n",
    "    '<': np.less,\n",
    "    '>=': np.greater_equal,\n",
    "    '<=': np.less_equal,\n",
    "    '=': np.equal,\n",
    "    '==': np.equal\n",
    "}\n",
    "\n",
    "\n",
    "def process_condition(cond, tables_all=None):\n",
    "    # parse a condition, either filter predicate or join operation\n",
    "    start = None\n",
    "    join = False\n",
    "    join_keys = {}\n",
    "    cond = cond.replace(\" in \", \" IN \")\n",
    "    cond = cond.replace(\" not in \", \" NOT IN \")\n",
    "    cond = cond.replace(\" like \", \" LIKE \")\n",
    "    cond = cond.replace(\" not like \", \" NOT LIKE \")\n",
    "    cond = cond.replace(\" between \", \" BETWEEN \")\n",
    "    s = None\n",
    "    ops = None\n",
    "    \n",
    "    if ' IN ' in cond:\n",
    "        s = cond.split(' IN ')\n",
    "        ops = \"in\"\n",
    "    elif \" NOT IN \" in cond:\n",
    "        s = cond.split(' NOT IN ')\n",
    "        ops = \"not in\"\n",
    "    elif \" LIKE \" in cond:\n",
    "        s = cond.split(' LIKE ')\n",
    "        ops = \"like\"\n",
    "    elif \" NOT LIKE \" in cond:\n",
    "        s = cond.split(' NOT LIKE ')\n",
    "        ops = \"not like\"\n",
    "    elif \" BETWEEN \" in cond:\n",
    "        s = cond.split(' BETWEEN ')\n",
    "        ops = \"between\"\n",
    "    elif \" IS \" in cond:\n",
    "        s = cond.split(' IS ')\n",
    "        ops = OPS[\"=\"]\n",
    "        \n",
    "        \n",
    "    if ' IN ' in cond or \" NOT IN \" in cond:\n",
    "        attr = s[0].strip()\n",
    "        try:\n",
    "            value = list(ast.literal_eval(s[1].strip()))\n",
    "        except:\n",
    "            temp_value = s[1].strip()[1:][:-1].split(',')\n",
    "            value = []\n",
    "            for v in temp_value:\n",
    "                value.append(v.strip())\n",
    "        if tables_all:\n",
    "            table = tables_all[attr.split(\".\")[0].strip()]\n",
    "            attr = table + \".\" + attr.split(\".\")[-1].strip()\n",
    "        else:\n",
    "            table = attr.split(\".\")[0].strip()\n",
    "        return table, [attr, ops, value], join, join_keys\n",
    "    \n",
    "    elif s is not None:\n",
    "        attr = s[0].strip()\n",
    "        value = s[1].strip()\n",
    "        if tables_all:\n",
    "            table = tables_all[attr.split(\".\")[0].strip()]\n",
    "            attr = table + \".\" + attr.split(\".\")[-1].strip()\n",
    "        else:\n",
    "            table = attr.split(\".\")[0].strip()\n",
    "        return table, [attr, ops, value], join, join_keys\n",
    "    \n",
    "        \n",
    "\n",
    "    for i in range(len(cond)):\n",
    "        s = cond[i]\n",
    "        if s in OPS:\n",
    "            start = i\n",
    "            if cond[i + 1] in OPS:\n",
    "                end = i + 2\n",
    "            else:\n",
    "                end = i + 1\n",
    "            break\n",
    "    \n",
    "    if start is None:\n",
    "        return None, [None, None, None], join, join_keys\n",
    "    assert start is not None\n",
    "    left = cond[:start].strip()\n",
    "    ops = cond[start:end].strip()\n",
    "    right = cond[end:].strip()\n",
    "    table1 = left.split(\".\")[0].strip().lower()\n",
    "    if tables_all:\n",
    "        cond = cond.replace(table1 + \".\", tables_all[table1] + \".\")\n",
    "        table1 = tables_all[table1]\n",
    "        left = table1 + \".\" + left.split(\".\")[-1].strip()\n",
    "    if \".\" in right:\n",
    "        table2 = right.split(\".\")[0].strip().lower()\n",
    "        if table2 in tables_all:\n",
    "            cond = cond.replace(table2 + \".\", tables_all[table2] + \".\")\n",
    "            table2 = tables_all[table2]\n",
    "            right = table2 + \".\" + right.split(\".\")[-1].strip()\n",
    "            join = True\n",
    "            join_keys[table1] = left\n",
    "            join_keys[table2] = right\n",
    "            return table1 + \" \" + table2, cond, join, join_keys\n",
    "\n",
    "    value = right.strip()\n",
    "    if value[0] == \"'\" and value[-1] == \"'\":\n",
    "        value = value[1:-1]\n",
    "    try:\n",
    "        value = list(ast.literal_eval(value.strip()))\n",
    "    except:\n",
    "        try:\n",
    "            value = int(value)\n",
    "        except:\n",
    "            try:\n",
    "                value = float(value)\n",
    "            except:\n",
    "                value = value\n",
    "\n",
    "    return table1, [left, ops, value], join, join_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a64ca2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_condition_join(cond, tables_all):\n",
    "    start = None\n",
    "    join = False\n",
    "    join_keys = {}\n",
    "    for i in range(len(cond)):\n",
    "        s = cond[i]\n",
    "        if s == \"=\":\n",
    "            start = i\n",
    "            if cond[i + 1] == \"=\":\n",
    "                end = i + 2\n",
    "            else:\n",
    "                end = i + 1\n",
    "            break\n",
    "    \n",
    "    if start is None:\n",
    "        return None, None, False, None\n",
    "    \n",
    "    left = cond[:start].strip()\n",
    "    ops = cond[start:end].strip()\n",
    "    right = cond[end:].strip()\n",
    "    table1 = left.split(\".\")[0].strip().lower()\n",
    "    if table1 in tables_all:\n",
    "        left = tables_all[table1] + \".\" + left.split(\".\")[-1].strip()\n",
    "    else:\n",
    "        assert False, table1\n",
    "        return None, None, False, None\n",
    "    if \".\" in right:\n",
    "        table2 = right.split(\".\")[0].strip().lower()\n",
    "        if table2 in tables_all:\n",
    "            right = tables_all[table2] + \".\" + right.split(\".\")[-1].strip()\n",
    "            join = True\n",
    "            join_keys[table1] = left\n",
    "            join_keys[table2] = right\n",
    "            return table1 + \" \" + table2, left + \" = \" + right, join, join_keys\n",
    "    return None, None, False, None\n",
    "    \n",
    "    \n",
    "\n",
    "def parse_query_all_join(query):\n",
    "    \"\"\"\n",
    "    This function will parse out all join conditions from the query. \n",
    "    \"\"\"\n",
    "    query = query.replace(\" where \", \" WHERE \")\n",
    "    query = query.replace(\" from \", \" FROM \")\n",
    "    #query = query.replace(\" and \", \" AND \")\n",
    "    query = query.split(\";\")[0]\n",
    "    query = query.strip()\n",
    "    tables_all = {}\n",
    "    join_cond = []\n",
    "    join_keys = {}\n",
    "    tables_str = query.split(\" WHERE \")[0].split(\" FROM \")[-1]\n",
    "    for table_str in tables_str.split(\",\"):\n",
    "        table_str = table_str.strip()\n",
    "        if \" as \" in table_str:\n",
    "            tables_all[table_str.split(\" as \")[-1]] = table_str.split(\" as \")[0]\n",
    "        else:\n",
    "            tables_all[table_str.split(\" \")[-1]] = table_str.split(\" \")[0]\n",
    "    # processing conditions\n",
    "    conditions = query.split(\" WHERE \")[-1].split(\" AND \")\n",
    "    for cond in conditions:\n",
    "        cond = cond.strip()\n",
    "        if cond[0] == \"(\" and cond[-1] == \")\":\n",
    "            cond = cond[1:-1]\n",
    "        table, cond, join, join_key = process_condition_join(cond, tables_all)\n",
    "        \n",
    "        if join:\n",
    "            join_cond.append(cond)\n",
    "            for tab in join_key:\n",
    "                if tab in join_keys:\n",
    "                    join_keys[tab].add(join_key[tab])\n",
    "                else:\n",
    "                    join_keys[tab] = set([join_key[tab]])\n",
    "\n",
    "    return tables_all, join_cond, join_keys\n",
    "\n",
    "def get_join_hyper_graph(join_keys, equivalent_keys):\n",
    "    equivalent_group = dict()\n",
    "    for table in join_keys:\n",
    "        for key in join_keys[table]:\n",
    "            seen = False\n",
    "            for indicator in equivalent_keys:\n",
    "                if key in equivalent_keys[indicator]:\n",
    "                    if seen:\n",
    "                        assert False, f\"{key} appears in multiple equivalent groups.\"\n",
    "                    if indicator not in equivalent_group:\n",
    "                        equivalent_group[indicator] = [key]\n",
    "                    else:\n",
    "                        equivalent_group[indicator].append(key)\n",
    "                    seen = True\n",
    "            if not seen:\n",
    "                assert False, f\"no equivalent groups found for {key}.\"\n",
    "    return equivalent_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "336bbda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/home/ubuntu/data_CE/{}.csv\"\n",
    "schema = gen_imdb_schema(data_path)\n",
    "BN = Bound_ensemble(None, None, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c24d7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_path = \"/home/ubuntu/data_CE/job/\"\n",
    "queries = dict()\n",
    "for file in os.listdir(query_path):\n",
    "    if file.endswith(\".sql\"):\n",
    "        with open(query_path+file, \"r\") as f:\n",
    "            q = f.readline()\n",
    "            queries[file.split(\".sql\")[0]] = q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85f64c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "\n",
    "class Bucket:\n",
    "    \"\"\"\n",
    "    The class of bucketization of a key attribute\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name, bins=[], bin_modes=[], bin_vars=[], bin_means=[], rest_bins_remaining=None):\n",
    "        self.name = name\n",
    "        self.bins = bins\n",
    "        self.bin_modes = bin_modes\n",
    "        self.bin_vars = bin_vars\n",
    "        self.bin_means = bin_means\n",
    "        self.rest_bins_remaining = rest_bins_remaining\n",
    "        if len(bins) != 0:\n",
    "            assert len(bins) == len(bin_modes)\n",
    "\n",
    "\n",
    "class Table_bucket:\n",
    "    \"\"\"\n",
    "    The class of bucketization for all key attributes in a table.\n",
    "    Supporting more than three dimensional bin modes requires simplifying the causal structure, which is left as a\n",
    "    future work.\n",
    "    \"\"\"\n",
    "    def __init__(self, table_name, id_attributes, bin_sizes):\n",
    "        self.table_name = table_name\n",
    "        self.id_attributes = id_attributes\n",
    "        self.bin_sizes = bin_sizes\n",
    "        self.oned_bin_modes = dict()\n",
    "        self.twod_bin_modes = dict()\n",
    "\n",
    "\n",
    "class Bucket_group:\n",
    "    \"\"\"\n",
    "    The class of bucketization for a group of equivalent join keys\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, buckets, start_key, sample_rate, bins=None, primary_keys=[]):\n",
    "        self.buckets = buckets\n",
    "        self.start_key = start_key\n",
    "        self.sample_rate = sample_rate\n",
    "        self.bins = bins\n",
    "        self.primary_keys = primary_keys\n",
    "\n",
    "    def bucketize(self, data):\n",
    "        \"\"\"\n",
    "        Discretize data based on the bucket\n",
    "        \"\"\"\n",
    "        res = dict()\n",
    "        seen_remain_key = np.array([])\n",
    "        cumulative_bin = copy.deepcopy(self.buckets[self.start_key].bins)\n",
    "        start_means = np.asarray(self.buckets[self.start_key].bin_means)\n",
    "\n",
    "        for key in data:\n",
    "            if key in self.primary_keys:\n",
    "                continue\n",
    "            res[key] = copy.deepcopy(data[key])\n",
    "            if key != self.start_key:\n",
    "                unique_remain = np.setdiff1d(self.buckets[key].rest_bins_remaining, seen_remain_key)\n",
    "                assert sum([np.sum(np.isin(unique_remain, b) == 1) for b in cumulative_bin]) == 0\n",
    "\n",
    "                if len(unique_remain) != 0:\n",
    "                    remaining_data = data[key][np.isin(data[key], unique_remain)]\n",
    "                    unique_remain, count_remain = np.unique(remaining_data, return_counts=True)\n",
    "                    unique_counts = np.unique(count_remain)\n",
    "                    for u in unique_counts:\n",
    "                        temp_idx = np.searchsorted(start_means, u)\n",
    "                        if temp_idx == len(cumulative_bin):\n",
    "                            idx = -1\n",
    "                            if u > self.buckets[key].bin_modes[-1]:\n",
    "                                self.buckets[key].bin_modes[-1] = u\n",
    "                        elif temp_idx == 0:\n",
    "                            idx = 0\n",
    "                        else:\n",
    "                            if (u - start_means[temp_idx - 1]) >= (start_means[temp_idx] - u):\n",
    "                                idx = temp_idx - 1\n",
    "                            else:\n",
    "                                idx = temp_idx\n",
    "                        temp_unique = unique_remain[count_remain == u]\n",
    "                        cumulative_bin[idx] = np.concatenate((cumulative_bin[idx], temp_unique))\n",
    "                        seen_remain_key = np.concatenate((seen_remain_key, temp_unique))\n",
    "                        if u > self.buckets[key].bin_modes[idx]:\n",
    "                            self.buckets[key].bin_modes[idx] = u\n",
    "            res[key] = copy.deepcopy(data[key])\n",
    "            count = 0\n",
    "            for i, b in enumerate(cumulative_bin):\n",
    "                count += len(data[key][np.isin(data[key], b)])\n",
    "                res[key][np.isin(data[key], b)] = i\n",
    "\n",
    "            if self.sample_rate[key] < 1.0:\n",
    "                bin_modes = self.buckets[key].bin_modes\n",
    "                bin_modes[bin_modes != 1] = bin_modes[bin_modes != 1] / self.sample_rate[key]\n",
    "                self.buckets[key].bin_modes = bin_modes\n",
    "\n",
    "        self.bins = cumulative_bin\n",
    "\n",
    "        for key in data:\n",
    "            if key in self.primary_keys:\n",
    "                res[key] = self.bucketize_PK(data[key])\n",
    "                self.buckets[key] = Bucket(key)\n",
    "        return res\n",
    "\n",
    "    def bucketize_PK(self, data):\n",
    "        res = copy.deepcopy(data)\n",
    "        remaining_data = np.unique(data)\n",
    "        for i, b in enumerate(self.bins):\n",
    "            res[np.isin(data, b)] = i\n",
    "            remaining_data = np.setdiff1d(remaining_data, b)\n",
    "        if len(remaining_data) != 0:\n",
    "            self.bins.append(list(remaining_data))\n",
    "            for key in self.buckets:\n",
    "                if key not in self.primary_keys:\n",
    "                    self.buckets[key].bin_modes = np.append(self.buckets[key].bin_modes, 0)\n",
    "        res[np.isin(data, remaining_data)] = len(self.bins)\n",
    "        return res\n",
    "\n",
    "\n",
    "\n",
    "def identify_key_values(schema):\n",
    "    \"\"\"\n",
    "    identify all the key attributes from the schema of a DB, currently we assume all possible joins are known\n",
    "    It is also easy to support unseen joins, which we left as a future work.\n",
    "    :param schema: the schema of a DB\n",
    "    :return: a dict of all keys, {table: [keys]};\n",
    "             a dict of set, each indicating which keys on different tables are considered the same key.\n",
    "    \"\"\"\n",
    "    all_keys = set()\n",
    "    equivalent_keys = dict()\n",
    "    for i, join in enumerate(schema.relationships):\n",
    "        keys = join.identifier.split(\" = \")\n",
    "        all_keys.add(keys[0])\n",
    "        all_keys.add(keys[1])\n",
    "        seen = False\n",
    "        for k in equivalent_keys:\n",
    "            if keys[0] in equivalent_keys[k]:\n",
    "                equivalent_keys[k].add(keys[1])\n",
    "                seen = True\n",
    "                break\n",
    "            elif keys[1] in equivalent_keys[k]:\n",
    "                equivalent_keys[k].add(keys[0])\n",
    "                seen = True\n",
    "                break\n",
    "        if not seen:\n",
    "            # set the keys[-1] as the identifier of this equivalent join key group for convenience.\n",
    "            equivalent_keys[keys[-1]] = set(keys)\n",
    "\n",
    "    assert len(all_keys) == sum([len(equivalent_keys[k]) for k in equivalent_keys])\n",
    "    return all_keys, equivalent_keys\n",
    "\n",
    "\n",
    "def equal_freq_binning(name, data, n_bins, data_len):\n",
    "    uniques, counts = data\n",
    "    if len(uniques) <= n_bins:\n",
    "        bins = []\n",
    "        bin_modes = []\n",
    "        bin_vars = []\n",
    "        bin_means = []\n",
    "        \n",
    "        for i, uni in enumerate(uniques):\n",
    "            bins.append([uni])\n",
    "            bin_modes.append(counts[i])\n",
    "            bin_vars.append(0)\n",
    "            bin_means.append(counts[i])\n",
    "        return Bucket(name, bins, bin_modes, bin_vars, bin_means)\n",
    "    \n",
    "    unique_counts, count_counts = np.unique(counts, return_counts=True)\n",
    "    idx = np.argsort(unique_counts)\n",
    "    unique_counts = unique_counts[idx]\n",
    "    count_counts = count_counts[idx]\n",
    "\n",
    "    bins = []\n",
    "    bin_modes = []\n",
    "    bin_vars = []\n",
    "    bin_means = []\n",
    "\n",
    "    bin_freq = data_len / n_bins\n",
    "    cur_freq = 0\n",
    "    cur_bin = []\n",
    "    cur_bin_count = []\n",
    "    for i, uni_c in enumerate(unique_counts):\n",
    "        cur_freq += count_counts[i] * uni_c\n",
    "        cur_bin.append(uniques[np.where(counts == uni_c)[0]])\n",
    "        cur_bin_count.extend([uni_c] * count_counts[i])\n",
    "        if (cur_freq > bin_freq) or (i == (len(unique_counts) - 1)):\n",
    "            bins.append(np.concatenate(cur_bin))\n",
    "            cur_bin_count = np.asarray(cur_bin_count)\n",
    "            bin_modes.append(uni_c)\n",
    "            bin_means.append(np.mean(cur_bin_count))\n",
    "            bin_vars.append(np.var(cur_bin_count))\n",
    "            cur_freq = 0\n",
    "            cur_bin = []\n",
    "            cur_bin_count = []\n",
    "    assert len(uniques) == sum([len(b) for b in bins]), f\"some unique values missed or duplicated\"\n",
    "    return Bucket(name, bins, bin_modes, bin_vars, bin_means)\n",
    "\n",
    "\n",
    "def compute_variance_score(buckets):\n",
    "    \"\"\"\n",
    "    compute the variance of products of random variables\n",
    "    \"\"\"\n",
    "    all_mean = np.asarray([buckets[k].bin_means for k in buckets])\n",
    "    all_var = np.asarray([buckets[k].bin_vars for k in buckets])\n",
    "    return np.sum(np.prod(all_var + all_mean ** 2, axis=0) - np.prod(all_mean, axis=0) ** 2)\n",
    "\n",
    "\n",
    "def sub_optimal_bucketize(data, sample_rate, n_bins=30, primary_keys=[]):\n",
    "    \"\"\"\n",
    "    Perform sub-optimal bucketization on a group of equivalent join keys.\n",
    "    :param data: a dict of (potentially sampled) table data of the keys\n",
    "                 the keys of this dict are one group of equivalent join keys\n",
    "    :param sample_rate: the sampling rate the data, could be all 1 if no sampling is performed\n",
    "    :param n_bins: how many bins can we allocate\n",
    "    :param primary_keys: the primary keys in the equivalent group since we don't need to bucketize PK.\n",
    "    :return: new data, where the keys are bucketized\n",
    "             the mode of each bucket\n",
    "    \"\"\"\n",
    "    unique_values = dict()\n",
    "    for key in data:\n",
    "        if key not in primary_keys:\n",
    "            unique_values[key] = np.unique(data[key], return_counts=True)\n",
    "\n",
    "    best_variance_score = np.infty\n",
    "    best_bin_len = 0\n",
    "    best_start_key = None\n",
    "    best_buckets = None\n",
    "    for start_key in data:\n",
    "        if start_key in primary_keys:\n",
    "            continue\n",
    "        start_bucket = equal_freq_binning(start_key, unique_values[start_key], n_bins, len(data[start_key]))\n",
    "        rest_buckets = dict()\n",
    "        for key in data:\n",
    "            if key == start_key or key in primary_keys:\n",
    "                continue\n",
    "            uniques = unique_values[key][0]\n",
    "            counts = unique_values[key][1]\n",
    "            rest_buckets[key] = Bucket(key, [], [0] * len(start_bucket.bins), [0] * len(start_bucket.bins),\n",
    "                                       [0] * len(start_bucket.bins), uniques)\n",
    "            for i, bin in enumerate(start_bucket.bins):\n",
    "                idx = np.where(np.isin(uniques, bin) == 1)[0]\n",
    "                if len(idx) != 0:\n",
    "                    bin_count = counts[idx]\n",
    "                    unique_bin_keys = uniques[idx]\n",
    "                    # unique_bin_count = np.unique(bin_count)\n",
    "                    # bin_count = np.concatenate([counts[counts == j] for j in unique_bin_count])\n",
    "                    # unique_bin_keys = np.concatenate([uniques[counts == j] for j in unique_bin_count])\n",
    "                    rest_buckets[key].rest_bins_remaining = np.setdiff1d(rest_buckets[key].rest_bins_remaining,\n",
    "                                                                         unique_bin_keys)\n",
    "                    rest_buckets[key].bin_modes[i] = np.max(bin_count)\n",
    "                    rest_buckets[key].bin_vars[i] = np.var(bin_count)\n",
    "                    rest_buckets[key].bin_means[i] = np.mean(bin_count)\n",
    "\n",
    "        rest_buckets[start_key] = start_bucket\n",
    "        var_score = compute_variance_score(rest_buckets)\n",
    "        if len(start_bucket.bins) > best_bin_len:\n",
    "            best_variance_score = var_score\n",
    "            best_start_key = start_key\n",
    "            best_buckets = rest_buckets\n",
    "            best_bin_len = len(start_bucket.bins)\n",
    "        elif len(start_bucket.bins) >= best_bin_len * 0.9 and var_score < best_variance_score:\n",
    "            best_variance_score = var_score\n",
    "            best_start_key = start_key\n",
    "            best_buckets = rest_buckets\n",
    "            best_bin_len = len(start_bucket.bins)\n",
    "\n",
    "    best_buckets = Bucket_group(best_buckets, best_start_key, sample_rate, primary_keys=primary_keys)\n",
    "    new_data = best_buckets.bucketize(data)\n",
    "    return new_data, best_buckets\n",
    "\n",
    "\n",
    "def fixed_start_key_bucketize(start_key, data, sample_rate, n_bins=30, primary_keys=[]):\n",
    "    \"\"\"\n",
    "    Perform sub-optimal bucketization on a group of equivalent join keys based on the pre-defined start_key.\n",
    "    :param data: a dict of (potentially sampled) table data of the keys\n",
    "                 the keys of this dict are one group of equivalent join keys\n",
    "    :param sample_rate: the sampling rate the data, could be all 1 if no sampling is performed\n",
    "    :param n_bins: how many bins can we allocate\n",
    "    :param primary_keys: the primary keys in the equivalent group since we don't need to bucketize PK.\n",
    "    :return: new data, where the keys are bucketized\n",
    "             the mode of each bucket\n",
    "    \"\"\"\n",
    "    unique_values = dict()\n",
    "    for key in data:\n",
    "        if key not in primary_keys:\n",
    "            unique_values[key] = np.unique(data[key], return_counts=True)\n",
    "\n",
    "    start_bucket = equal_freq_binning(start_key, unique_values[start_key], n_bins, len(data[start_key]))\n",
    "    rest_buckets = dict()\n",
    "    for key in data:\n",
    "        if key == start_key or key in primary_keys:\n",
    "            continue\n",
    "        uniques = unique_values[key][0]\n",
    "        counts = unique_values[key][1]\n",
    "        rest_buckets[key] = Bucket(key, [], [0] * len(start_bucket.bins), [0] * len(start_bucket.bins),\n",
    "                                   [0] * len(start_bucket.bins), uniques)\n",
    "        for i, bin in enumerate(start_bucket.bins):\n",
    "            idx = np.where(np.isin(uniques, bin) == 1)[0]\n",
    "            if len(idx) != 0:\n",
    "                bin_count = counts[idx]\n",
    "                unique_bin_keys = uniques[idx]\n",
    "                rest_buckets[key].rest_bins_remaining = np.setdiff1d(rest_buckets[key].rest_bins_remaining,\n",
    "                                                                     unique_bin_keys)\n",
    "                rest_buckets[key].bin_modes[i] = np.max(bin_count)\n",
    "                rest_buckets[key].bin_means[i] = np.mean(bin_count)\n",
    "\n",
    "    best_buckets = Bucket_group(rest_buckets, start_key, sample_rate, primary_keys=primary_keys)\n",
    "    new_data = best_buckets.bucketize(data, n_bins)\n",
    "    return new_data, best_buckets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10939d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'kind_type.id': {'aka_title.kind_id', 'kind_type.id', 'title.kind_id'}, 'info_type.id': {'movie_info.info_type_id', 'info_type.id', 'person_info.info_type_id', 'movie_info_idx.info_type_id'}, 'title.id': {'cast_info.movie_id', 'movie_info_idx.movie_id', 'complete_cast.movie_id', 'movie_info.movie_id', 'movie_keyword.movie_id', 'aka_title.movie_id', 'title.id', 'movie_companies.movie_id'}, 'name.id': {'name.id', 'person_info.person_id', 'cast_info.person_id', 'aka_name.person_id'}, 'char_name.id': {'cast_info.person_role_id', 'char_name.id'}, 'role_type.id': {'cast_info.role_id', 'role_type.id'}, 'comp_cast_type.id': {'complete_cast.subject_id', 'complete_cast.status_id', 'comp_cast_type.id'}, 'keyword.id': {'keyword.id', 'movie_keyword.keyword_id'}, 'company_name.id': {'company_name.id', 'movie_companies.company_id'}, 'company_type.id': {'movie_companies.company_type_id', 'company_type.id'}}\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "all_keys, equivalent_keys = identify_key_values(schema)\n",
    "print(equivalent_keys)\n",
    "print(len(equivalent_keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe3f6875",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_join_keys_stats = dict()\n",
    "for key in queries:\n",
    "    q = queries[key]\n",
    "    res = parse_query_all_join(q)\n",
    "    for table in res[-1]:\n",
    "        for join_key in list(res[-1][table]):\n",
    "            for PK in equivalent_keys:\n",
    "                if join_key in equivalent_keys[PK]:\n",
    "                    if PK in all_join_keys_stats:\n",
    "                        all_join_keys_stats[PK] += 1\n",
    "                    else:\n",
    "                        all_join_keys_stats[PK] = 1\n",
    "                    break\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53e11407",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title.id': ['title.id',\n",
       "  'movie_info.movie_id',\n",
       "  'movie_companies.movie_id',\n",
       "  'cast_info.movie_id',\n",
       "  'movie_keyword.movie_id',\n",
       "  'complete_cast.movie_id'],\n",
       " 'info_type.id': ['movie_info.info_type_id',\n",
       "  'info_type.id',\n",
       "  'person_info.info_type_id'],\n",
       " 'company_name.id': ['movie_companies.company_id', 'company_name.id'],\n",
       " 'role_type.id': ['cast_info.role_id', 'role_type.id'],\n",
       " 'name.id': ['cast_info.person_id',\n",
       "  'name.id',\n",
       "  'aka_name.person_id',\n",
       "  'person_info.person_id'],\n",
       " 'char_name.id': ['cast_info.person_role_id', 'char_name.id'],\n",
       " 'keyword.id': ['movie_keyword.keyword_id', 'keyword.id'],\n",
       " 'comp_cast_type.id': ['complete_cast.subject_id',\n",
       "  'complete_cast.status_id',\n",
       "  'comp_cast_type.id']}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tables_all, join_cond, join_keys = parse_query_all_join(queries['29b'])\n",
    "get_join_hyper_graph(join_keys, equivalent_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ad7fc57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title.id': 447,\n",
       " 'info_type.id': 160,\n",
       " 'company_name.id': 142,\n",
       " 'char_name.id': 44,\n",
       " 'name.id': 138,\n",
       " 'role_type.id': 40,\n",
       " 'char_name.id': 44,\n",
       " 'keyword.id': 150,\n",
       " 'comp_cast_type.id': 60,\n",
       " 'kind_type.id': 52,\n",
       " 'company_type.id': 82}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_join_keys_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "06d8546e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bins = {\n",
    "    'title.id': 800,\n",
    "    'info_type.id': 100,\n",
    "    'keyword.id': 100,\n",
    "    'company_name.id': 100,\n",
    "    'name.id': 100,\n",
    "    'company_type.id': 100,\n",
    "    'comp_cast_type.id': 50,\n",
    "    'kind_type.id': 50,\n",
    "    'char_name.id': 50,\n",
    "    'role_type.id': 50\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d65952d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sample(np_data, nrows=1000000, seed=0):\n",
    "    np.random.seed(seed)\n",
    "    if len(np_data) <= nrows:\n",
    "        return np_data, 1.0\n",
    "    else:\n",
    "        selected = np.random.choice(len(np_data), size=nrows, replace=False)\n",
    "        return np_data[selected], nrows/len(np_data)\n",
    "\n",
    "def stats_analysis(sample, data, sample_rate, show=10):\n",
    "    n, c = np.unique(sample, return_counts=True)\n",
    "    idx = np.argsort(c)[::-1]\n",
    "    for i in range(min(show, len(idx))):\n",
    "        print(c[idx[i]], c[idx[i]]/sample_rate, len(data[data == n[idx[i]]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b86b246",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/bayescard/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3173: DtypeWarning: Columns (5,10) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title.id\n",
      "0\n",
      "2528312 2528312\n",
      "2528312 2528312\n",
      "0\n",
      "title.kind_id\n",
      "0\n",
      "2528312 2528312\n",
      "2528312 2528312\n",
      "0\n",
      "movie_info_idx.movie_id\n",
      "0\n",
      "1380035 1380035\n",
      "1380035 1380035\n",
      "0\n",
      "movie_info_idx.info_type_id\n",
      "0\n",
      "1380035 1380035\n",
      "1380035 1380035\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/bayescard/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3173: DtypeWarning: Columns (4) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movie_info.movie_id\n",
      "0\n",
      "14835720 14835720\n",
      "14835720 14835720\n",
      "0\n",
      "movie_info.info_type_id\n",
      "0\n",
      "14835720 14835720\n",
      "14835720 14835720\n",
      "0\n",
      "info_type.id\n",
      "0\n",
      "113 113\n",
      "113 113\n",
      "0\n",
      "cast_info.person_id\n",
      "0\n",
      "36244344 36244344\n",
      "36244344 36244344\n",
      "0\n",
      "cast_info.movie_id\n",
      "0\n",
      "36244344 36244344\n",
      "36244344 36244344\n",
      "0\n",
      "cast_info.person_role_id\n",
      "18672825\n",
      "36244344 36244344\n",
      "17571519 36244344\n",
      "0\n",
      "cast_info.role_id\n",
      "0\n",
      "36244344 36244344\n",
      "36244344 36244344\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/bayescard/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3173: DtypeWarning: Columns (2) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "char_name.id\n",
      "0\n",
      "3140339 3140339\n",
      "3140339 3140339\n",
      "0\n",
      "role_type.id\n",
      "0\n",
      "12 12\n",
      "12 12\n",
      "0\n",
      "complete_cast.movie_id\n",
      "0\n",
      "135086 135086\n",
      "135086 135086\n",
      "0\n",
      "complete_cast.subject_id\n",
      "0\n",
      "135086 135086\n",
      "135086 135086\n",
      "0\n",
      "complete_cast.status_id\n",
      "0\n",
      "135086 135086\n",
      "135086 135086\n",
      "0\n",
      "comp_cast_type.id\n",
      "0\n",
      "4 4\n",
      "4 4\n",
      "0\n",
      "name.id\n",
      "0\n",
      "4167491 4167491\n",
      "4167491 4167491\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/bayescard/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3173: DtypeWarning: Columns (3) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aka_name.person_id\n",
      "0\n",
      "901343 901343\n",
      "901343 901343\n",
      "0\n",
      "movie_keyword.movie_id\n",
      "0\n",
      "4523930 4523930\n",
      "4523930 4523930\n",
      "0\n",
      "movie_keyword.keyword_id\n",
      "0\n",
      "4523930 4523930\n",
      "4523930 4523930\n",
      "0\n",
      "keyword.id\n",
      "0\n",
      "134170 134170\n",
      "134170 134170\n",
      "0\n",
      "person_info.person_id\n",
      "0\n",
      "2963664 2963664\n",
      "2963664 2963664\n",
      "0\n",
      "person_info.info_type_id\n",
      "0\n",
      "2963664 2963664\n",
      "2963664 2963664\n",
      "0\n",
      "movie_companies.movie_id\n",
      "0\n",
      "2609129 2609129\n",
      "2609129 2609129\n",
      "0\n",
      "movie_companies.company_id\n",
      "0\n",
      "2609129 2609129\n",
      "2609129 2609129\n",
      "0\n",
      "movie_companies.company_type_id\n",
      "0\n",
      "2609129 2609129\n",
      "2609129 2609129\n",
      "0\n",
      "company_name.id\n",
      "0\n",
      "234997 234997\n",
      "234997 234997\n",
      "0\n",
      "company_type.id\n",
      "0\n",
      "4 4\n",
      "4 4\n",
      "0\n",
      "aka_title.movie_id\n",
      "0\n",
      "361472 361472\n",
      "361472 361472\n",
      "0\n",
      "aka_title.kind_id\n",
      "0\n",
      "361472 361472\n",
      "361472 361472\n",
      "0\n",
      "kind_type.id\n",
      "0\n",
      "7 7\n",
      "7 7\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "data = dict()\n",
    "sample_rate = dict()\n",
    "primary_keys = []\n",
    "for table_obj in schema.tables:\n",
    "    df_rows = pd.read_csv(table_obj.csv_file_location, header=None, escapechar='\\\\', encoding='utf-8', quotechar='\"',\n",
    "                          sep=\",\")\n",
    "    \n",
    "    df_rows.columns = [table_obj.table_name + '.' + attr for attr in table_obj.attributes]\n",
    "\n",
    "    for attribute in table_obj.irrelevant_attributes:\n",
    "        df_rows = df_rows.drop(table_obj.table_name + '.' + attribute, axis=1)\n",
    "\n",
    "    df_rows.apply(pd.to_numeric, errors=\"ignore\")\n",
    "    for attr in df_rows.columns:\n",
    "        if attr in all_keys:\n",
    "            print(attr)\n",
    "            print(np.sum(np.isnan(df_rows[attr].values)))\n",
    "            data[attr] = df_rows[attr].values\n",
    "            data[attr][np.isnan(data[attr])] = -1\n",
    "            data[attr][data[attr] < 0] = -1\n",
    "            print(len(data[attr]), len(df_rows[attr].values))\n",
    "            data[attr] = copy.deepcopy(data[attr])[data[attr]>=0]\n",
    "            print(len(data[attr]), len(df_rows[attr].values))\n",
    "            if len(np.unique(data[attr])) >= len(data[attr]) - 10:\n",
    "                primary_keys.append(attr)\n",
    "            print(np.sum(np.isnan(df_rows[attr].values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1294f672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title.id\n",
      "1 2.528312 1\n",
      "1 2.528312 1\n",
      "1 2.528312 1\n",
      "1 2.528312 1\n",
      "1 2.528312 1\n",
      "1 2.528312 1\n",
      "1 2.528312 1\n",
      "1 2.528312 1\n",
      "1 2.528312 1\n",
      "1 2.528312 1\n",
      "title.kind_id\n",
      "609674 1541446.090288 1543264\n",
      "262591 663911.976392 662824\n",
      "46890 118552.54968 118234\n",
      "39888 100849.309056 100537\n",
      "35940 90867.53328 90852\n",
      "5016 12682.012992 12600\n",
      "1 2.528312 1\n",
      "movie_info_idx.movie_id\n",
      "4 5.52014 4\n",
      "4 5.52014 4\n",
      "4 5.52014 4\n",
      "4 5.52014 4\n",
      "4 5.52014 4\n",
      "4 5.52014 4\n",
      "4 5.52014 4\n",
      "4 5.52014 4\n",
      "4 5.52014 4\n",
      "4 5.52014 4\n",
      "movie_info_idx.info_type_id\n",
      "333438 460156.11033 459925\n",
      "333295 459958.765325 459925\n",
      "333076 459656.53766 459925\n",
      "185 255.30647499999998 250\n",
      "6 8.28021 10\n",
      "movie_info.movie_id\n",
      "194 2878.12968 2937\n",
      "118 1750.61496 1584\n",
      "109 1617.09348 1638\n",
      "97 1439.06484 1344\n",
      "92 1364.88624 1281\n",
      "89 1320.37908 1081\n",
      "83 1231.36476 1219\n",
      "82 1216.5290400000001 1160\n",
      "77 1142.35044 1189\n",
      "77 1142.35044 1207\n",
      "movie_info.info_type_id\n",
      "204500 3033904.74 3036719\n",
      "103748 1539176.27856 1533909\n",
      "94821 1406737.80612 1401902\n",
      "89252 1324117.68144 1325361\n",
      "87478 1297799.11416 1298989\n",
      "87209 1293808.30548 1288928\n",
      "54058 801989.35176 802140\n",
      "44377 658364.74644 660923\n",
      "40308 597998.20176 598457\n",
      "32495 482086.72140000004 486554\n",
      "info_type.id\n",
      "1 1.0 1\n",
      "1 1.0 1\n",
      "1 1.0 1\n",
      "1 1.0 1\n",
      "1 1.0 1\n",
      "1 1.0 1\n",
      "1 1.0 1\n",
      "1 1.0 1\n",
      "1 1.0 1\n",
      "1 1.0 1\n",
      "cast_info.person_id\n",
      "279 10112.171976 10249\n",
      "236 8553.665184 8242\n",
      "225 8154.9774 7725\n",
      "219 7937.511336 7684\n",
      "187 6777.692328 6034\n",
      "186 6741.447984 6912\n",
      "180 6523.98192 6243\n",
      "178 6451.493232 5980\n",
      "166 6016.561104 5310\n",
      "162 5871.583728 6516\n",
      "cast_info.movie_id\n",
      "54 1957.194576 1741\n",
      "48 1739.728512 1531\n",
      "37 1341.040728 1572\n",
      "35 1268.55204 728\n",
      "27 978.597288 1027\n",
      "25 906.1086 911\n",
      "24 869.864256 987\n",
      "23 833.619912 790\n",
      "22 797.375568 667\n",
      "21 761.131224 660\n",
      "cast_info.person_role_id\n",
      "75296 1323065.094624 1322872\n",
      "37827 664677.8492129999 662713\n",
      "16303 286468.47425699997 286435\n",
      "5099 89597.175381 90715\n",
      "3665 64399.617135 63110\n",
      "3571 62747.894348999995 62610\n",
      "2419 42505.504461 42872\n",
      "2278 40027.920282 40445\n",
      "2266 39817.062054 40768\n",
      "2239 39342.631041 37837\n",
      "cast_info.role_id\n",
      "350218 12693421.666992 12670688\n",
      "205047 7431794.004168 7451973\n",
      "118622 4299376.573968 4323018\n",
      "110600 4008624.4464 4008037\n",
      "75521 2737209.103224 2728943\n",
      "46984 1702904.258496 1703543\n",
      "30167 1093383.125448 1093558\n",
      "24993 905854.889592 898389\n",
      "21455 777622.40052 773674\n",
      "8693 315072.082392 316118\n",
      "char_name.id\n",
      "1 3.140339 1\n",
      "1 3.140339 1\n",
      "1 3.140339 1\n",
      "1 3.140339 1\n",
      "1 3.140339 1\n",
      "1 3.140339 1\n",
      "1 3.140339 1\n",
      "1 3.140339 1\n",
      "1 3.140339 1\n",
      "1 3.140339 1\n",
      "role_type.id\n",
      "1 1.0 1\n",
      "1 1.0 1\n",
      "1 1.0 1\n",
      "1 1.0 1\n",
      "1 1.0 1\n",
      "1 1.0 1\n",
      "1 1.0 1\n",
      "1 1.0 1\n",
      "1 1.0 1\n",
      "1 1.0 1\n",
      "complete_cast.movie_id\n",
      "2 2.0 2\n",
      "2 2.0 2\n",
      "2 2.0 2\n",
      "2 2.0 2\n",
      "2 2.0 2\n",
      "2 2.0 2\n",
      "2 2.0 2\n",
      "2 2.0 2\n",
      "2 2.0 2\n",
      "2 2.0 2\n",
      "complete_cast.subject_id\n",
      "85941 85941.0 85941\n",
      "49145 49145.0 49145\n",
      "complete_cast.status_id\n",
      "110494 110494.0 110494\n",
      "24592 24592.0 24592\n",
      "comp_cast_type.id\n",
      "1 1.0 1\n",
      "1 1.0 1\n",
      "1 1.0 1\n",
      "1 1.0 1\n",
      "name.id\n",
      "1 4.167491 1\n",
      "1 4.167491 1\n",
      "1 4.167491 1\n",
      "1 4.167491 1\n",
      "1 4.167491 1\n",
      "1 4.167491 1\n",
      "1 4.167491 1\n",
      "1 4.167491 1\n",
      "1 4.167491 1\n",
      "1 4.167491 1\n",
      "aka_name.person_id\n",
      "76 76.0 76\n",
      "72 72.0 72\n",
      "63 63.0 63\n",
      "51 51.0 51\n",
      "51 51.0 51\n",
      "41 41.0 41\n",
      "39 39.0 39\n",
      "37 37.0 37\n",
      "37 37.0 37\n",
      "36 36.0 36\n",
      "movie_keyword.movie_id\n",
      "118 533.82374 459\n",
      "115 520.2519500000001 540\n",
      "112 506.68016000000006 455\n",
      "111 502.15623000000005 462\n",
      "104 470.48872 503\n",
      "102 461.44086000000004 444\n",
      "101 456.91693000000004 378\n",
      "99 447.86907 413\n",
      "97 438.82121 394\n",
      "96 434.29728 435\n",
      "movie_keyword.keyword_id\n",
      "16093 72803.60549 72496\n",
      "12846 58114.404780000004 58448\n",
      "9257 41878.02001 41840\n",
      "7827 35408.800110000004 34710\n",
      "6387 28894.340910000003 29440\n",
      "4451 20136.012430000002 20522\n",
      "4360 19724.3348 19528\n",
      "3247 14689.200710000001 14364\n",
      "3075 13911.08475 13753\n",
      "3069 13883.94117 14120\n",
      "keyword.id\n",
      "1 1.0 1\n",
      "1 1.0 1\n",
      "1 1.0 1\n",
      "1 1.0 1\n",
      "1 1.0 1\n",
      "1 1.0 1\n",
      "1 1.0 1\n",
      "1 1.0 1\n",
      "1 1.0 1\n",
      "1 1.0 1\n",
      "person_info.person_id\n",
      "479 1419.595056 1479\n",
      "422 1250.666208 1209\n",
      "384 1138.046976 1178\n",
      "301 892.0628640000001 864\n",
      "286 847.6079040000001 835\n",
      "272 806.116608 836\n",
      "254 752.770656 696\n",
      "236 699.424704 705\n",
      "232 687.570048 652\n",
      "228 675.7153920000001 648\n",
      "person_info.info_type_id\n",
      "208893 619088.6639520001 620526\n",
      "141278 418700.522592 419654\n",
      "119451 354012.628464 353282\n",
      "111521 330510.772944 329504\n",
      "48204 142860.459456 142345\n",
      "45427 134630.364528 134574\n",
      "42743 126675.890352 125700\n",
      "41184 122055.538176 122779\n",
      "38399 113801.733936 113543\n",
      "33618 99632.45635200001 100269\n",
      "movie_companies.movie_id\n",
      "49 127.847321 94\n",
      "33 86.10125699999999 56\n",
      "32 83.492128 87\n",
      "31 80.882999 75\n",
      "30 78.27387 60\n",
      "30 78.27387 58\n",
      "29 75.66474099999999 72\n",
      "29 75.66474099999999 70\n",
      "28 73.055612 59\n",
      "28 73.055612 70\n",
      "movie_companies.company_id\n",
      "23824 62159.889295999994 62075\n",
      "17937 46799.946873 47252\n",
      "13826 36073.817554 36136\n",
      "10555 27539.356594999997 27303\n",
      "6130 15993.96077 16220\n",
      "4963 12949.107226999999 12948\n",
      "4956 12930.843324 12894\n",
      "4499 11738.471371 11817\n",
      "4116 10739.174964 10667\n",
      "3874 10107.765746 10160\n",
      "movie_companies.company_type_id\n",
      "511780 1335300.03962 1334883\n",
      "488220 1273828.9603799998 1274246\n",
      "company_name.id\n",
      "1 1.0 1\n",
      "1 1.0 1\n",
      "1 1.0 1\n",
      "1 1.0 1\n",
      "1 1.0 1\n",
      "1 1.0 1\n",
      "1 1.0 1\n",
      "1 1.0 1\n",
      "1 1.0 1\n",
      "1 1.0 1\n",
      "company_type.id\n",
      "1 1.0 1\n",
      "1 1.0 1\n",
      "1 1.0 1\n",
      "1 1.0 1\n",
      "aka_title.movie_id\n",
      "93 93.0 93\n",
      "43 43.0 43\n",
      "43 43.0 43\n",
      "27 27.0 27\n",
      "26 26.0 26\n",
      "22 22.0 22\n",
      "20 20.0 20\n",
      "19 19.0 19\n",
      "18 18.0 18\n",
      "16 16.0 16\n",
      "aka_title.kind_id\n",
      "293275 293275.0 293275\n",
      "26939 26939.0 26939\n",
      "20320 20320.0 20320\n",
      "13497 13497.0 13497\n",
      "4565 4565.0 4565\n",
      "2876 2876.0 2876\n",
      "kind_type.id\n",
      "1 1.0 1\n",
      "1 1.0 1\n",
      "1 1.0 1\n",
      "1 1.0 1\n",
      "1 1.0 1\n",
      "1 1.0 1\n",
      "1 1.0 1\n"
     ]
    }
   ],
   "source": [
    "sample_rate = dict()\n",
    "sampled_data = dict()\n",
    "for k in data:\n",
    "    print(k)\n",
    "    temp = make_sample(data[k], 1000000)\n",
    "    stats_analysis(temp[0], data[k], temp[1])\n",
    "    sampled_data[k] = temp[0]\n",
    "    sample_rate[k] = temp[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759e7e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sampled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca97c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(sampled_data[\"kind_type.id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2dafda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(equivalent_keys))\n",
    "print(len(temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d27d91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "equivalent_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3349d510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'kind_type.id', 'aka_title.kind_id', 'title.kind_id'}\n",
      "{'info_type.id', 'person_info.info_type_id', 'movie_info_idx.info_type_id', 'movie_info.info_type_id'}\n",
      "{'movie_keyword.movie_id', 'movie_companies.movie_id', 'complete_cast.movie_id', 'movie_info_idx.movie_id', 'movie_info.movie_id', 'cast_info.movie_id', 'title.id', 'aka_title.movie_id'}\n",
      "{'aka_name.person_id', 'cast_info.person_id', 'name.id', 'person_info.person_id'}\n",
      "{'cast_info.person_role_id', 'char_name.id'}\n",
      "{'role_type.id', 'cast_info.role_id'}\n",
      "{'comp_cast_type.id', 'complete_cast.subject_id', 'complete_cast.status_id'}\n",
      "{'keyword.id', 'movie_keyword.keyword_id'}\n",
      "{'movie_companies.company_id', 'company_name.id'}\n",
      "{'company_type.id', 'movie_companies.company_type_id'}\n"
     ]
    }
   ],
   "source": [
    "temp = dict()\n",
    "for PK in equivalent_keys:\n",
    "    #if PK != 'kind_type.id':\n",
    "     #   continue\n",
    "    print(equivalent_keys[PK])\n",
    "    group_data = {}\n",
    "    group_sample_rate = {}\n",
    "    for K in equivalent_keys[PK]:\n",
    "        group_data[K] = sampled_data[K]\n",
    "        group_sample_rate[K] = sample_rate[K]\n",
    "    d, bucket = sub_optimal_bucketize(group_data, sample_rate, n_bins=n_bins[PK], primary_keys=primary_keys)\n",
    "    temp[PK] = bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ae13b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_data['kind_type.id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b5001f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket.bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48352d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_bins = dict()\n",
    "for key in temp:\n",
    "    temp_bins[key] = temp[key].bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dba662ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "72\n",
      "74\n",
      "29\n",
      "28\n",
      "12\n",
      "2\n",
      "86\n",
      "75\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "for k in temp_bins:\n",
    "    print(len(temp_bins[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "899412f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"bins.pkl\", \"wb\") as f:\n",
    "    pickle.dump(temp_bins, f, pickle.HIGHEST_PROTOCOL)\n",
    "with open(\"equivalent_keys.pkl\", \"wb\") as f:\n",
    "    pickle.dump(equivalent_keys, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5450365",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"bins.pkl\", \"rb\") as f:\n",
    "    bins = pickle.load(f)\n",
    "with open(\"equivalent_keys.pkl\", \"rb\") as f:\n",
    "    equivalent_keys = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa9bfb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "equivalent_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79350c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "[len(temp_bins[k]) for k in temp_bins]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5cbedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = temp['company_type.id']\n",
    "for k in bucket.buckets:\n",
    "    print(\"==============================================================\")\n",
    "    print(k, len(bucket.buckets[k].bin_modes), len(bucket.bins))\n",
    "    print(bucket.buckets[k].bin_modes)\n",
    "    print([len(b) for b in bucket.buckets[k].bins])\n",
    "    print(bucket.buckets[k].bin_means)\n",
    "    print(bucket.buckets[k].bin_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49973d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/Users/ziniuw/Desktop/past_research/End-to-End-CardEst-Benchmark/datasets/stats_simplified/{}.csv\"\n",
    "model_folder = \"../../CE_scheme_models\"\n",
    "data, null_values, key_attrs, table_buckets, equivalent_keys, schema, bin_size = process_stats_data(data_path,\n",
    "                                                                       model_folder, 200, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6868f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = \"votes\"\n",
    "bucket = table_buckets[table]\n",
    "for attr in bucket.id_attributes:\n",
    "    print(attr)\n",
    "    print(np.sum(bucket.oned_bin_modes[attr]), bucket.oned_bin_modes[attr].shape)\n",
    "    if len(bucket.twod_bin_modes[attr]) != 0:\n",
    "        print(np.sum(bucket.twod_bin_modes[attr]), bucket.twod_bin_modes[attr].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844f0b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = \"postLinks\"\n",
    "bucket = table_buckets[table]\n",
    "for attr in bucket.id_attributes:\n",
    "    print(attr)\n",
    "    print(np.sum(bucket.oned_bin_modes[attr]), bucket.oned_bin_modes[attr].shape)\n",
    "    if len(bucket.twod_bin_modes[attr]) != 0:\n",
    "        print(np.sum(bucket.twod_bin_modes[attr]), bucket.twod_bin_modes[attr].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba052cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bucket.oned_bin_modes[bucket.id_attributes[0]])\n",
    "print(np.sum(bucket.twod_bin_modes[bucket.id_attributes[0]], axis=1))\n",
    "print(bucket.oned_bin_modes[bucket.id_attributes[1]])\n",
    "print(np.sum(bucket.twod_bin_modes[bucket.id_attributes[1]], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d89f1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
