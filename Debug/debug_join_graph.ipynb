{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6025f1cb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'load_sample_imdb_one_query' from 'Sampling.load_sample' (/home/ubuntu/CE_scheme/Sampling/load_sample.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d3006fdaa3d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/home/ubuntu/CE_scheme/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mSchemas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgen_imdb_schema\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mJoin_scheme\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_prepare\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mread_table_csv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mJoin_scheme\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinning\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0midentify_key_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/CE_scheme/Join_scheme/data_prepare.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mSchemas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgen_stats_light_schema\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mJoin_scheme\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinning\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0midentify_key_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_optimal_bucketize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTable_bucket\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mSampling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_sample\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_ground_truth_no_filter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/CE_scheme/Sampling/load_sample.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mJoin_scheme\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbound\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFactor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mJoin_scheme\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinning\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapply_binning_to_data_value_count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/CE_scheme/Join_scheme/bound.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mJoin_scheme\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin_graph\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_join_hyper_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_query_all_join\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mJoin_scheme\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_prepare\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0midentify_key_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mSampling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_sample\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_sample_imdb_one_query\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'load_sample_imdb_one_query' from 'Sampling.load_sample' (/home/ubuntu/CE_scheme/Sampling/load_sample.py)"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "sys.path.append(\"/home/ubuntu/CE_scheme/\")\n",
    "from Schemas.imdb.schema import gen_imdb_schema\n",
    "from Join_scheme.data_prepare import read_table_csv\n",
    "from Join_scheme.binning import identify_key_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c8fa69d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'get_ground_truth_no_filter' from 'Sampling.load_sample' (/home/ubuntu/CE_scheme/Sampling/load_sample.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-3c457d49c8c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mSampling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_sample\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_sample_imdb_one_query\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/CE_scheme/Sampling/load_sample.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mJoin_scheme\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbound\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFactor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mJoin_scheme\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinning\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapply_binning_to_data_value_count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/CE_scheme/Join_scheme/bound.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mJoin_scheme\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin_graph\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_join_hyper_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_query_all_join\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mJoin_scheme\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_prepare\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0midentify_key_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mSampling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_sample\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_sample_imdb_one_query\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/CE_scheme/Join_scheme/data_prepare.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mSchemas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgen_stats_light_schema\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mJoin_scheme\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinning\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0midentify_key_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_optimal_bucketize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTable_bucket\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mSampling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_sample\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_ground_truth_no_filter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'get_ground_truth_no_filter' from 'Sampling.load_sample' (/home/ubuntu/CE_scheme/Sampling/load_sample.py)"
     ]
    }
   ],
   "source": [
    "from Sampling.load_sample import load_sample_imdb_one_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1cf74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "\n",
    "class Bucket:\n",
    "    \"\"\"\n",
    "    The class of bucketization of a key attribute\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name, bins=[], bin_modes=[], bin_vars=[], bin_means=[], rest_bins_remaining=None):\n",
    "        self.name = name\n",
    "        self.bins = bins\n",
    "        self.bin_modes = bin_modes\n",
    "        self.bin_vars = bin_vars\n",
    "        self.bin_means = bin_means\n",
    "        self.rest_bins_remaining = rest_bins_remaining\n",
    "        if len(bins) != 0:\n",
    "            assert len(bins) == len(bin_modes)\n",
    "\n",
    "\n",
    "class Table_bucket:\n",
    "    \"\"\"\n",
    "    The class of bucketization for all key attributes in a table.\n",
    "    Supporting more than three dimensional bin modes requires simplifying the causal structure\n",
    "    \"\"\"\n",
    "    def __init__(self, table_name, id_attributes, bin_sizes, oned_bin_modes=None):\n",
    "        self.table_name = table_name\n",
    "        self.id_attributes = id_attributes\n",
    "        self.bin_sizes = bin_sizes\n",
    "        if oned_bin_modes:\n",
    "            self.oned_bin_modes = oned_bin_modes\n",
    "        else:\n",
    "            self.oned_bin_modes = dict()\n",
    "        self.twod_bin_modes = dict()\n",
    "\n",
    "\n",
    "class Bucket_group:\n",
    "    \"\"\"\n",
    "    The class of bucketization for a group of equivalent join keys\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, buckets, start_key, sample_rate, bins=None, primary_keys=[]):\n",
    "        self.buckets = buckets\n",
    "        self.start_key = start_key\n",
    "        self.sample_rate = sample_rate\n",
    "        self.bins = bins\n",
    "        self.primary_keys = primary_keys\n",
    "\n",
    "    def bucketize(self, data):\n",
    "        \"\"\"\n",
    "        Discretize data based on the bucket\n",
    "        \"\"\"\n",
    "        res = dict()\n",
    "        seen_remain_key = np.array([])\n",
    "        cumulative_bin = copy.deepcopy(self.buckets[self.start_key].bins)\n",
    "        start_means = np.asarray(self.buckets[self.start_key].bin_means)\n",
    "\n",
    "        for key in data:\n",
    "            if key in self.primary_keys:\n",
    "                continue\n",
    "            res[key] = copy.deepcopy(data[key])\n",
    "            if key != self.start_key:\n",
    "                unique_remain = np.setdiff1d(self.buckets[key].rest_bins_remaining, seen_remain_key)\n",
    "                assert sum([np.sum(np.isin(unique_remain, b) == 1) for b in cumulative_bin]) == 0\n",
    "\n",
    "                if len(unique_remain) != 0:\n",
    "                    remaining_data = data[key][np.isin(data[key], unique_remain)]\n",
    "                    unique_remain, count_remain = np.unique(remaining_data, return_counts=True)\n",
    "                    unique_counts = np.unique(count_remain)\n",
    "                    for u in unique_counts:\n",
    "                        temp_idx = np.searchsorted(start_means, u)\n",
    "                        if temp_idx == len(cumulative_bin):\n",
    "                            idx = -1\n",
    "                            if u > self.buckets[key].bin_modes[-1]:\n",
    "                                self.buckets[key].bin_modes[-1] = u\n",
    "                        elif temp_idx == 0:\n",
    "                            idx = 0\n",
    "                        else:\n",
    "                            if (u - start_means[temp_idx - 1]) >= (start_means[temp_idx] - u):\n",
    "                                idx = temp_idx - 1\n",
    "                            else:\n",
    "                                idx = temp_idx\n",
    "                        temp_unique = unique_remain[count_remain == u]\n",
    "                        cumulative_bin[idx] = np.concatenate((cumulative_bin[idx], temp_unique))\n",
    "                        seen_remain_key = np.concatenate((seen_remain_key, temp_unique))\n",
    "                        if u > self.buckets[key].bin_modes[idx]:\n",
    "                            self.buckets[key].bin_modes[idx] = u\n",
    "            res[key] = copy.deepcopy(data[key])\n",
    "            count = 0\n",
    "            for i, b in enumerate(cumulative_bin):\n",
    "                count += len(data[key][np.isin(data[key], b)])\n",
    "                res[key][np.isin(data[key], b)] = i\n",
    "\n",
    "        self.bins = cumulative_bin\n",
    "\n",
    "        for key in data:\n",
    "            if key in self.primary_keys:\n",
    "                res[key] = self.bucketize_PK(data[key])\n",
    "                self.buckets[key] = Bucket(key, bin_modes=np.ones(len(self.bins)))\n",
    "        \n",
    "        for key in data:\n",
    "            if key in self.primary_keys:\n",
    "                continue\n",
    "            if self.sample_rate[key] < 1.0:\n",
    "                bin_modes = np.asarray(self.buckets[key].bin_modes)\n",
    "                bin_modes[bin_modes != 1] = bin_modes[bin_modes != 1] / self.sample_rate[key]\n",
    "                self.buckets[key].bin_modes = bin_modes\n",
    "        \n",
    "        return res\n",
    "\n",
    "    def bucketize_PK(self, data):\n",
    "        res = copy.deepcopy(data)\n",
    "        remaining_data = np.unique(data)\n",
    "        for i, b in enumerate(self.bins):\n",
    "            res[np.isin(data, b)] = i\n",
    "            remaining_data = np.setdiff1d(remaining_data, b)\n",
    "        if len(remaining_data) != 0:\n",
    "            self.bins.append(list(remaining_data))\n",
    "            for key in self.buckets:\n",
    "                if key not in self.primary_keys:\n",
    "                    self.buckets[key].bin_modes = np.append(self.buckets[key].bin_modes, 0)\n",
    "        res[np.isin(data, remaining_data)] = len(self.bins)\n",
    "        return res\n",
    "\n",
    "\n",
    "\n",
    "def identify_key_values(schema):\n",
    "    \"\"\"\n",
    "    identify all the key attributes from the schema of a DB, currently we assume all possible joins are known\n",
    "    It is also easy to support unseen joins, which we left as a future work.\n",
    "    :param schema: the schema of a DB\n",
    "    :return: a dict of all keys, {table: [keys]};\n",
    "             a dict of set, each indicating which keys on different tables are considered the same key.\n",
    "    \"\"\"\n",
    "    all_keys = set()\n",
    "    equivalent_keys = dict()\n",
    "    for i, join in enumerate(schema.relationships):\n",
    "        keys = join.identifier.split(\" = \")\n",
    "        all_keys.add(keys[0])\n",
    "        all_keys.add(keys[1])\n",
    "        seen = False\n",
    "        for k in equivalent_keys:\n",
    "            if keys[0] in equivalent_keys[k]:\n",
    "                equivalent_keys[k].add(keys[1])\n",
    "                seen = True\n",
    "                break\n",
    "            elif keys[1] in equivalent_keys[k]:\n",
    "                equivalent_keys[k].add(keys[0])\n",
    "                seen = True\n",
    "                break\n",
    "        if not seen:\n",
    "            # set the keys[-1] as the identifier of this equivalent join key group for convenience.\n",
    "            equivalent_keys[keys[-1]] = set(keys)\n",
    "\n",
    "    assert len(all_keys) == sum([len(equivalent_keys[k]) for k in equivalent_keys])\n",
    "    return all_keys, equivalent_keys\n",
    "\n",
    "\n",
    "def equal_freq_binning(name, data, n_bins, data_len):\n",
    "    uniques, counts = data\n",
    "    if len(uniques) <= n_bins:\n",
    "        bins = []\n",
    "        bin_modes = []\n",
    "        bin_vars = []\n",
    "        bin_means = []\n",
    "        \n",
    "        for i, uni in enumerate(uniques):\n",
    "            bins.append([uni])\n",
    "            bin_modes.append(counts[i])\n",
    "            bin_vars.append(0)\n",
    "            bin_means.append(counts[i])\n",
    "        return Bucket(name, bins, bin_modes, bin_vars, bin_means)\n",
    "    \n",
    "    unique_counts, count_counts = np.unique(counts, return_counts=True)\n",
    "    idx = np.argsort(unique_counts)\n",
    "    unique_counts = unique_counts[idx]\n",
    "    count_counts = count_counts[idx]\n",
    "\n",
    "    bins = []\n",
    "    bin_modes = []\n",
    "    bin_vars = []\n",
    "    bin_means = []\n",
    "\n",
    "    bin_freq = data_len / n_bins\n",
    "    cur_freq = 0\n",
    "    cur_bin = []\n",
    "    cur_bin_count = []\n",
    "    for i, uni_c in enumerate(unique_counts):\n",
    "        cur_freq += count_counts[i] * uni_c\n",
    "        cur_bin.append(uniques[np.where(counts == uni_c)[0]])\n",
    "        cur_bin_count.extend([uni_c] * count_counts[i])\n",
    "        if (cur_freq > bin_freq) or (i == (len(unique_counts) - 1)):\n",
    "            bins.append(np.concatenate(cur_bin))\n",
    "            cur_bin_count = np.asarray(cur_bin_count)\n",
    "            bin_modes.append(uni_c)\n",
    "            bin_means.append(np.mean(cur_bin_count))\n",
    "            bin_vars.append(np.var(cur_bin_count))\n",
    "            cur_freq = 0\n",
    "            cur_bin = []\n",
    "            cur_bin_count = []\n",
    "    assert len(uniques) == sum([len(b) for b in bins]), f\"some unique values missed or duplicated\"\n",
    "    return Bucket(name, bins, bin_modes, bin_vars, bin_means)\n",
    "\n",
    "\n",
    "def compute_variance_score(buckets):\n",
    "    \"\"\"\n",
    "    compute the variance of products of random variables\n",
    "    \"\"\"\n",
    "    all_mean = np.asarray([buckets[k].bin_means for k in buckets])\n",
    "    all_var = np.asarray([buckets[k].bin_vars for k in buckets])\n",
    "    return np.sum(np.prod(all_var + all_mean ** 2, axis=0) - np.prod(all_mean, axis=0) ** 2)\n",
    "\n",
    "\n",
    "def sub_optimal_bucketize(data, sample_rate, n_bins=30, primary_keys=[]):\n",
    "    \"\"\"\n",
    "    Perform sub-optimal bucketization on a group of equivalent join keys.\n",
    "    :param data: a dict of (potentially sampled) table data of the keys\n",
    "                 the keys of this dict are one group of equivalent join keys\n",
    "    :param sample_rate: the sampling rate the data, could be all 1 if no sampling is performed\n",
    "    :param n_bins: how many bins can we allocate\n",
    "    :param primary_keys: the primary keys in the equivalent group since we don't need to bucketize PK.\n",
    "    :return: new data, where the keys are bucketized\n",
    "             the mode of each bucket\n",
    "    \"\"\"\n",
    "    unique_values = dict()\n",
    "    for key in data:\n",
    "        if key not in primary_keys:\n",
    "            unique_values[key] = np.unique(data[key], return_counts=True)\n",
    "\n",
    "    best_variance_score = np.infty\n",
    "    best_bin_len = 0\n",
    "    best_start_key = None\n",
    "    best_buckets = None\n",
    "    for start_key in data:\n",
    "        if start_key in primary_keys:\n",
    "            continue\n",
    "        start_bucket = equal_freq_binning(start_key, unique_values[start_key], n_bins, len(data[start_key]))\n",
    "        rest_buckets = dict()\n",
    "        for key in data:\n",
    "            if key == start_key or key in primary_keys:\n",
    "                continue\n",
    "            uniques = unique_values[key][0]\n",
    "            counts = unique_values[key][1]\n",
    "            rest_buckets[key] = Bucket(key, [], [0] * len(start_bucket.bins), [0] * len(start_bucket.bins),\n",
    "                                       [0] * len(start_bucket.bins), uniques)\n",
    "            for i, bin in enumerate(start_bucket.bins):\n",
    "                idx = np.where(np.isin(uniques, bin) == 1)[0]\n",
    "                if len(idx) != 0:\n",
    "                    bin_count = counts[idx]\n",
    "                    unique_bin_keys = uniques[idx]\n",
    "                    # unique_bin_count = np.unique(bin_count)\n",
    "                    # bin_count = np.concatenate([counts[counts == j] for j in unique_bin_count])\n",
    "                    # unique_bin_keys = np.concatenate([uniques[counts == j] for j in unique_bin_count])\n",
    "                    rest_buckets[key].rest_bins_remaining = np.setdiff1d(rest_buckets[key].rest_bins_remaining,\n",
    "                                                                         unique_bin_keys)\n",
    "                    rest_buckets[key].bin_modes[i] = np.max(bin_count)\n",
    "                    rest_buckets[key].bin_vars[i] = np.var(bin_count)\n",
    "                    rest_buckets[key].bin_means[i] = np.mean(bin_count)\n",
    "\n",
    "        rest_buckets[start_key] = start_bucket\n",
    "        var_score = compute_variance_score(rest_buckets)\n",
    "        if len(start_bucket.bins) > best_bin_len:\n",
    "            best_variance_score = var_score\n",
    "            best_start_key = start_key\n",
    "            best_buckets = rest_buckets\n",
    "            best_bin_len = len(start_bucket.bins)\n",
    "        elif len(start_bucket.bins) >= best_bin_len * 0.9 and var_score < best_variance_score:\n",
    "            best_variance_score = var_score\n",
    "            best_start_key = start_key\n",
    "            best_buckets = rest_buckets\n",
    "            best_bin_len = len(start_bucket.bins)\n",
    "    \n",
    "    best_buckets = Bucket_group(best_buckets, best_start_key, sample_rate, primary_keys=primary_keys)\n",
    "    new_data = best_buckets.bucketize(data)\n",
    "    return new_data, best_buckets\n",
    "\n",
    "\n",
    "def fixed_start_key_bucketize(start_key, data, sample_rate, n_bins=30, primary_keys=[]):\n",
    "    \"\"\"\n",
    "    Perform sub-optimal bucketization on a group of equivalent join keys based on the pre-defined start_key.\n",
    "    :param data: a dict of (potentially sampled) table data of the keys\n",
    "                 the keys of this dict are one group of equivalent join keys\n",
    "    :param sample_rate: the sampling rate the data, could be all 1 if no sampling is performed\n",
    "    :param n_bins: how many bins can we allocate\n",
    "    :param primary_keys: the primary keys in the equivalent group since we don't need to bucketize PK.\n",
    "    :return: new data, where the keys are bucketized\n",
    "             the mode of each bucket\n",
    "    \"\"\"\n",
    "    unique_values = dict()\n",
    "    for key in data:\n",
    "        if key not in primary_keys:\n",
    "            unique_values[key] = np.unique(data[key], return_counts=True)\n",
    "\n",
    "    start_bucket = equal_freq_binning(start_key, unique_values[start_key], n_bins, len(data[start_key]))\n",
    "    rest_buckets = dict()\n",
    "    for key in data:\n",
    "        if key == start_key or key in primary_keys:\n",
    "            continue\n",
    "        uniques = unique_values[key][0]\n",
    "        counts = unique_values[key][1]\n",
    "        rest_buckets[key] = Bucket(key, [], [0] * len(start_bucket.bins), [0] * len(start_bucket.bins),\n",
    "                                   [0] * len(start_bucket.bins), uniques)\n",
    "        for i, bin in enumerate(start_bucket.bins):\n",
    "            idx = np.where(np.isin(uniques, bin) == 1)[0]\n",
    "            if len(idx) != 0:\n",
    "                bin_count = counts[idx]\n",
    "                unique_bin_keys = uniques[idx]\n",
    "                rest_buckets[key].rest_bins_remaining = np.setdiff1d(rest_buckets[key].rest_bins_remaining,\n",
    "                                                                     unique_bin_keys)\n",
    "                rest_buckets[key].bin_modes[i] = np.max(bin_count)\n",
    "                rest_buckets[key].bin_means[i] = np.mean(bin_count)\n",
    "\n",
    "    best_buckets = Bucket_group(rest_buckets, start_key, sample_rate, primary_keys=primary_keys)\n",
    "    new_data = best_buckets.bucketize(data)\n",
    "    return new_data, best_buckets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d81e2b3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'identify_key_values' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-b354848a618d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdata_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/home/ubuntu/data_CE/{}.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_imdb_schema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mall_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mequivalent_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midentify_key_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"new_table_buckets.pkl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtable_buckets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'identify_key_values' is not defined"
     ]
    }
   ],
   "source": [
    "data_path = \"/home/ubuntu/data_CE/{}.csv\"\n",
    "schema = gen_imdb_schema(data_path)\n",
    "all_keys, equivalent_keys = identify_key_values(schema)\n",
    "with open(\"new_table_buckets.pkl\", \"rb\") as f:\n",
    "    table_buckets = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabd33ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/ubuntu/data_CE/job/job_sub_plan_queries.txt\", \"r\") as f:\n",
    "    sub_plan_queries = f.read()\n",
    "psql_raw = sub_plan_queries.split(\"query: 0\")[1:]\n",
    "queries = []\n",
    "q_file_names = []\n",
    "query_path = \"/home/ubuntu/data_CE/job/\"\n",
    "for file in os.listdir(query_path):\n",
    "    if file.endswith(\".sql\") and file[0].isnumeric():\n",
    "        q_file_names.append(file.split(\".sql\")[0]+\".pkl\")\n",
    "        with open(query_path+file, \"r\") as f:\n",
    "            q = f.readline()\n",
    "            queries.append(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c46b7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "psql_raw = sub_plan_queries.split(\"query: 0\")[1:]\n",
    "sub_plan_queries_str_all = []\n",
    "for per_query in psql_raw:\n",
    "    sub_plan_queries = []\n",
    "    sub_plan_queries_str = []\n",
    "    num_sub_plan_queries = len(per_query.split(\"query: \"))\n",
    "    all_info = per_query.split(\"RELOPTINFO (\")[1:]\n",
    "    assert num_sub_plan_queries*2 == len(all_info)\n",
    "    for i in range(num_sub_plan_queries):\n",
    "        idx = i*2\n",
    "        table1 = all_info[idx].split(\"): rows=\")[0]\n",
    "        table2 = all_info[idx+1].split(\"): rows=\")[0]\n",
    "        table_str = (table1, table2)\n",
    "        sub_plan_queries_str.append(table_str)\n",
    "    sub_plan_queries_str_all.append(sub_plan_queries_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a870ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "from Join_scheme.join_graph import get_join_hyper_graph, parse_query_all_join\n",
    "from Join_scheme.data_prepare import identify_key_values\n",
    "#from Sampling.load_sample import load_sample_imdb_one_query\n",
    "\n",
    "\n",
    "class Factor:\n",
    "    \"\"\"\n",
    "    This the class defines a multidimensional conditional probability on one table.\n",
    "    \"\"\"\n",
    "    def __init__(self, table, table_len, variables, pdfs, na_values=None):\n",
    "        self.table = table\n",
    "        self.table_len = table_len\n",
    "        self.variables = variables\n",
    "        self.pdfs = pdfs\n",
    "        self.na_values = na_values  # this is the percentage of data, which is not nan.\n",
    "\n",
    "\n",
    "class Group_Factor:\n",
    "    \"\"\"\n",
    "        This the class defines a multidimensional conditional probability on a group of tables.\n",
    "    \"\"\"\n",
    "    def __init__(self, tables, tables_size, pdfs, bin_modes, equivalent_groups=None, table_key_equivalent_group=None,\n",
    "                 na_values=None, join_cond=None):\n",
    "        self.table = tables\n",
    "        self.tables_size = tables_size\n",
    "        self.pdfs = pdfs\n",
    "        self.bin_modes = bin_modes\n",
    "        self.equivalent_groups = equivalent_groups\n",
    "        self.table_key_equivalent_group = table_key_equivalent_group\n",
    "        self.na_values = na_values\n",
    "        self.join_cond = join_cond\n",
    "\n",
    "\n",
    "class Bound_ensemble:\n",
    "    \"\"\"\n",
    "    This the class where we store all the trained models and perform inference on the bound.\n",
    "    \"\"\"\n",
    "    def __init__(self, table_buckets, schema, ground_truth_factors_no_filter=None):\n",
    "        self.table_buckets = table_buckets\n",
    "        self.schema = schema\n",
    "        self.all_keys, self.equivalent_keys = identify_key_values(schema)\n",
    "        self.all_join_conds = None\n",
    "        self.ground_truth_factors_no_filter = ground_truth_factors_no_filter\n",
    "        #self.reverse_table_alias = None\n",
    "\n",
    "    def parse_query_simple(self, query):\n",
    "        \"\"\"\n",
    "        If your selection query contains no aggregation and nested sub-queries, you can use this function to parse a\n",
    "        join query. Otherwise, use parse_query function.\n",
    "        \"\"\"\n",
    "        tables_all, join_cond, join_keys = parse_query_all_join(query)\n",
    "        #TODO: implement functions on parsing filter conditions.\n",
    "        table_filters = dict()\n",
    "        return tables_all, table_filters, join_cond, join_keys\n",
    "\n",
    "    def get_all_id_conidtional_distribution(self, query_file_name, tables_alias, join_keys):\n",
    "        #TODO: make it work on query-driven and sampling based\n",
    "        return load_sample_imdb_one_query(self.table_buckets, tables_alias, query_file_name, join_keys,\n",
    "                                          self.ground_truth_factors_no_filter)\n",
    "\n",
    "\n",
    "    def eliminate_one_key_group(self, tables, key_group, factors, relevant_keys):\n",
    "        \"\"\"This version only supports 2D distributions (i.e. the distribution learned with tree-structured PGM)\"\"\"\n",
    "        rest_group = None\n",
    "        rest_group_cardinalty = 0\n",
    "        eliminated_tables = []\n",
    "        rest_group_tables = []\n",
    "        for table in tables:\n",
    "            assert key_group in factors[table].equivalent_variables\n",
    "            temp = copy.deepcopy(factors[table].equivalent_variables)\n",
    "            temp.remove(key_group)\n",
    "            if len(temp) == 0:\n",
    "                eliminated_tables.append(table)\n",
    "            for key in temp:\n",
    "                if rest_group:\n",
    "                    assert factors[table].cardinalities[key] == rest_group_cardinalty\n",
    "                    rest_group_tables.append(table)\n",
    "                else:\n",
    "                    rest_group = key\n",
    "                    rest_group_cardinalty = factors[table].cardinalities[key]\n",
    "                    rest_group_tables = [table]\n",
    "\n",
    "        all_probs_eliminated = []\n",
    "        all_modes_eliminated = []\n",
    "        for table in eliminated_tables:\n",
    "            bin_modes = self.table_buckets[table].oned_bin_modes[relevant_keys[key_group][table]]\n",
    "            all_probs_eliminated.append(factors[table].pdfs)\n",
    "            all_modes_eliminated.append(np.minimum(bin_modes, factors[table].pdfs))\n",
    "\n",
    "        if rest_group:\n",
    "            new_factor_pdf = np.zeros(rest_group_cardinalty)\n",
    "        else:\n",
    "            return self.compute_bound_oned(all_probs_eliminated, all_modes_eliminated)\n",
    "\n",
    "        for i in range(rest_group_cardinalty):\n",
    "            for table in rest_group_tables:\n",
    "                idx_f = factors[table].equivalent_variables.index(key_group)\n",
    "                idx_b = self.table_buckets[table].id_attributes.index(relevant_keys[key_group][table])\n",
    "                bin_modes = self.table_buckets[table].twod_bin_modes[relevant_keys[key_group][table]]\n",
    "                if idx_f == 0 and idx_b == 0:\n",
    "                    all_probs_eliminated.append(factors[table].pdfs[:, i])\n",
    "                    all_modes_eliminated.append(np.minimum(bin_modes[:, i], factors[table].pdfs[:, i]))\n",
    "                elif idx_f == 0 and idx_b == 1:\n",
    "                    all_probs_eliminated.append(factors[table].pdfs[:, i])\n",
    "                    all_modes_eliminated.append(np.minimum(bin_modes[i, :], factors[table].pdfs[:, i]))\n",
    "                elif idx_f == 1 and idx_b == 0:\n",
    "                    all_probs_eliminated.append(factors[table].pdfs[i, :])\n",
    "                    all_modes_eliminated.append(np.minimum(bin_modes[:, i], factors[table].pdfs[i, :]))\n",
    "                else:\n",
    "                    all_probs_eliminated.append(factors[table].pdfs[i, :])\n",
    "                    all_modes_eliminated.append(np.minimum(bin_modes[i, :], factors[table].pdfs[i, :]))\n",
    "            new_factor_pdf[i] = self.compute_bound_oned(all_probs_eliminated, all_modes_eliminated)\n",
    "\n",
    "        for table in rest_group_tables:\n",
    "            factors[table] = Factor([rest_group], new_factor_pdf, [rest_group])\n",
    "\n",
    "        return None\n",
    "\n",
    "    def compute_bound_oned(self, all_probs, all_modes, return_factor=False):\n",
    "        temp_all_modes = []\n",
    "        for i in range(len(all_modes)):\n",
    "            temp_all_modes.append(np.minimum(all_probs[i], all_modes[i]))\n",
    "        all_probs = np.stack(all_probs, axis=0)\n",
    "        temp_all_modes = np.stack(temp_all_modes, axis=0)\n",
    "        multiplier = np.prod(temp_all_modes, axis=0)\n",
    "        non_zero_idx = np.where(multiplier != 0)[0]\n",
    "        min_number = np.amin(all_probs[:, non_zero_idx]/temp_all_modes[:, non_zero_idx], axis=0)\n",
    "        #print(min_number, multiplier[non_zero_idx])\n",
    "        if return_factor:\n",
    "            new_probs = np.zeros(multiplier.shape)\n",
    "            new_probs[non_zero_idx] = multiplier[non_zero_idx] * min_number\n",
    "            return new_probs, multiplier\n",
    "        else:\n",
    "            multiplier[non_zero_idx] = multiplier[non_zero_idx] * min_number\n",
    "            return np.sum(multiplier)\n",
    "\n",
    "    def get_optimal_elimination_order(self, equivalent_group, join_keys, factors):\n",
    "        \"\"\"\n",
    "        This function determines the optimial elimination order for each key group\n",
    "        \"\"\"\n",
    "        cardinalities = dict()\n",
    "        lengths = dict()\n",
    "        tables_involved = dict()\n",
    "        relevant_keys = dict()\n",
    "        for group in equivalent_group:\n",
    "            relevant_keys[group] = dict()\n",
    "            lengths[group] = len(equivalent_group[group])\n",
    "            cardinalities[group] = []\n",
    "            tables_involved[group] = set([])\n",
    "            for keys in equivalent_group[group]:\n",
    "                for table in join_keys:\n",
    "                    if keys in join_keys[table]:\n",
    "                        cardinalities[group].append(len(join_keys[table]))\n",
    "                        tables_involved[group].add(table)\n",
    "                        variables = factors[table].variables\n",
    "                        variables[variables.index(keys)] = group\n",
    "                        factors[table].variables = variables\n",
    "                        relevant_keys[group][table] = keys\n",
    "                        break\n",
    "            cardinalities[group] = np.asarray(cardinalities[group])\n",
    "\n",
    "        optimal_order = list(equivalent_group.keys())\n",
    "        for i in range(len(optimal_order)):\n",
    "            min_idx = i\n",
    "            for j in range(i+1, len(optimal_order)):\n",
    "                min_group = optimal_order[min_idx]\n",
    "                curr_group = optimal_order[j]\n",
    "                if np.max(cardinalities[curr_group]) < np.max(cardinalities[min_group]):\n",
    "                    min_idx = j\n",
    "                else:\n",
    "                    min_max_tables = np.max(cardinalities[min_group])\n",
    "                    min_num_max_tables = len(np.where(cardinalities[min_group] == min_max_tables)[0])\n",
    "                    curr_max_tables = np.max(cardinalities[curr_group])\n",
    "                    curr_num_max_tables = len(np.where(cardinalities[curr_group] == curr_max_tables)[0])\n",
    "                    if curr_num_max_tables < min_num_max_tables:\n",
    "                        min_idx = j\n",
    "                    elif lengths[curr_group] < lengths[min_group]:\n",
    "                        min_idx = j\n",
    "            optimal_order[i], optimal_order[min_idx] = optimal_order[min_idx], optimal_order[i]\n",
    "        return optimal_order, tables_involved, relevant_keys\n",
    "\n",
    "    def get_cardinality_bound_one(self, query_str):\n",
    "        tables_all, table_queries, join_cond, join_keys = self.parse_query_simple(query_str)\n",
    "        equivalent_group = get_join_hyper_graph(join_keys, self.equivalent_keys)\n",
    "        conditional_factors = self.get_all_id_conidtional_distribution(table_queries, join_keys, equivalent_group)\n",
    "        optimal_order, tables_involved, relevant_keys = self.get_optimal_elimination_order(equivalent_group, join_keys,\n",
    "                                                                            conditional_factors)\n",
    "\n",
    "        for key_group in optimal_order:\n",
    "            tables = tables_involved[key_group]\n",
    "            res = self.eliminate_one_key_group(tables, key_group, conditional_factors, relevant_keys)\n",
    "        return res\n",
    "\n",
    "    def get_sub_plan_queries_sql(self, query_str, sub_plan_query_str_all, query_name=None):\n",
    "        tables_all, table_queries, join_cond, join_keys = self.parse_query_simple(query_str)\n",
    "        equivalent_group, table_equivalent_group, table_key_equivalent_group = get_join_hyper_graph(join_keys,\n",
    "                                                                                            self.equivalent_keys)\n",
    "        cached_sub_queries_sql = dict()\n",
    "        cached_union_key_group = dict()\n",
    "        res_sql = []\n",
    "        for (left_tables, right_tables) in sub_plan_query_str_all:\n",
    "            assert \" \" not in left_tables, f\"{left_tables} contains more than one tables, violating left deep plan\"\n",
    "            sub_plan_query_list = right_tables.split(\" \") + [left_tables]\n",
    "            sub_plan_query_list.sort()\n",
    "            sub_plan_query_str = \" \".join(sub_plan_query_list)  # get the string name of the sub plan query\n",
    "            sql_header = \"SELECT COUNT(*) FROM \"\n",
    "            for alias in sub_plan_query_list:\n",
    "                sql_header += (tables_all[alias] + \" AS \" + alias + \", \")\n",
    "            sql_header = sql_header[:-2] + \" WHERE \"\n",
    "            if \" \" in right_tables:\n",
    "                assert right_tables in cached_sub_queries_sql, f\"{right_tables} not in cache, input is not ordered\"\n",
    "                right_sql = cached_sub_queries_sql[right_tables]\n",
    "                right_union_key_group = cached_union_key_group[right_tables]\n",
    "                if left_tables in table_queries:\n",
    "                    left_sql = table_queries[left_tables]\n",
    "                    curr_sql = right_sql + \" AND (\" + left_sql + \")\"\n",
    "                else:\n",
    "                    curr_sql = right_sql\n",
    "                additional_joins, union_key_group = self.get_additional_join_with_table_group(left_tables,\n",
    "                                                                        right_union_key_group,\n",
    "                                                                        table_equivalent_group,\n",
    "                                                                        table_key_equivalent_group)\n",
    "                for join in additional_joins:\n",
    "                    curr_sql = curr_sql + \" AND \" + join\n",
    "            else:\n",
    "                curr_sql = \"\"\n",
    "                if left_tables in table_queries:\n",
    "                    curr_sql += (\"(\" + table_queries[left_tables] + \")\")\n",
    "                if right_tables in table_queries:\n",
    "                    if curr_sql != \"\":\n",
    "                        curr_sql += \" AND \"\n",
    "                    curr_sql += (\"(\" + table_queries[right_tables] + \")\")\n",
    "\n",
    "                additional_joins, union_key_group = self.get_additional_joins_two_tables(left_tables, right_tables,\n",
    "                                                                        table_equivalent_group,\n",
    "                                                                        table_key_equivalent_group)\n",
    "                for join in additional_joins:\n",
    "                    if curr_sql == \"\":\n",
    "                        curr_sql += join\n",
    "                    else:\n",
    "                        curr_sql = curr_sql + \" AND \" + join\n",
    "            cached_sub_queries_sql[sub_plan_query_str] = curr_sql\n",
    "            cached_union_key_group[sub_plan_query_str] = union_key_group\n",
    "            res_sql.append(sql_header + curr_sql + \";\")\n",
    "        return res_sql\n",
    "\n",
    "    def get_cardinality_bound_all(self, query_str, sub_plan_query_str_all, query_name=None, debug=False, true_card=None):\n",
    "        \"\"\"\n",
    "        Get the cardinality bounds for all sub_plan_queires of a query.\n",
    "        Note: Due to efficiency, this current version only support left_deep plans (like the one generated by postgres),\n",
    "              but it can easily support right deep or bushy plans.\n",
    "        :param query_str: the target query\n",
    "        :param sub_plan_query_str_all: all sub_plan_queries of the target query,\n",
    "               it should be sorted by number of the tables in the sub_plan_query\n",
    "        \"\"\"\n",
    "        tables_all, table_queries, join_cond, join_keys = self.parse_query_simple(query_str)\n",
    "        #print(join_cond)\n",
    "        #print(join_keys)\n",
    "        equivalent_group, table_equivalent_group, table_key_equivalent_group, table_key_group_map = \\\n",
    "            get_join_hyper_graph(join_keys, self.equivalent_keys)\n",
    "        conditional_factors = self.get_all_id_conidtional_distribution(query_name, tables_all, join_keys)\n",
    "        #self.reverse_table_alias = {v: k for k, v in tables_all.items()}\n",
    "        cached_sub_queries = dict()\n",
    "        cardinality_bounds = []\n",
    "        for i, (left_tables, right_tables) in enumerate(sub_plan_query_str_all):\n",
    "            assert \" \" not in left_tables, f\"{left_tables} contains more than one tables, violating left deep plan\"\n",
    "            sub_plan_query_list = right_tables.split(\" \") + [left_tables]\n",
    "            sub_plan_query_list.sort()\n",
    "            sub_plan_query_str = \" \".join(sub_plan_query_list)  #get the string name of the sub plan query\n",
    "            #print(sub_plan_query_str)\n",
    "            if \" \" in right_tables:\n",
    "                assert right_tables in cached_sub_queries, f\"{right_tables} not in cache, input is not ordered\"\n",
    "                right_bound_factor = cached_sub_queries[right_tables]\n",
    "                curr_bound_factor, res = self.join_with_one_table(sub_plan_query_str,\n",
    "                                                                  left_tables,\n",
    "                                                                  tables_all,\n",
    "                                                                  right_bound_factor,\n",
    "                                                                  conditional_factors[left_tables],\n",
    "                                                                  table_equivalent_group,\n",
    "                                                                  table_key_equivalent_group,\n",
    "                                                                  table_key_group_map,\n",
    "                                                                  join_cond)\n",
    "            else:\n",
    "                curr_bound_factor, res = self.join_two_tables(sub_plan_query_str,\n",
    "                                                              left_tables,\n",
    "                                                              right_tables,\n",
    "                                                              tables_all,\n",
    "                                                              conditional_factors,\n",
    "                                                              join_keys,\n",
    "                                                              table_equivalent_group,\n",
    "                                                              table_key_equivalent_group,\n",
    "                                                              table_key_group_map,\n",
    "                                                              join_cond)\n",
    "            cached_sub_queries[sub_plan_query_str] = curr_bound_factor\n",
    "            res = max(res, 1)\n",
    "            if debug:\n",
    "                if true_card[i] == -1:\n",
    "                    error = \"NA\"\n",
    "                else:\n",
    "                    error = max(res/true_card[i], true_card[i]/res)\n",
    "                print(f\"{left_tables}, {right_tables}|| estimate: {res}, true: {true_card[i]}, error: {error}\")\n",
    "            cardinality_bounds.append(res)\n",
    "        return cardinality_bounds\n",
    "\n",
    "\n",
    "    def join_with_one_table(self, sub_plan_query_str, left_table, tables_all, right_bound_factor, cond_factor_left,\n",
    "                            table_equivalent_group, table_key_equivalent_group, table_key_group_map, join_cond):\n",
    "        \"\"\"\n",
    "        Get the cardinality bound by joining the left_table with the seen right_tables\n",
    "        :param left_table:\n",
    "        :param right_tables:\n",
    "        \"\"\"\n",
    "        equivalent_key_group, union_key_group_set, union_key_group, new_join_cond = \\\n",
    "            self.get_join_keys_with_table_group(left_table, right_bound_factor, tables_all, table_equivalent_group,\n",
    "                                                table_key_equivalent_group, table_key_group_map, join_cond)\n",
    "        bin_mode_left = self.table_buckets[tables_all[left_table]].oned_bin_modes\n",
    "        bin_mode_right = right_bound_factor.bin_modes\n",
    "        key_group_pdf = dict()\n",
    "        key_group_bin_mode = dict()\n",
    "        new_union_key_group = dict()\n",
    "        new_na_values = dict()\n",
    "        res = right_bound_factor.tables_size\n",
    "        print(\"==================================================\")\n",
    "        for key_group in equivalent_key_group:\n",
    "            print(key_group, equivalent_key_group[key_group], res)\n",
    "            #print(cond_factor_left.na_values)\n",
    "            #print(right_bound_factor.na_values)\n",
    "            all_pdfs = [cond_factor_left.pdfs[key] * cond_factor_left.table_len * cond_factor_left.na_values[key]\n",
    "                        for key in equivalent_key_group[key_group][\"left\"]] + \\\n",
    "                       [right_bound_factor.pdfs[key] * res * right_bound_factor.na_values[key]\n",
    "                        for key in equivalent_key_group[key_group][\"right\"]]\n",
    "            all_bin_modes = [bin_mode_left[key] for key in equivalent_key_group[key_group][\"left\"]] + \\\n",
    "                            [bin_mode_right[key] for key in equivalent_key_group[key_group][\"right\"]]\n",
    "            if key_group == \"info_type.id\":\n",
    "                print(all_pdfs)\n",
    "                print(\"****************************************************\")\n",
    "                print(all_bin_modes)\n",
    "            new_pdf, new_bin_mode = self.compute_bound_oned(all_pdfs, all_bin_modes, return_factor=True)\n",
    "            res = np.sum(new_pdf)\n",
    "            if res == 0:\n",
    "                res = 10.0\n",
    "                new_pdf[-1] = 1\n",
    "                key_group_pdf[key_group] = new_pdf\n",
    "            else:\n",
    "                key_group_pdf[key_group] = new_pdf / res\n",
    "            key_group_bin_mode[key_group] = new_bin_mode\n",
    "            new_union_key_group[key_group] = [key_group]\n",
    "            new_na_values[key_group] = 1\n",
    "\n",
    "        for group in union_key_group:\n",
    "            table, keys = union_key_group[group]\n",
    "            new_union_key_group[group] = keys\n",
    "            for key in keys:\n",
    "                if table == \"left\":\n",
    "                    key_group_pdf[key] = cond_factor_left.pdfs[key]\n",
    "                    key_group_bin_mode[key] = self.table_buckets[tables_all[left_table]].oned_bin_modes[key]\n",
    "                    new_na_values[key] = cond_factor_left.na_values[key]\n",
    "                else:\n",
    "                    key_group_pdf[key] = right_bound_factor.pdfs[key]\n",
    "                    key_group_bin_mode[key] = right_bound_factor.bin_modes[key]\n",
    "                    new_na_values[key] = right_bound_factor.na_values[key]\n",
    "\n",
    "        new_factor = Group_Factor(sub_plan_query_str, res, key_group_pdf, key_group_bin_mode,\n",
    "                                  union_key_group_set, new_union_key_group, new_na_values)\n",
    "        return new_factor, res\n",
    "\n",
    "    def get_join_keys_with_table_group(self, left_table, right_bound_factor, tables_all, table_equivalent_group,\n",
    "                                       table_key_equivalent_group, table_key_group_map, join_cond):\n",
    "        \"\"\"\n",
    "            Get the join keys between two tables\n",
    "        \"\"\"\n",
    "\n",
    "        actual_join_cond = []\n",
    "        for cond in join_cond[left_table]:\n",
    "            if cond in right_bound_factor.join_cond:\n",
    "                actual_join_cond.append(cond)\n",
    "        equivalent_key_group = dict()\n",
    "        union_key_group_set = table_equivalent_group[left_table].union(right_bound_factor.equivalent_groups)\n",
    "        union_key_group = dict()\n",
    "        new_join_cond = right_bound_factor.join_cond.union(join_cond[left_table])\n",
    "        if len(actual_join_cond) != 0:\n",
    "            for cond in actual_join_cond:\n",
    "                key1 = cond.split(\"=\")[0].strip()\n",
    "                key2 = cond.split(\"=\")[1].strip()\n",
    "                if key1.split(\".\")[0] == left_table:\n",
    "                    key_left = key1.replace(left_table, tables_all[left_table])\n",
    "                    key_group = table_key_group_map[left_table][key_left]\n",
    "                    if key_group not in equivalent_key_group:\n",
    "                        equivalent_key_group[key_group] = dict()\n",
    "                    if left_table in equivalent_key_group[key_group]:\n",
    "                        equivalent_key_group[key_group][\"left\"].append(key_left)\n",
    "                    else:\n",
    "                        equivalent_key_group[key_group][\"left\"] = [key_left]\n",
    "                    right_table = key2.split(\".\")[0]\n",
    "                    key_right = key2.replace(right_table, tables_all[right_table])\n",
    "                    key_group_t = table_key_group_map[right_table][key_right]\n",
    "                    assert key_group_t == key_group, f\"key group mismatch for join {cond}\"\n",
    "                    if \"right\" in equivalent_key_group[key_group]:\n",
    "                        equivalent_key_group[key_group][\"right\"].append(key_right)\n",
    "                    else:\n",
    "                        equivalent_key_group[key_group][\"right\"] = [key_right]\n",
    "                else:\n",
    "                    assert key2.split(\".\")[0] == left_table, f\"unrecognized table alias\"\n",
    "                    key_left = key2.replace(left_table, tables_all[left_table])\n",
    "                    key_group = table_key_group_map[left_table][key_left]\n",
    "                    if key_group not in equivalent_key_group:\n",
    "                        equivalent_key_group[key_group] = dict()\n",
    "                    if left_table in equivalent_key_group[key_group]:\n",
    "                        equivalent_key_group[key_group][\"left\"].append(key_left)\n",
    "                    else:\n",
    "                        equivalent_key_group[key_group][\"left\"] = [key_left]\n",
    "                    right_table = key1.split(\".\")[0]\n",
    "                    key_right = key1.replace(right_table, tables_all[right_table])\n",
    "                    key_group_t = table_key_group_map[right_table][key_right]\n",
    "                    assert key_group_t == key_group, f\"key group mismatch for join {cond}\"\n",
    "                    if \"right\" in equivalent_key_group[key_group]:\n",
    "                        equivalent_key_group[key_group][\"right\"].append(key_right)\n",
    "                    else:\n",
    "                        equivalent_key_group[key_group][\"right\"] = [key_right]\n",
    "\n",
    "            for group in union_key_group_set:\n",
    "                if group in equivalent_key_group:\n",
    "                    continue\n",
    "                elif group in table_key_equivalent_group[left_table]:\n",
    "                    union_key_group[group] = (\"left\", table_key_equivalent_group[left_table][group])\n",
    "                else:\n",
    "                    union_key_group[group] = (\"right\", right_bound_factor.table_key_equivalent_group[group])\n",
    "\n",
    "        else:\n",
    "            common_key_group = table_equivalent_group[left_table].intersection(right_bound_factor.equivalent_groups)\n",
    "            common_key_group = list(common_key_group)[0]\n",
    "            for group in union_key_group_set:\n",
    "                if group == common_key_group:\n",
    "                    equivalent_key_group[group] = dict()\n",
    "                    equivalent_key_group[group][\"left\"] = table_key_equivalent_group[left_table][group]\n",
    "                    equivalent_key_group[group][\"right\"] = right_bound_factor.table_key_equivalent_group[group]\n",
    "                elif group in table_key_equivalent_group[left_table]:\n",
    "                    union_key_group[group] = (\"left\", table_key_equivalent_group[left_table][group])\n",
    "                else:\n",
    "                    union_key_group[group] = (\"right\", right_bound_factor.table_key_equivalent_group[group])\n",
    "\n",
    "        return equivalent_key_group, union_key_group_set, union_key_group, new_join_cond\n",
    "\n",
    "    def get_additional_join_with_table_group(self, left_table, right_union_key_group, table_equivalent_group,\n",
    "                                             table_key_equivalent_group):\n",
    "        common_key_group = table_equivalent_group[left_table].intersection(set(right_union_key_group.keys()))\n",
    "        union_key_group_set = table_equivalent_group[left_table].union(set(right_union_key_group.keys()))\n",
    "        union_key_group = copy.deepcopy(right_union_key_group)\n",
    "        all_join_predicates = []\n",
    "        for group in union_key_group_set:\n",
    "            if group in common_key_group:\n",
    "                left_key = table_key_equivalent_group[left_table][group][0]\n",
    "                left_key = left_table + \".\" + left_key.split(\".\")[-1]\n",
    "                right_key = right_union_key_group[group]\n",
    "                join_predicate = left_key + \" = \" + right_key\n",
    "                all_join_predicates.append(join_predicate)\n",
    "            if group not in union_key_group:\n",
    "                assert group in table_key_equivalent_group[left_table]\n",
    "                left_key = table_key_equivalent_group[left_table][group][0]\n",
    "                left_key = left_table + \".\" + left_key.split(\".\")[-1]\n",
    "                union_key_group[group] = left_key\n",
    "\n",
    "        return all_join_predicates, union_key_group\n",
    "\n",
    "    def join_two_tables(self, sub_plan_query_str, left_table, right_table, tables_all, conditional_factors, join_keys,\n",
    "                        table_equivalent_group, table_key_equivalent_group, table_key_group_map, join_cond):\n",
    "        \"\"\"\n",
    "            Get the cardinality bound by joining the left_table with the right_table\n",
    "            :param left_table:\n",
    "            :param right_table:\n",
    "        \"\"\"\n",
    "        equivalent_key_group, union_key_group_set, union_key_group, new_join_cond = \\\n",
    "            self.get_join_keys_two_tables(left_table, right_table, table_equivalent_group, table_key_equivalent_group,\n",
    "                                          table_key_group_map, join_cond, join_keys, tables_all)\n",
    "        #print(left_table, right_table)\n",
    "        #print(equivalent_key_group)\n",
    "        #print(union_key_group)\n",
    "        #print(conditional_factors.keys())\n",
    "        cond_factor_left = conditional_factors[left_table]\n",
    "        cond_factor_right = conditional_factors[right_table]\n",
    "        bin_mode_left = self.table_buckets[tables_all[left_table]].oned_bin_modes\n",
    "        bin_mode_right = self.table_buckets[tables_all[right_table]].oned_bin_modes\n",
    "        key_group_pdf = dict()\n",
    "        key_group_bin_mode = dict()\n",
    "        new_union_key_group = dict()\n",
    "        res = cond_factor_right.table_len\n",
    "        new_na_values = dict()\n",
    "        #print(equivalent_key_group)\n",
    "        for key_group in equivalent_key_group:\n",
    "            #print(key_group)\n",
    "            #print(\"========================\")\n",
    "            #print(bin_mode_left.keys())\n",
    "            #print(equivalent_key_group[key_group][left_table])\n",
    "            #print(\"==========================================\")\n",
    "            #print(bin_mode_right.keys())\n",
    "            #print(equivalent_key_group[key_group][right_table])\n",
    "            all_pdfs = [cond_factor_left.pdfs[key] * cond_factor_left.table_len * cond_factor_left.na_values[key]\n",
    "                        for key in equivalent_key_group[key_group][left_table]] + \\\n",
    "                       [cond_factor_right.pdfs[key] * res * cond_factor_right.na_values[key]\n",
    "                        for key in equivalent_key_group[key_group][right_table]]\n",
    "            all_bin_modes = [bin_mode_left[key] for key in equivalent_key_group[key_group][left_table]] + \\\n",
    "                            [bin_mode_right[key] for key in equivalent_key_group[key_group][right_table]]\n",
    "            #print(\"====================================================\")\n",
    "            #print(equivalent_key_group[key_group][left_table])\n",
    "            #print(equivalent_key_group[key_group][right_table])\n",
    "            new_pdf, new_bin_mode = self.compute_bound_oned(all_pdfs, all_bin_modes, return_factor=True)\n",
    "            res = np.sum(new_pdf)\n",
    "            key_group_pdf[key_group] = new_pdf / res\n",
    "            key_group_bin_mode[key_group] = new_bin_mode\n",
    "            new_union_key_group[key_group] = [key_group]\n",
    "            new_na_values[key_group] = 1.0\n",
    "\n",
    "        for group in union_key_group:\n",
    "            table, keys = union_key_group[group]\n",
    "            new_union_key_group[group] = keys\n",
    "            for key in keys:\n",
    "                key_group_pdf[key] = conditional_factors[table].pdfs[key]\n",
    "                key_group_bin_mode[key] = self.table_buckets[tables_all[table]].oned_bin_modes[key]\n",
    "                new_na_values[key] = conditional_factors[table].na_values[key]\n",
    "\n",
    "        new_factor = Group_Factor(sub_plan_query_str, res, key_group_pdf, key_group_bin_mode,\n",
    "                                  union_key_group_set, new_union_key_group, new_na_values, new_join_cond)\n",
    "        return new_factor, res\n",
    "\n",
    "    def get_join_keys_two_tables(self, left_table, right_table, table_equivalent_group, table_key_equivalent_group,\n",
    "                                 table_key_group_map, join_cond, join_keys, tables_all):\n",
    "        \"\"\"\n",
    "            Get the join keys between two tables\n",
    "        \"\"\"\n",
    "        actual_join_cond = []\n",
    "        for cond in join_cond[left_table]:\n",
    "            if cond in join_cond[right_table]:\n",
    "                actual_join_cond.append(cond)\n",
    "        equivalent_key_group = dict()\n",
    "        union_key_group_set = table_equivalent_group[left_table].union(table_equivalent_group[right_table])\n",
    "        union_key_group = dict()\n",
    "        new_join_cond = join_cond[left_table].union(join_cond[right_table])\n",
    "        if len(actual_join_cond) != 0:\n",
    "            for cond in actual_join_cond:\n",
    "                key1 = cond.split(\"=\")[0].strip()\n",
    "                key2 = cond.split(\"=\")[1].strip()\n",
    "                if key1.split(\".\")[0] == left_table:\n",
    "                    key_left = key1.replace(left_table, tables_all[left_table])\n",
    "                    key_group = table_key_group_map[left_table][key_left]\n",
    "                    if key_group not in equivalent_key_group:\n",
    "                        equivalent_key_group[key_group] = dict()\n",
    "                    if left_table in equivalent_key_group[key_group]:\n",
    "                        equivalent_key_group[key_group][left_table].append(key_left)\n",
    "                    else:\n",
    "                        equivalent_key_group[key_group][left_table] = [key_left]\n",
    "                    assert key2.split(\".\")[0] == right_table, f\"unrecognized table alias\"\n",
    "                    key_right = key2.replace(right_table, tables_all[right_table])\n",
    "                    key_group_t = table_key_group_map[right_table][key_right]\n",
    "                    assert key_group_t == key_group, f\"key group mismatch for join {cond}\"\n",
    "                    if right_table in equivalent_key_group[key_group]:\n",
    "                        equivalent_key_group[key_group][right_table].append(key_right)\n",
    "                    else:\n",
    "                        equivalent_key_group[key_group][right_table] = [key_right]\n",
    "                else:\n",
    "                    assert key2.split(\".\")[0] == left_table, f\"unrecognized table alias\"\n",
    "                    key_left = key2.replace(left_table, tables_all[left_table])\n",
    "                    key_group = table_key_group_map[left_table][key_left]\n",
    "                    if key_group not in equivalent_key_group:\n",
    "                        equivalent_key_group[key_group] = dict()\n",
    "                    if left_table in equivalent_key_group[key_group]:\n",
    "                        equivalent_key_group[key_group][left_table].append(key_left)\n",
    "                    else:\n",
    "                        equivalent_key_group[key_group][left_table] = [key_left]\n",
    "                    assert key1.split(\".\")[0] == right_table, f\"unrecognized table alias\"\n",
    "                    key_right = key1.replace(right_table, tables_all[right_table])\n",
    "                    key_group_t = table_key_group_map[right_table][key_right]\n",
    "                    assert key_group_t == key_group, f\"key group mismatch for join {cond}\"\n",
    "                    if right_table in equivalent_key_group[key_group]:\n",
    "                        equivalent_key_group[key_group][right_table].append(key_right)\n",
    "                    else:\n",
    "                        equivalent_key_group[key_group][right_table] = [key_right]\n",
    "\n",
    "            for group in union_key_group_set:\n",
    "                if group in equivalent_key_group:\n",
    "                    continue\n",
    "                elif group in table_key_equivalent_group[left_table]:\n",
    "                    union_key_group[group] = (left_table, table_key_equivalent_group[left_table][group])\n",
    "                else:\n",
    "                    union_key_group[group] = (right_table, table_key_equivalent_group[right_table][group])\n",
    "\n",
    "        else:\n",
    "            common_key_group = table_equivalent_group[left_table].intersection(table_equivalent_group[right_table])\n",
    "            common_key_group = list(common_key_group)[0]\n",
    "            for group in union_key_group_set:\n",
    "                if group == common_key_group:\n",
    "                    equivalent_key_group[group] = dict()\n",
    "                    equivalent_key_group[group][left_table] = table_key_equivalent_group[left_table][group]\n",
    "                    equivalent_key_group[group][right_table] = table_key_equivalent_group[right_table][group]\n",
    "                elif group in table_key_equivalent_group[left_table]:\n",
    "                    union_key_group[group] = (left_table, table_key_equivalent_group[left_table][group])\n",
    "                else:\n",
    "                    union_key_group[group] = (right_table, table_key_equivalent_group[right_table][group])\n",
    "\n",
    "        return equivalent_key_group, union_key_group_set, union_key_group, new_join_cond\n",
    "\n",
    "    def get_additional_joins_two_tables(self, left_table, right_table, table_equivalent_group,\n",
    "                                        table_key_equivalent_group):\n",
    "        common_key_group = table_equivalent_group[left_table].intersection(table_equivalent_group[right_table])\n",
    "        union_key_group_set = table_equivalent_group[left_table].union(table_equivalent_group[right_table])\n",
    "        union_key_group = dict()\n",
    "        all_join_predicates = []\n",
    "        for group in union_key_group_set:\n",
    "            if group in common_key_group:\n",
    "                left_key = table_key_equivalent_group[left_table][group][0]\n",
    "                left_key = left_table + \".\" + left_key.split(\".\")[-1]\n",
    "                right_key = table_key_equivalent_group[right_table][group][0]\n",
    "                right_key = right_table + \".\" + right_key.split(\".\")[-1]\n",
    "                join_predicate = left_key + \" = \" + right_key\n",
    "                all_join_predicates.append(join_predicate)\n",
    "            if group in table_key_equivalent_group[left_table]:\n",
    "                left_key = table_key_equivalent_group[left_table][group][0]\n",
    "                left_key = left_table + \".\" + left_key.split(\".\")[-1]\n",
    "                union_key_group[group] = left_key\n",
    "            else:\n",
    "                right_key = table_key_equivalent_group[right_table][group][0]\n",
    "                right_key = right_table + \".\" + right_key.split(\".\")[-1]\n",
    "                union_key_group[group] = right_key\n",
    "\n",
    "        return all_join_predicates, union_key_group\n",
    "\n",
    "    def get_sub_plan_join_key(self, sub_plan_query, join_cond):\n",
    "        # returning a subset of join_keys covered by the tables in sub_plan_query\n",
    "        touched_join_cond = set()\n",
    "        untouched_join_cond = set()\n",
    "        for tab in join_cond:\n",
    "            if tab in sub_plan_query:\n",
    "                touched_join_cond = touched_join_cond.union(join_cond[tab])\n",
    "            else:\n",
    "                untouched_join_cond = untouched_join_cond.union(join_cond[tab])\n",
    "        touched_join_cond -= untouched_join_cond\n",
    "\n",
    "        join_keys = dict()\n",
    "        for cond in touched_join_cond:\n",
    "            key1 = cond.split(\"=\")[0].strip()\n",
    "            table1 = key1.split(\".\")[0].strip()\n",
    "            if table1 not in join_keys:\n",
    "                join_keys[table1] = set([key1])\n",
    "            else:\n",
    "                join_keys[table1].add(key1)\n",
    "\n",
    "            key2 = cond.split(\"=\")[1].strip()\n",
    "            table2 = key2.split(\".\")[0].strip()\n",
    "            if table2 not in join_keys:\n",
    "                join_keys[table2] = set([key2])\n",
    "            else:\n",
    "                join_keys[table2].add(key2)\n",
    "\n",
    "        return join_keys\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d9ebfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"ground_truth_factors_no_filter.pkl\", \"rb\") as f:\n",
    "    ground_truth_factors_no_filter = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5daf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle5 as pickle\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def load_sample_imdb(table_buckets, tables_alias, query_file_orders, join_keys, table_key_equivalent_group,\n",
    "                     SPERCENTAGE=1.0, qdir=\"/home/ubuntu/data_CE/saved_models/binned_cards/{}/job/all_job/\"):\n",
    "    qdir = qdir.format(SPERCENTAGE)\n",
    "    all_sample_factors = []\n",
    "    for fn in query_file_orders:\n",
    "        conditional_factors = load_sample_imdb_one_query(table_buckets, tables_alias, fn, join_keys,\n",
    "                                                         table_key_equivalent_group, SPERCENTAGE, qdir)\n",
    "        all_sample_factors.append(conditional_factors)\n",
    "    return all_sample_factors\n",
    "\n",
    "\n",
    "def load_sample_imdb_one_query(table_buckets, tables_alias, query_file_name, join_keys, table_key_equivalent_group,\n",
    "                               SPERCENTAGE=10.0, qdir=\"/home/ubuntu/data_CE/saved_models/binned_cards/{}/job/all_job/\"):\n",
    "    qdir = qdir.format(SPERCENTAGE)\n",
    "    fpath = os.path.join(qdir, query_file_name)\n",
    "    with open(fpath, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "    conditional_factors = dict()\n",
    "    table_pdfs = dict()\n",
    "    filter_size = dict()\n",
    "    for i, alias in enumerate(data[\"all_aliases\"]):\n",
    "        column = data[\"all_columns\"][i]\n",
    "        alias = alias[0]\n",
    "        key = tables_alias[alias] + \".\" + column\n",
    "        cards = data[\"results\"][i][0]\n",
    "        n_bins = table_buckets[tables_alias[alias]].bin_sizes[key]\n",
    "        pdfs = np.zeros(n_bins)\n",
    "        for (j, val) in cards:\n",
    "            if j is None:\n",
    "                j = 0\n",
    "            pdfs[j] += val\n",
    "        table_len = np.sum(pdfs)\n",
    "        print(alias+\".\"+column, table_len, pdfs)\n",
    "        if table_len == 0:\n",
    "            # no sample satisfy the filter, set it with a small value\n",
    "            #print(\"========================\", alias+\".\"+column)\n",
    "            table_len = 1\n",
    "            pdfs = table_key_equivalent_group[tables_alias[alias]].pdfs[key]\n",
    "        else:\n",
    "            pdfs /= table_len\n",
    "        if alias not in table_pdfs:\n",
    "            table_pdfs[alias] = dict()\n",
    "            filter_size[alias] = table_len\n",
    "        table_pdfs[alias][key] = pdfs\n",
    "\n",
    "    for alias in tables_alias:\n",
    "        if alias in table_pdfs:\n",
    "            table_len = min(table_key_equivalent_group[tables_alias[alias]].table_len,\n",
    "                            filter_size[alias]/(SPERCENTAGE/100))\n",
    "            na_values = table_key_equivalent_group[tables_alias[alias]].na_values\n",
    "            conditional_factors[alias] = Factor(tables_alias[alias], table_len, list(table_pdfs[alias].keys()),\n",
    "                                                table_pdfs[alias], na_values)\n",
    "        else:\n",
    "            #TODO: ground-truth distribution\n",
    "            conditional_factors[alias] = table_key_equivalent_group[tables_alias[alias]]\n",
    "    return conditional_factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b154ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7745d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_file = \"29b.pkl\"\n",
    "idx = q_file_names.index(q_file)\n",
    "BE = Bound_ensemble(table_buckets, schema, ground_truth_factors_no_filter)\n",
    "temp = BE.get_cardinality_bound_all(queries[idx], sub_plan_queries_str_all[idx], \n",
    "                                    q_file_names[idx], True, true_card[q_file.split(\".pkl\")[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a94efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_true_card = np.load(\"/home/ubuntu/CEB/all_true_card_job.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bf438d",
   "metadata": {},
   "outputs": [],
   "source": [
    "BE = Bound_ensemble(table_buckets, schema, ground_truth_factors_no_filter)\n",
    "res = dict()\n",
    "for q_file_id in range(len(queries)):\n",
    "    temp = BE.get_cardinality_bound_all(queries[q_file_id], sub_plan_queries_str_all[q_file_id], q_file_names[q_file_id])\n",
    "    res[q_file_names[q_file_id].split(\".pkl\")[0]] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af322f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "res['5a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6877d8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(true_card['12a'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f547592",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_file_names_real = []\n",
    "sub_query_len = []\n",
    "for query_no in range(1, 34):\n",
    "    for suffix in ['a', 'b', 'c', 'd', 'e', 'f', 'g']:\n",
    "        file = f\"{query_no}{suffix}.sql\"\n",
    "        if file in os.listdir(query_path):\n",
    "            q_file_names_real.append(file.split(\".sql\")[0])\n",
    "            sub_query_len.append(len(res[file.split(\".sql\")[0]]))\n",
    "            \n",
    "start_idx = 0\n",
    "true_card = dict()\n",
    "all_true_card_new = []\n",
    "for i, q_file in enumerate(q_file_names_real):\n",
    "    print(\"=============================================\")\n",
    "    print(f\"query no {i}, {q_file} with {len(res[q_file])} number of sub-plan queries\")\n",
    "    pred = res[q_file]\n",
    "    pred = np.asarray(pred)\n",
    "    end_idx = start_idx + sub_query_len[i]\n",
    "    true = all_true_card[start_idx: end_idx]\n",
    "    temp_true = copy.deepcopy(true)\n",
    "    temp_true[true == -1] = pred[true == -1]\n",
    "    all_true_card_new.append(temp_true)\n",
    "    true_card[q_file] = true\n",
    "    assert len(pred) == len(true)\n",
    "    pred = pred[true != -1]\n",
    "    pred[pred <= 0] = 1\n",
    "    true = true[true != -1]\n",
    "    q_errors = np.maximum(pred/true, true/pred)\n",
    "    for i in [50, 90, 95, 99, 100]:\n",
    "        print(f\"q-error {i}% percentile is {np.percentile(q_errors, i)}\")\n",
    "    start_idx = end_idx\n",
    "all_true_card_new = np.concatenate(all_true_card_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f92b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ok = 0\n",
    "with open(\"CE_scheme_est.txt\", \"w\") as f:\n",
    "    for t in q_file_names_real:\n",
    "        for val in res[t]:\n",
    "            ok += 1\n",
    "            f.write(str(val)+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb09ff83",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"true_est_job.txt\", \"w\") as f:\n",
    "    for i in all_true_card_new:\n",
    "        f.write(str(i)+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523c5232",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_factors_no_filter[\"char_name\"].pdfs['char_name.id'] * ground_truth_factors_no_filter[\"char_name\"].table_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdb39cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056a9aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/Users/ziniuw/Desktop/research/Learned_QO/CC_model/CE_scheme_models/stats/model_200.pkl\"\n",
    "with open(model_path, \"rb\") as f:\n",
    "    new_BE = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de13cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_str = \"SELECT COUNT(*) FROM users as u, comments as c, posts as p WHERE p.OwnerUserId = u.Id AND p.Id = c.PostId AND u.UpVotes>=0 AND u.CreationDate>='2010-08-21 21:27:38'::timestamp AND c.CreationDate>='2010-07-21 11:05:37'::timestamp AND c.CreationDate<='2014-08-25 17:59:25'::timestamp\"\n",
    "t = time.time()\n",
    "res = new_BE.get_cardinality_bound(query57)\n",
    "print(time.time() - t)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ff26f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_file = \"/Users/ziniuw/Desktop/past_research/End-to-End-CardEst-Benchmark/workloads/stats_CEB/sub_plan_queries/stats_CEB_sub_queries.sql\"\n",
    "with open(query_file, \"r\") as f:\n",
    "    queries = f.readlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef76e71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "qerror = []\n",
    "latency = []\n",
    "pred = []\n",
    "for i, query_str in enumerate(queries):\n",
    "    query = query_str.split(\"||\")[0][:-1]\n",
    "    print(\"========================\")\n",
    "    true_card = int(query_str.split(\"||\")[-1])\n",
    "    t = time.time()\n",
    "    res = new_BE.get_cardinality_bound(query)\n",
    "    pred.append(res)\n",
    "    latency.append(time.time() - t)\n",
    "    qerror.append(res/true_card)\n",
    "    print(f\"estimating query {i}: predicted {res}, true_card {true_card}, qerror {res/true_card}, latency {time.time() - t}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e595aff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "qerror = np.asarray(qerror)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e6d0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_qerror = copy.deepcopy(qerror)\n",
    "temp_qerror[temp_qerror < 1] = 1/temp_qerror[temp_qerror < 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73add80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [50, 90, 95, 99, 100]:\n",
    "    print(f\"q-error {i}% percentile is {np.percentile(temp_qerror, i)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f81d047",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"stats_CEB_sub_queries_CE_scheme.txt\", \"w\") as f:\n",
    "    for p in pred:\n",
    "        f.write(str(p)+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301c046f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"stats_CEB_exec.sql\", \"r\") as f:\n",
    "    queries = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e23ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"stats_CEB_exec.sql\", \"w\") as f:\n",
    "    for q in queries:\n",
    "        q = q.split(\"||\")[-1]\n",
    "        f.write(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38019fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
