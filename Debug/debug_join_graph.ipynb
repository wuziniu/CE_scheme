{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6025f1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "sys.path.append(\"/home/ubuntu/CE_scheme/\")\n",
    "from Schemas.imdb.schema import gen_imdb_schema\n",
    "from Join_scheme.data_prepare import read_table_csv\n",
    "from Join_scheme.binning import identify_key_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c1cf74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "\n",
    "class Bucket:\n",
    "    \"\"\"\n",
    "    The class of bucketization of a key attribute\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name, bins=[], bin_modes=[], bin_vars=[], bin_means=[], rest_bins_remaining=None):\n",
    "        self.name = name\n",
    "        self.bins = bins\n",
    "        self.bin_modes = bin_modes\n",
    "        self.bin_vars = bin_vars\n",
    "        self.bin_means = bin_means\n",
    "        self.rest_bins_remaining = rest_bins_remaining\n",
    "        if len(bins) != 0:\n",
    "            assert len(bins) == len(bin_modes)\n",
    "\n",
    "\n",
    "class Table_bucket:\n",
    "    \"\"\"\n",
    "    The class of bucketization for all key attributes in a table.\n",
    "    Supporting more than three dimensional bin modes requires simplifying the causal structure\n",
    "    \"\"\"\n",
    "    def __init__(self, table_name, id_attributes, bin_sizes, oned_bin_modes=None):\n",
    "        self.table_name = table_name\n",
    "        self.id_attributes = id_attributes\n",
    "        self.bin_sizes = bin_sizes\n",
    "        if oned_bin_modes:\n",
    "            self.oned_bin_modes = oned_bin_modes\n",
    "        else:\n",
    "            self.oned_bin_modes = dict()\n",
    "        self.twod_bin_modes = dict()\n",
    "\n",
    "\n",
    "class Bucket_group:\n",
    "    \"\"\"\n",
    "    The class of bucketization for a group of equivalent join keys\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, buckets, start_key, sample_rate, bins=None, primary_keys=[]):\n",
    "        self.buckets = buckets\n",
    "        self.start_key = start_key\n",
    "        self.sample_rate = sample_rate\n",
    "        self.bins = bins\n",
    "        self.primary_keys = primary_keys\n",
    "\n",
    "    def bucketize(self, data):\n",
    "        \"\"\"\n",
    "        Discretize data based on the bucket\n",
    "        \"\"\"\n",
    "        res = dict()\n",
    "        seen_remain_key = np.array([])\n",
    "        cumulative_bin = copy.deepcopy(self.buckets[self.start_key].bins)\n",
    "        start_means = np.asarray(self.buckets[self.start_key].bin_means)\n",
    "\n",
    "        for key in data:\n",
    "            if key in self.primary_keys:\n",
    "                continue\n",
    "            res[key] = copy.deepcopy(data[key])\n",
    "            if key != self.start_key:\n",
    "                unique_remain = np.setdiff1d(self.buckets[key].rest_bins_remaining, seen_remain_key)\n",
    "                assert sum([np.sum(np.isin(unique_remain, b) == 1) for b in cumulative_bin]) == 0\n",
    "\n",
    "                if len(unique_remain) != 0:\n",
    "                    remaining_data = data[key][np.isin(data[key], unique_remain)]\n",
    "                    unique_remain, count_remain = np.unique(remaining_data, return_counts=True)\n",
    "                    unique_counts = np.unique(count_remain)\n",
    "                    for u in unique_counts:\n",
    "                        temp_idx = np.searchsorted(start_means, u)\n",
    "                        if temp_idx == len(cumulative_bin):\n",
    "                            idx = -1\n",
    "                            if u > self.buckets[key].bin_modes[-1]:\n",
    "                                self.buckets[key].bin_modes[-1] = u\n",
    "                        elif temp_idx == 0:\n",
    "                            idx = 0\n",
    "                        else:\n",
    "                            if (u - start_means[temp_idx - 1]) >= (start_means[temp_idx] - u):\n",
    "                                idx = temp_idx - 1\n",
    "                            else:\n",
    "                                idx = temp_idx\n",
    "                        temp_unique = unique_remain[count_remain == u]\n",
    "                        cumulative_bin[idx] = np.concatenate((cumulative_bin[idx], temp_unique))\n",
    "                        seen_remain_key = np.concatenate((seen_remain_key, temp_unique))\n",
    "                        if u > self.buckets[key].bin_modes[idx]:\n",
    "                            self.buckets[key].bin_modes[idx] = u\n",
    "            res[key] = copy.deepcopy(data[key])\n",
    "            count = 0\n",
    "            for i, b in enumerate(cumulative_bin):\n",
    "                count += len(data[key][np.isin(data[key], b)])\n",
    "                res[key][np.isin(data[key], b)] = i\n",
    "\n",
    "        self.bins = cumulative_bin\n",
    "\n",
    "        for key in data:\n",
    "            if key in self.primary_keys:\n",
    "                res[key] = self.bucketize_PK(data[key])\n",
    "                self.buckets[key] = Bucket(key, bin_modes=np.ones(len(self.bins)))\n",
    "        \n",
    "        for key in data:\n",
    "            if key in self.primary_keys:\n",
    "                continue\n",
    "            if self.sample_rate[key] < 1.0:\n",
    "                bin_modes = np.asarray(self.buckets[key].bin_modes)\n",
    "                bin_modes[bin_modes != 1] = bin_modes[bin_modes != 1] / self.sample_rate[key]\n",
    "                self.buckets[key].bin_modes = bin_modes\n",
    "        \n",
    "        return res\n",
    "\n",
    "    def bucketize_PK(self, data):\n",
    "        res = copy.deepcopy(data)\n",
    "        remaining_data = np.unique(data)\n",
    "        for i, b in enumerate(self.bins):\n",
    "            res[np.isin(data, b)] = i\n",
    "            remaining_data = np.setdiff1d(remaining_data, b)\n",
    "        if len(remaining_data) != 0:\n",
    "            self.bins.append(list(remaining_data))\n",
    "            for key in self.buckets:\n",
    "                if key not in self.primary_keys:\n",
    "                    self.buckets[key].bin_modes = np.append(self.buckets[key].bin_modes, 0)\n",
    "        res[np.isin(data, remaining_data)] = len(self.bins)\n",
    "        return res\n",
    "\n",
    "\n",
    "\n",
    "def identify_key_values(schema):\n",
    "    \"\"\"\n",
    "    identify all the key attributes from the schema of a DB, currently we assume all possible joins are known\n",
    "    It is also easy to support unseen joins, which we left as a future work.\n",
    "    :param schema: the schema of a DB\n",
    "    :return: a dict of all keys, {table: [keys]};\n",
    "             a dict of set, each indicating which keys on different tables are considered the same key.\n",
    "    \"\"\"\n",
    "    all_keys = set()\n",
    "    equivalent_keys = dict()\n",
    "    for i, join in enumerate(schema.relationships):\n",
    "        keys = join.identifier.split(\" = \")\n",
    "        all_keys.add(keys[0])\n",
    "        all_keys.add(keys[1])\n",
    "        seen = False\n",
    "        for k in equivalent_keys:\n",
    "            if keys[0] in equivalent_keys[k]:\n",
    "                equivalent_keys[k].add(keys[1])\n",
    "                seen = True\n",
    "                break\n",
    "            elif keys[1] in equivalent_keys[k]:\n",
    "                equivalent_keys[k].add(keys[0])\n",
    "                seen = True\n",
    "                break\n",
    "        if not seen:\n",
    "            # set the keys[-1] as the identifier of this equivalent join key group for convenience.\n",
    "            equivalent_keys[keys[-1]] = set(keys)\n",
    "\n",
    "    assert len(all_keys) == sum([len(equivalent_keys[k]) for k in equivalent_keys])\n",
    "    return all_keys, equivalent_keys\n",
    "\n",
    "\n",
    "def equal_freq_binning(name, data, n_bins, data_len):\n",
    "    uniques, counts = data\n",
    "    if len(uniques) <= n_bins:\n",
    "        bins = []\n",
    "        bin_modes = []\n",
    "        bin_vars = []\n",
    "        bin_means = []\n",
    "        \n",
    "        for i, uni in enumerate(uniques):\n",
    "            bins.append([uni])\n",
    "            bin_modes.append(counts[i])\n",
    "            bin_vars.append(0)\n",
    "            bin_means.append(counts[i])\n",
    "        return Bucket(name, bins, bin_modes, bin_vars, bin_means)\n",
    "    \n",
    "    unique_counts, count_counts = np.unique(counts, return_counts=True)\n",
    "    idx = np.argsort(unique_counts)\n",
    "    unique_counts = unique_counts[idx]\n",
    "    count_counts = count_counts[idx]\n",
    "\n",
    "    bins = []\n",
    "    bin_modes = []\n",
    "    bin_vars = []\n",
    "    bin_means = []\n",
    "\n",
    "    bin_freq = data_len / n_bins\n",
    "    cur_freq = 0\n",
    "    cur_bin = []\n",
    "    cur_bin_count = []\n",
    "    for i, uni_c in enumerate(unique_counts):\n",
    "        cur_freq += count_counts[i] * uni_c\n",
    "        cur_bin.append(uniques[np.where(counts == uni_c)[0]])\n",
    "        cur_bin_count.extend([uni_c] * count_counts[i])\n",
    "        if (cur_freq > bin_freq) or (i == (len(unique_counts) - 1)):\n",
    "            bins.append(np.concatenate(cur_bin))\n",
    "            cur_bin_count = np.asarray(cur_bin_count)\n",
    "            bin_modes.append(uni_c)\n",
    "            bin_means.append(np.mean(cur_bin_count))\n",
    "            bin_vars.append(np.var(cur_bin_count))\n",
    "            cur_freq = 0\n",
    "            cur_bin = []\n",
    "            cur_bin_count = []\n",
    "    assert len(uniques) == sum([len(b) for b in bins]), f\"some unique values missed or duplicated\"\n",
    "    return Bucket(name, bins, bin_modes, bin_vars, bin_means)\n",
    "\n",
    "\n",
    "def compute_variance_score(buckets):\n",
    "    \"\"\"\n",
    "    compute the variance of products of random variables\n",
    "    \"\"\"\n",
    "    all_mean = np.asarray([buckets[k].bin_means for k in buckets])\n",
    "    all_var = np.asarray([buckets[k].bin_vars for k in buckets])\n",
    "    return np.sum(np.prod(all_var + all_mean ** 2, axis=0) - np.prod(all_mean, axis=0) ** 2)\n",
    "\n",
    "\n",
    "def sub_optimal_bucketize(data, sample_rate, n_bins=30, primary_keys=[]):\n",
    "    \"\"\"\n",
    "    Perform sub-optimal bucketization on a group of equivalent join keys.\n",
    "    :param data: a dict of (potentially sampled) table data of the keys\n",
    "                 the keys of this dict are one group of equivalent join keys\n",
    "    :param sample_rate: the sampling rate the data, could be all 1 if no sampling is performed\n",
    "    :param n_bins: how many bins can we allocate\n",
    "    :param primary_keys: the primary keys in the equivalent group since we don't need to bucketize PK.\n",
    "    :return: new data, where the keys are bucketized\n",
    "             the mode of each bucket\n",
    "    \"\"\"\n",
    "    unique_values = dict()\n",
    "    for key in data:\n",
    "        if key not in primary_keys:\n",
    "            unique_values[key] = np.unique(data[key], return_counts=True)\n",
    "\n",
    "    best_variance_score = np.infty\n",
    "    best_bin_len = 0\n",
    "    best_start_key = None\n",
    "    best_buckets = None\n",
    "    for start_key in data:\n",
    "        if start_key in primary_keys:\n",
    "            continue\n",
    "        start_bucket = equal_freq_binning(start_key, unique_values[start_key], n_bins, len(data[start_key]))\n",
    "        rest_buckets = dict()\n",
    "        for key in data:\n",
    "            if key == start_key or key in primary_keys:\n",
    "                continue\n",
    "            uniques = unique_values[key][0]\n",
    "            counts = unique_values[key][1]\n",
    "            rest_buckets[key] = Bucket(key, [], [0] * len(start_bucket.bins), [0] * len(start_bucket.bins),\n",
    "                                       [0] * len(start_bucket.bins), uniques)\n",
    "            for i, bin in enumerate(start_bucket.bins):\n",
    "                idx = np.where(np.isin(uniques, bin) == 1)[0]\n",
    "                if len(idx) != 0:\n",
    "                    bin_count = counts[idx]\n",
    "                    unique_bin_keys = uniques[idx]\n",
    "                    # unique_bin_count = np.unique(bin_count)\n",
    "                    # bin_count = np.concatenate([counts[counts == j] for j in unique_bin_count])\n",
    "                    # unique_bin_keys = np.concatenate([uniques[counts == j] for j in unique_bin_count])\n",
    "                    rest_buckets[key].rest_bins_remaining = np.setdiff1d(rest_buckets[key].rest_bins_remaining,\n",
    "                                                                         unique_bin_keys)\n",
    "                    rest_buckets[key].bin_modes[i] = np.max(bin_count)\n",
    "                    rest_buckets[key].bin_vars[i] = np.var(bin_count)\n",
    "                    rest_buckets[key].bin_means[i] = np.mean(bin_count)\n",
    "\n",
    "        rest_buckets[start_key] = start_bucket\n",
    "        var_score = compute_variance_score(rest_buckets)\n",
    "        if len(start_bucket.bins) > best_bin_len:\n",
    "            best_variance_score = var_score\n",
    "            best_start_key = start_key\n",
    "            best_buckets = rest_buckets\n",
    "            best_bin_len = len(start_bucket.bins)\n",
    "        elif len(start_bucket.bins) >= best_bin_len * 0.9 and var_score < best_variance_score:\n",
    "            best_variance_score = var_score\n",
    "            best_start_key = start_key\n",
    "            best_buckets = rest_buckets\n",
    "            best_bin_len = len(start_bucket.bins)\n",
    "    \n",
    "    best_buckets = Bucket_group(best_buckets, best_start_key, sample_rate, primary_keys=primary_keys)\n",
    "    new_data = best_buckets.bucketize(data)\n",
    "    return new_data, best_buckets\n",
    "\n",
    "\n",
    "def fixed_start_key_bucketize(start_key, data, sample_rate, n_bins=30, primary_keys=[]):\n",
    "    \"\"\"\n",
    "    Perform sub-optimal bucketization on a group of equivalent join keys based on the pre-defined start_key.\n",
    "    :param data: a dict of (potentially sampled) table data of the keys\n",
    "                 the keys of this dict are one group of equivalent join keys\n",
    "    :param sample_rate: the sampling rate the data, could be all 1 if no sampling is performed\n",
    "    :param n_bins: how many bins can we allocate\n",
    "    :param primary_keys: the primary keys in the equivalent group since we don't need to bucketize PK.\n",
    "    :return: new data, where the keys are bucketized\n",
    "             the mode of each bucket\n",
    "    \"\"\"\n",
    "    unique_values = dict()\n",
    "    for key in data:\n",
    "        if key not in primary_keys:\n",
    "            unique_values[key] = np.unique(data[key], return_counts=True)\n",
    "\n",
    "    start_bucket = equal_freq_binning(start_key, unique_values[start_key], n_bins, len(data[start_key]))\n",
    "    rest_buckets = dict()\n",
    "    for key in data:\n",
    "        if key == start_key or key in primary_keys:\n",
    "            continue\n",
    "        uniques = unique_values[key][0]\n",
    "        counts = unique_values[key][1]\n",
    "        rest_buckets[key] = Bucket(key, [], [0] * len(start_bucket.bins), [0] * len(start_bucket.bins),\n",
    "                                   [0] * len(start_bucket.bins), uniques)\n",
    "        for i, bin in enumerate(start_bucket.bins):\n",
    "            idx = np.where(np.isin(uniques, bin) == 1)[0]\n",
    "            if len(idx) != 0:\n",
    "                bin_count = counts[idx]\n",
    "                unique_bin_keys = uniques[idx]\n",
    "                rest_buckets[key].rest_bins_remaining = np.setdiff1d(rest_buckets[key].rest_bins_remaining,\n",
    "                                                                     unique_bin_keys)\n",
    "                rest_buckets[key].bin_modes[i] = np.max(bin_count)\n",
    "                rest_buckets[key].bin_means[i] = np.mean(bin_count)\n",
    "\n",
    "    best_buckets = Bucket_group(rest_buckets, start_key, sample_rate, primary_keys=primary_keys)\n",
    "    new_data = best_buckets.bucketize(data)\n",
    "    return new_data, best_buckets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d81e2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/home/ubuntu/data_CE/{}.csv\"\n",
    "schema = gen_imdb_schema(data_path)\n",
    "all_keys, equivalent_keys = identify_key_values(schema)\n",
    "with open(\"new_table_buckets.pkl\", \"rb\") as f:\n",
    "    table_buckets = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fabd33ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/ubuntu/data_CE/job/job_sub_plan_queries.txt\", \"r\") as f:\n",
    "    sub_plan_queries = f.read()\n",
    "psql_raw = sub_plan_queries.split(\"query: 0\")[1:]\n",
    "queries = []\n",
    "q_file_names = []\n",
    "query_path = \"/home/ubuntu/data_CE/job/\"\n",
    "for file in os.listdir(query_path):\n",
    "    if file.endswith(\".sql\") and file[0].isnumeric():\n",
    "        q_file_names.append(file.split(\".sql\")[0]+\".pkl\")\n",
    "        with open(query_path+file, \"r\") as f:\n",
    "            q = f.readline()\n",
    "            queries.append(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c46b7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "psql_raw = sub_plan_queries.split(\"query: 0\")[1:]\n",
    "sub_plan_queries_str_all = []\n",
    "for per_query in psql_raw:\n",
    "    sub_plan_queries = []\n",
    "    sub_plan_queries_str = []\n",
    "    num_sub_plan_queries = len(per_query.split(\"query: \"))\n",
    "    all_info = per_query.split(\"RELOPTINFO (\")[1:]\n",
    "    assert num_sub_plan_queries*2 == len(all_info)\n",
    "    for i in range(num_sub_plan_queries):\n",
    "        idx = i*2\n",
    "        table1 = all_info[idx].split(\"): rows=\")[0]\n",
    "        table2 = all_info[idx+1].split(\"): rows=\")[0]\n",
    "        table_str = (table1, table2)\n",
    "        sub_plan_queries_str.append(table_str)\n",
    "    sub_plan_queries_str_all.append(sub_plan_queries_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0a870ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "from Join_scheme.join_graph import get_join_hyper_graph, parse_query_all_join\n",
    "from Join_scheme.data_prepare import identify_key_values\n",
    "#from Sampling.load_sample import load_sample_imdb_one_query\n",
    "\n",
    "\n",
    "class Factor:\n",
    "    \"\"\"\n",
    "    This the class defines a multidimensional conditional probability on one table.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, table, table_len, variables, pdfs, na_values=None):\n",
    "        self.table = table\n",
    "        self.table_len = table_len\n",
    "        self.variables = variables\n",
    "        self.pdfs = pdfs\n",
    "        self.na_values = na_values  # this is the percentage of data, which is not nan.\n",
    "\n",
    "\n",
    "class Group_Factor:\n",
    "    \"\"\"\n",
    "        This the class defines a multidimensional conditional probability on a group of tables.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tables, tables_size, variables, pdfs, bin_modes, equivalent_groups=None, \n",
    "                 table_key_equivalent_group=None, na_values=None, join_cond=None):\n",
    "        self.table = tables\n",
    "        self.tables_size = tables_size\n",
    "        self.variables = variables\n",
    "        self.pdfs = pdfs\n",
    "        self.bin_modes = bin_modes\n",
    "        self.equivalent_groups = equivalent_groups\n",
    "        self.table_key_equivalent_group = table_key_equivalent_group\n",
    "        self.na_values = na_values\n",
    "        self.join_cond = join_cond\n",
    "\n",
    "\n",
    "class Bound_ensemble:\n",
    "    \"\"\"\n",
    "    This the class where we store all the trained models and perform inference on the bound.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, table_buckets, schema, ground_truth_factors_no_filter=None):\n",
    "        self.table_buckets = table_buckets\n",
    "        self.schema = schema\n",
    "        self.all_keys, self.equivalent_keys = identify_key_values(schema)\n",
    "        self.all_join_conds = None\n",
    "        self.ground_truth_factors_no_filter = ground_truth_factors_no_filter\n",
    "        # self.reverse_table_alias = None\n",
    "\n",
    "    def parse_query_simple(self, query):\n",
    "        \"\"\"\n",
    "        If your selection query contains no aggregation and nested sub-queries, you can use this function to parse a\n",
    "        join query. Otherwise, use parse_query function.\n",
    "        \"\"\"\n",
    "        tables_all, join_cond, join_keys = parse_query_all_join(query)\n",
    "        # TODO: implement functions on parsing filter conditions.\n",
    "        table_filters = dict()\n",
    "        return tables_all, table_filters, join_cond, join_keys\n",
    "\n",
    "    def get_all_id_conidtional_distribution(self, query_file_name, tables_alias, join_keys):\n",
    "        # TODO: make it work on query-driven and sampling based\n",
    "        return load_sample_imdb_one_query(self.table_buckets, tables_alias, query_file_name, join_keys,\n",
    "                                          self.ground_truth_factors_no_filter)\n",
    "\n",
    "    def eliminate_one_key_group(self, tables, key_group, factors, relevant_keys):\n",
    "        \"\"\"This version only supports 2D distributions (i.e. the distribution learned with tree-structured PGM)\"\"\"\n",
    "        rest_group = None\n",
    "        rest_group_cardinalty = 0\n",
    "        eliminated_tables = []\n",
    "        rest_group_tables = []\n",
    "        for table in tables:\n",
    "            assert key_group in factors[table].equivalent_variables\n",
    "            temp = copy.deepcopy(factors[table].equivalent_variables)\n",
    "            temp.remove(key_group)\n",
    "            if len(temp) == 0:\n",
    "                eliminated_tables.append(table)\n",
    "            for key in temp:\n",
    "                if rest_group:\n",
    "                    assert factors[table].cardinalities[key] == rest_group_cardinalty\n",
    "                    rest_group_tables.append(table)\n",
    "                else:\n",
    "                    rest_group = key\n",
    "                    rest_group_cardinalty = factors[table].cardinalities[key]\n",
    "                    rest_group_tables = [table]\n",
    "\n",
    "        all_probs_eliminated = []\n",
    "        all_modes_eliminated = []\n",
    "        for table in eliminated_tables:\n",
    "            bin_modes = self.table_buckets[table].oned_bin_modes[relevant_keys[key_group][table]]\n",
    "            all_probs_eliminated.append(factors[table].pdfs)\n",
    "            all_modes_eliminated.append(np.minimum(bin_modes, factors[table].pdfs))\n",
    "\n",
    "        if rest_group:\n",
    "            new_factor_pdf = np.zeros(rest_group_cardinalty)\n",
    "        else:\n",
    "            return self.compute_bound_oned(all_probs_eliminated, all_modes_eliminated)\n",
    "\n",
    "        for i in range(rest_group_cardinalty):\n",
    "            for table in rest_group_tables:\n",
    "                idx_f = factors[table].equivalent_variables.index(key_group)\n",
    "                idx_b = self.table_buckets[table].id_attributes.index(relevant_keys[key_group][table])\n",
    "                bin_modes = self.table_buckets[table].twod_bin_modes[relevant_keys[key_group][table]]\n",
    "                if idx_f == 0 and idx_b == 0:\n",
    "                    all_probs_eliminated.append(factors[table].pdfs[:, i])\n",
    "                    all_modes_eliminated.append(np.minimum(bin_modes[:, i], factors[table].pdfs[:, i]))\n",
    "                elif idx_f == 0 and idx_b == 1:\n",
    "                    all_probs_eliminated.append(factors[table].pdfs[:, i])\n",
    "                    all_modes_eliminated.append(np.minimum(bin_modes[i, :], factors[table].pdfs[:, i]))\n",
    "                elif idx_f == 1 and idx_b == 0:\n",
    "                    all_probs_eliminated.append(factors[table].pdfs[i, :])\n",
    "                    all_modes_eliminated.append(np.minimum(bin_modes[:, i], factors[table].pdfs[i, :]))\n",
    "                else:\n",
    "                    all_probs_eliminated.append(factors[table].pdfs[i, :])\n",
    "                    all_modes_eliminated.append(np.minimum(bin_modes[i, :], factors[table].pdfs[i, :]))\n",
    "            new_factor_pdf[i] = self.compute_bound_oned(all_probs_eliminated, all_modes_eliminated)\n",
    "\n",
    "        for table in rest_group_tables:\n",
    "            factors[table] = Factor([rest_group], new_factor_pdf, [rest_group])\n",
    "\n",
    "        return None\n",
    "\n",
    "    def compute_bound_oned(self, all_probs, all_modes, return_factor=False):\n",
    "        temp_all_modes = []\n",
    "        for i in range(len(all_modes)):\n",
    "            temp_all_modes.append(np.minimum(all_probs[i], all_modes[i]))\n",
    "        all_probs = np.stack(all_probs, axis=0)\n",
    "        temp_all_modes = np.stack(temp_all_modes, axis=0)\n",
    "        multiplier = np.prod(temp_all_modes, axis=0)\n",
    "        non_zero_idx = np.where(multiplier != 0)[0]\n",
    "        min_number = np.amin(all_probs[:, non_zero_idx] / temp_all_modes[:, non_zero_idx], axis=0)\n",
    "        # print(min_number, multiplier[non_zero_idx])\n",
    "        if return_factor:\n",
    "            new_probs = np.zeros(multiplier.shape)\n",
    "            new_probs[non_zero_idx] = multiplier[non_zero_idx] * min_number\n",
    "            return new_probs, multiplier\n",
    "        else:\n",
    "            multiplier[non_zero_idx] = multiplier[non_zero_idx] * min_number\n",
    "            return np.sum(multiplier)\n",
    "\n",
    "    def get_optimal_elimination_order(self, equivalent_group, join_keys, factors):\n",
    "        \"\"\"\n",
    "        This function determines the optimial elimination order for each key group\n",
    "        \"\"\"\n",
    "        cardinalities = dict()\n",
    "        lengths = dict()\n",
    "        tables_involved = dict()\n",
    "        relevant_keys = dict()\n",
    "        for group in equivalent_group:\n",
    "            relevant_keys[group] = dict()\n",
    "            lengths[group] = len(equivalent_group[group])\n",
    "            cardinalities[group] = []\n",
    "            tables_involved[group] = set([])\n",
    "            for keys in equivalent_group[group]:\n",
    "                for table in join_keys:\n",
    "                    if keys in join_keys[table]:\n",
    "                        cardinalities[group].append(len(join_keys[table]))\n",
    "                        tables_involved[group].add(table)\n",
    "                        variables = factors[table].variables\n",
    "                        variables[variables.index(keys)] = group\n",
    "                        factors[table].variables = variables\n",
    "                        relevant_keys[group][table] = keys\n",
    "                        break\n",
    "            cardinalities[group] = np.asarray(cardinalities[group])\n",
    "\n",
    "        optimal_order = list(equivalent_group.keys())\n",
    "        for i in range(len(optimal_order)):\n",
    "            min_idx = i\n",
    "            for j in range(i + 1, len(optimal_order)):\n",
    "                min_group = optimal_order[min_idx]\n",
    "                curr_group = optimal_order[j]\n",
    "                if np.max(cardinalities[curr_group]) < np.max(cardinalities[min_group]):\n",
    "                    min_idx = j\n",
    "                else:\n",
    "                    min_max_tables = np.max(cardinalities[min_group])\n",
    "                    min_num_max_tables = len(np.where(cardinalities[min_group] == min_max_tables)[0])\n",
    "                    curr_max_tables = np.max(cardinalities[curr_group])\n",
    "                    curr_num_max_tables = len(np.where(cardinalities[curr_group] == curr_max_tables)[0])\n",
    "                    if curr_num_max_tables < min_num_max_tables:\n",
    "                        min_idx = j\n",
    "                    elif lengths[curr_group] < lengths[min_group]:\n",
    "                        min_idx = j\n",
    "            optimal_order[i], optimal_order[min_idx] = optimal_order[min_idx], optimal_order[i]\n",
    "        return optimal_order, tables_involved, relevant_keys\n",
    "\n",
    "    def get_cardinality_bound_one(self, query_str):\n",
    "        tables_all, table_queries, join_cond, join_keys = self.parse_query_simple(query_str)\n",
    "        equivalent_group = get_join_hyper_graph(join_keys, self.equivalent_keys)\n",
    "        conditional_factors = self.get_all_id_conidtional_distribution(table_queries, join_keys, equivalent_group)\n",
    "        optimal_order, tables_involved, relevant_keys = self.get_optimal_elimination_order(equivalent_group, join_keys,\n",
    "                                                                                           conditional_factors)\n",
    "\n",
    "        for key_group in optimal_order:\n",
    "            tables = tables_involved[key_group]\n",
    "            res = self.eliminate_one_key_group(tables, key_group, conditional_factors, relevant_keys)\n",
    "        return res\n",
    "\n",
    "    def get_sub_plan_queries_sql(self, query_str, sub_plan_query_str_all, query_name=None):\n",
    "        tables_all, table_queries, join_cond, join_keys = self.parse_query_simple(query_str)\n",
    "        equivalent_group, table_equivalent_group, table_key_equivalent_group = get_join_hyper_graph(join_keys,\n",
    "                                                                                                    self.equivalent_keys)\n",
    "        cached_sub_queries_sql = dict()\n",
    "        cached_union_key_group = dict()\n",
    "        res_sql = []\n",
    "        for (left_tables, right_tables) in sub_plan_query_str_all:\n",
    "            assert \" \" not in left_tables, f\"{left_tables} contains more than one tables, violating left deep plan\"\n",
    "            sub_plan_query_list = right_tables.split(\" \") + [left_tables]\n",
    "            sub_plan_query_list.sort()\n",
    "            sub_plan_query_str = \" \".join(sub_plan_query_list)  # get the string name of the sub plan query\n",
    "            sql_header = \"SELECT COUNT(*) FROM \"\n",
    "            for alias in sub_plan_query_list:\n",
    "                sql_header += (tables_all[alias] + \" AS \" + alias + \", \")\n",
    "            sql_header = sql_header[:-2] + \" WHERE \"\n",
    "            if \" \" in right_tables:\n",
    "                assert right_tables in cached_sub_queries_sql, f\"{right_tables} not in cache, input is not ordered\"\n",
    "                right_sql = cached_sub_queries_sql[right_tables]\n",
    "                right_union_key_group = cached_union_key_group[right_tables]\n",
    "                if left_tables in table_queries:\n",
    "                    left_sql = table_queries[left_tables]\n",
    "                    curr_sql = right_sql + \" AND (\" + left_sql + \")\"\n",
    "                else:\n",
    "                    curr_sql = right_sql\n",
    "                additional_joins, union_key_group = self.get_additional_join_with_table_group(left_tables,\n",
    "                                                                                              right_union_key_group,\n",
    "                                                                                              table_equivalent_group,\n",
    "                                                                                              table_key_equivalent_group)\n",
    "                for join in additional_joins:\n",
    "                    curr_sql = curr_sql + \" AND \" + join\n",
    "            else:\n",
    "                curr_sql = \"\"\n",
    "                if left_tables in table_queries:\n",
    "                    curr_sql += (\"(\" + table_queries[left_tables] + \")\")\n",
    "                if right_tables in table_queries:\n",
    "                    if curr_sql != \"\":\n",
    "                        curr_sql += \" AND \"\n",
    "                    curr_sql += (\"(\" + table_queries[right_tables] + \")\")\n",
    "\n",
    "                additional_joins, union_key_group = self.get_additional_joins_two_tables(left_tables, right_tables,\n",
    "                                                                                         table_equivalent_group,\n",
    "                                                                                         table_key_equivalent_group)\n",
    "                for join in additional_joins:\n",
    "                    if curr_sql == \"\":\n",
    "                        curr_sql += join\n",
    "                    else:\n",
    "                        curr_sql = curr_sql + \" AND \" + join\n",
    "            cached_sub_queries_sql[sub_plan_query_str] = curr_sql\n",
    "            cached_union_key_group[sub_plan_query_str] = union_key_group\n",
    "            res_sql.append(sql_header + curr_sql + \";\")\n",
    "        return res_sql\n",
    "\n",
    "    def get_cardinality_bound_all(self, query_str, sub_plan_query_str_all, query_name=None, debug=False,\n",
    "                                  true_card=None):\n",
    "        \"\"\"\n",
    "        Get the cardinality bounds for all sub_plan_queires of a query.\n",
    "        Note: Due to efficiency, this current version only support left_deep plans (like the one generated by postgres),\n",
    "              but it can easily support right deep or bushy plans.\n",
    "        :param query_str: the target query\n",
    "        :param sub_plan_query_str_all: all sub_plan_queries of the target query,\n",
    "               it should be sorted by number of the tables in the sub_plan_query\n",
    "        \"\"\"\n",
    "        tables_all, table_queries, join_cond, join_keys = self.parse_query_simple(query_str)\n",
    "        #print(join_cond)\n",
    "        # print(join_keys)\n",
    "        equivalent_group, table_equivalent_group, table_key_equivalent_group, table_key_group_map = \\\n",
    "            get_join_hyper_graph(join_keys, self.equivalent_keys)\n",
    "        conditional_factors = self.get_all_id_conidtional_distribution(query_name, tables_all, join_keys)\n",
    "        # self.reverse_table_alias = {v: k for k, v in tables_all.items()}\n",
    "        cached_sub_queries = dict()\n",
    "        cardinality_bounds = []\n",
    "        for i, (left_tables, right_tables) in enumerate(sub_plan_query_str_all):\n",
    "            assert \" \" not in left_tables, f\"{left_tables} contains more than one tables, violating left deep plan\"\n",
    "            sub_plan_query_list = right_tables.split(\" \") + [left_tables]\n",
    "            sub_plan_query_list.sort()\n",
    "            sub_plan_query_str = \" \".join(sub_plan_query_list)  # get the string name of the sub plan query\n",
    "            # print(sub_plan_query_str)\n",
    "            #print(sub_plan_query_str, \"=========================================\")\n",
    "            if \" \" in right_tables:\n",
    "                assert right_tables in cached_sub_queries, f\"{right_tables} not in cache, input is not ordered\"\n",
    "                right_bound_factor = cached_sub_queries[right_tables]\n",
    "                curr_bound_factor, res = self.join_with_one_table(sub_plan_query_str,\n",
    "                                                                  left_tables,\n",
    "                                                                  tables_all,\n",
    "                                                                  right_bound_factor,\n",
    "                                                                  conditional_factors[left_tables],\n",
    "                                                                  table_equivalent_group,\n",
    "                                                                  table_key_equivalent_group,\n",
    "                                                                  table_key_group_map,\n",
    "                                                                  join_cond)\n",
    "            else:\n",
    "                curr_bound_factor, res = self.join_two_tables(sub_plan_query_str,\n",
    "                                                              left_tables,\n",
    "                                                              right_tables,\n",
    "                                                              tables_all,\n",
    "                                                              conditional_factors,\n",
    "                                                              join_keys,\n",
    "                                                              table_equivalent_group,\n",
    "                                                              table_key_equivalent_group,\n",
    "                                                              table_key_group_map,\n",
    "                                                              join_cond)\n",
    "            cached_sub_queries[sub_plan_query_str] = curr_bound_factor\n",
    "            res = max(res, 1)\n",
    "            if debug:\n",
    "                if true_card[i] == -1:\n",
    "                    error = \"NA\"\n",
    "                else:\n",
    "                    error = max(res / true_card[i], true_card[i] / res)\n",
    "                #print(f\"{left_tables}, {right_tables}|| estimate: {res}, true: {true_card[i]}, error: {error}\")\n",
    "            cardinality_bounds.append(res)\n",
    "        return cardinality_bounds\n",
    "\n",
    "    def join_with_one_table(self, sub_plan_query_str, left_table, tables_all, right_bound_factor, cond_factor_left,\n",
    "                            table_equivalent_group, table_key_equivalent_group, table_key_group_map, join_cond):\n",
    "        \"\"\"\n",
    "        Get the cardinality bound by joining the left_table with the seen right_tables\n",
    "        :param left_table:\n",
    "        :param right_tables:\n",
    "        \"\"\"\n",
    "        equivalent_key_group, union_key_group_set, union_key_group, new_join_cond = \\\n",
    "            self.get_join_keys_with_table_group(left_table, right_bound_factor, tables_all, table_equivalent_group,\n",
    "                                                table_key_equivalent_group, table_key_group_map, join_cond)\n",
    "        bin_mode_left = self.table_buckets[tables_all[left_table]].oned_bin_modes\n",
    "        bin_mode_right = right_bound_factor.bin_modes\n",
    "        key_group_pdf = dict()\n",
    "        key_group_bin_mode = dict()\n",
    "        new_union_key_group = dict()\n",
    "        new_na_values = dict()\n",
    "        right_variables = right_bound_factor.variables\n",
    "        new_variables = copy.deepcopy(right_variables)\n",
    "        res = right_bound_factor.tables_size\n",
    "        #print(\"\\n\")\n",
    "        #print(union_key_group_set)\n",
    "        #print(union_key_group)\n",
    "        for key_group in equivalent_key_group:\n",
    "            #print(key_group, equivalent_key_group[key_group], res)\n",
    "            # print(cond_factor_left.na_values)\n",
    "            # print(right_bound_factor.na_values)\n",
    "            #print(cond_factor_left.pdfs.keys())\n",
    "            #print(right_bound_factor.pdfs.keys())\n",
    "            all_pdfs = [cond_factor_left.pdfs[key] * cond_factor_left.table_len * cond_factor_left.na_values[key]\n",
    "                        for key in equivalent_key_group[key_group][\"left\"]]\n",
    "            all_bin_modes = [bin_mode_left[key] for key in equivalent_key_group[key_group][\"left\"]]\n",
    "            for key in equivalent_key_group[key_group][\"left\"]:\n",
    "                new_variables[key] = key_group\n",
    "            for key in equivalent_key_group[key_group][\"right\"]:\n",
    "                if key in right_bound_factor.pdfs:\n",
    "                    new_variables[key] = key_group\n",
    "                    all_pdfs.append(right_bound_factor.pdfs[key] * res * right_bound_factor.na_values[key])\n",
    "                    all_bin_modes.append(bin_mode_right[key])\n",
    "                else:\n",
    "                    key = right_variables[key]\n",
    "                    all_pdfs.append(right_bound_factor.pdfs[key] * res * right_bound_factor.na_values[key])\n",
    "                    all_bin_modes.append(bin_mode_right[key])\n",
    "\n",
    "            new_pdf, new_bin_mode = self.compute_bound_oned(all_pdfs, all_bin_modes, return_factor=True)\n",
    "            res = np.sum(new_pdf)\n",
    "            if res == 0:\n",
    "                res = 10.0\n",
    "                new_pdf[-1] = 1\n",
    "                key_group_pdf[key_group] = new_pdf\n",
    "            else:\n",
    "                key_group_pdf[key_group] = new_pdf / res\n",
    "            key_group_bin_mode[key_group] = new_bin_mode\n",
    "            new_union_key_group[key_group] = [key_group]\n",
    "            new_na_values[key_group] = 1\n",
    "        \n",
    "        for group in union_key_group:\n",
    "            if group not in new_union_key_group:\n",
    "                new_union_key_group[group] = []\n",
    "            for table, keys in union_key_group[group]:\n",
    "                for key in keys:\n",
    "                    new_union_key_group[group].append(key)\n",
    "                    if table == \"left\":\n",
    "                        key_group_pdf[key] = cond_factor_left.pdfs[key]\n",
    "                        key_group_bin_mode[key] = self.table_buckets[tables_all[left_table]].oned_bin_modes[key]\n",
    "                        new_na_values[key] = cond_factor_left.na_values[key]\n",
    "                    else:\n",
    "                        key_group_pdf[key] = right_bound_factor.pdfs[key]\n",
    "                        key_group_bin_mode[key] = right_bound_factor.bin_modes[key]\n",
    "                        new_na_values[key] = right_bound_factor.na_values[key]\n",
    "        \n",
    "        #print(\"****\", key_group_pdf.keys())\n",
    "        new_factor = Group_Factor(sub_plan_query_str, res, new_variables, key_group_pdf, key_group_bin_mode,\n",
    "                                  union_key_group_set, new_union_key_group, new_na_values, new_join_cond)\n",
    "        return new_factor, res\n",
    "\n",
    "    def get_join_keys_with_table_group(self, left_table, right_bound_factor, tables_all, table_equivalent_group,\n",
    "                                       table_key_equivalent_group, table_key_group_map, join_cond):\n",
    "        \"\"\"\n",
    "            Get the join keys between two tables\n",
    "        \"\"\"\n",
    "\n",
    "        actual_join_cond = []\n",
    "        for cond in join_cond[left_table]:\n",
    "            if cond in right_bound_factor.join_cond:\n",
    "                actual_join_cond.append(cond)\n",
    "        #print(join_cond[left_table], right_bound_factor.join_cond)\n",
    "        #print(actual_join_cond)\n",
    "        equivalent_key_group = dict()\n",
    "        union_key_group_set = table_equivalent_group[left_table].union(right_bound_factor.equivalent_groups)\n",
    "        union_key_group = dict()\n",
    "        new_join_cond = right_bound_factor.join_cond.union(join_cond[left_table])\n",
    "        if len(actual_join_cond) != 0:\n",
    "            for cond in actual_join_cond:\n",
    "                key1 = cond.split(\"=\")[0].strip()\n",
    "                key2 = cond.split(\"=\")[1].strip()\n",
    "                if key1.split(\".\")[0] == left_table:\n",
    "                    key_left = tables_all[left_table] + \".\" + key1.split(\".\")[-1]\n",
    "                    key_group = table_key_group_map[left_table][key_left]\n",
    "                    if key_group not in equivalent_key_group:\n",
    "                        equivalent_key_group[key_group] = dict()\n",
    "                    if left_table in equivalent_key_group[key_group]:\n",
    "                        equivalent_key_group[key_group][\"left\"].append(key_left)\n",
    "                    else:\n",
    "                        equivalent_key_group[key_group][\"left\"] = [key_left]\n",
    "                    right_table = key2.split(\".\")[0]\n",
    "                    key_right = tables_all[right_table] + \".\" + key2.split(\".\")[-1]\n",
    "                    key_group_t = table_key_group_map[right_table][key_right]\n",
    "                    assert key_group_t == key_group, f\"key group mismatch for join {cond}\"\n",
    "                    if \"right\" in equivalent_key_group[key_group]:\n",
    "                        equivalent_key_group[key_group][\"right\"].append(key_right)\n",
    "                    else:\n",
    "                        equivalent_key_group[key_group][\"right\"] = [key_right]\n",
    "                else:\n",
    "                    assert key2.split(\".\")[0] == left_table, f\"unrecognized table alias\"\n",
    "                    key_left = tables_all[left_table] + \".\" + key2.split(\".\")[-1]\n",
    "                    key_group = table_key_group_map[left_table][key_left]\n",
    "                    if key_group not in equivalent_key_group:\n",
    "                        equivalent_key_group[key_group] = dict()\n",
    "                    if left_table in equivalent_key_group[key_group]:\n",
    "                        equivalent_key_group[key_group][\"left\"].append(key_left)\n",
    "                    else:\n",
    "                        equivalent_key_group[key_group][\"left\"] = [key_left]\n",
    "                    right_table = key1.split(\".\")[0]\n",
    "                    key_right = tables_all[right_table] + \".\" + key1.split(\".\")[-1]\n",
    "                    key_group_t = table_key_group_map[right_table][key_right]\n",
    "                    assert key_group_t == key_group, f\"key group mismatch for join {cond}\"\n",
    "                    if \"right\" in equivalent_key_group[key_group]:\n",
    "                        equivalent_key_group[key_group][\"right\"].append(key_right)\n",
    "                    else:\n",
    "                        equivalent_key_group[key_group][\"right\"] = [key_right]\n",
    "            \n",
    "            for group in union_key_group_set:\n",
    "                if group in equivalent_key_group:\n",
    "                    new_left_key = []\n",
    "                    for key in table_key_equivalent_group[left_table][group]:\n",
    "                        if key not in equivalent_key_group[group][\"left\"]:\n",
    "                            new_left_key.append(key)\n",
    "                    if len(new_left_key) != 0:\n",
    "                        union_key_group[group] = [(\"left\", new_left_key)]\n",
    "                    new_right_key = []\n",
    "                    for key in right_bound_factor.table_key_equivalent_group[group]:\n",
    "                        if key not in equivalent_key_group[group][\"right\"]:\n",
    "                            new_right_key.append(key)\n",
    "                    if len(new_right_key) != 0:\n",
    "                        if group in union_key_group:\n",
    "                            union_key_group[group].append((\"right\", new_right_key))\n",
    "                        else:\n",
    "                            union_key_group[group] = [(\"right\", new_right_key)]\n",
    "                else:\n",
    "                    if group in table_key_equivalent_group[left_table]:\n",
    "                        if group in union_key_group:\n",
    "                            union_key_group[group].append((\"left\", table_key_equivalent_group[left_table][group]))\n",
    "                        else:\n",
    "                            union_key_group[group] = [(\"left\", table_key_equivalent_group[left_table][group])]\n",
    "                    if group in right_bound_factor.table_key_equivalent_group:\n",
    "                        if group in union_key_group:\n",
    "                            union_key_group[group].append((\"right\", right_bound_factor.table_key_equivalent_group[group]))\n",
    "                        else:\n",
    "                            union_key_group[group] = [(\"right\", right_bound_factor.table_key_equivalent_group[group])]\n",
    "                        \n",
    "        else:\n",
    "            common_key_group = table_equivalent_group[left_table].intersection(right_bound_factor.equivalent_groups)\n",
    "            common_key_group = list(common_key_group)[0]\n",
    "            for group in union_key_group_set:\n",
    "                if group == common_key_group:\n",
    "                    equivalent_key_group[group] = dict()\n",
    "                    equivalent_key_group[group][\"left\"] = table_key_equivalent_group[left_table][group]\n",
    "                    equivalent_key_group[group][\"right\"] = right_bound_factor.table_key_equivalent_group[group]\n",
    "                else:\n",
    "                    if group in table_key_equivalent_group[left_table]:\n",
    "                        if group in union_key_group:\n",
    "                            union_key_group[group].append((\"left\", table_key_equivalent_group[left_table][group]))\n",
    "                        else:\n",
    "                            union_key_group[group] = [(\"left\", table_key_equivalent_group[left_table][group])]\n",
    "                    if group in right_bound_factor.table_key_equivalent_group:\n",
    "                        if group in union_key_group:\n",
    "                            union_key_group[group].append((\"right\", right_bound_factor.table_key_equivalent_group[group]))\n",
    "                        else:\n",
    "                            union_key_group[group] = [(\"right\", right_bound_factor.table_key_equivalent_group[group])]\n",
    "\n",
    "        return equivalent_key_group, union_key_group_set, union_key_group, new_join_cond\n",
    "\n",
    "    def get_additional_join_with_table_group(self, left_table, right_union_key_group, table_equivalent_group,\n",
    "                                             table_key_equivalent_group):\n",
    "        common_key_group = table_equivalent_group[left_table].intersection(set(right_union_key_group.keys()))\n",
    "        union_key_group_set = table_equivalent_group[left_table].union(set(right_union_key_group.keys()))\n",
    "        union_key_group = copy.deepcopy(right_union_key_group)\n",
    "        all_join_predicates = []\n",
    "        for group in union_key_group_set:\n",
    "            if group in common_key_group:\n",
    "                left_key = table_key_equivalent_group[left_table][group][0]\n",
    "                left_key = left_table + \".\" + left_key.split(\".\")[-1]\n",
    "                right_key = right_union_key_group[group]\n",
    "                join_predicate = left_key + \" = \" + right_key\n",
    "                all_join_predicates.append(join_predicate)\n",
    "            if group not in union_key_group:\n",
    "                assert group in table_key_equivalent_group[left_table]\n",
    "                left_key = table_key_equivalent_group[left_table][group][0]\n",
    "                left_key = left_table + \".\" + left_key.split(\".\")[-1]\n",
    "                union_key_group[group] = left_key\n",
    "\n",
    "        return all_join_predicates, union_key_group\n",
    "\n",
    "    def join_two_tables(self, sub_plan_query_str, left_table, right_table, tables_all, conditional_factors, join_keys,\n",
    "                        table_equivalent_group, table_key_equivalent_group, table_key_group_map, join_cond):\n",
    "        \"\"\"\n",
    "            Get the cardinality bound by joining the left_table with the right_table\n",
    "            :param left_table:\n",
    "            :param right_table:\n",
    "        \"\"\"\n",
    "        equivalent_key_group, union_key_group_set, union_key_group, new_join_cond = \\\n",
    "            self.get_join_keys_two_tables(left_table, right_table, table_equivalent_group, table_key_equivalent_group,\n",
    "                                          table_key_group_map, join_cond, join_keys, tables_all)\n",
    "        # print(left_table, right_table)\n",
    "        # print(equivalent_key_group)\n",
    "        # print(union_key_group)\n",
    "        # print(conditional_factors.keys())\n",
    "        cond_factor_left = conditional_factors[left_table]\n",
    "        cond_factor_right = conditional_factors[right_table]\n",
    "        bin_mode_left = self.table_buckets[tables_all[left_table]].oned_bin_modes\n",
    "        bin_mode_right = self.table_buckets[tables_all[right_table]].oned_bin_modes\n",
    "        key_group_pdf = dict()\n",
    "        key_group_bin_mode = dict()\n",
    "        new_union_key_group = dict()\n",
    "        res = cond_factor_right.table_len\n",
    "        new_na_values = dict()\n",
    "        new_variables = dict()\n",
    "        # print(equivalent_key_group)\n",
    "        for key_group in equivalent_key_group:\n",
    "            # print(key_group)\n",
    "            # print(\"========================\")\n",
    "            #print(equivalent_key_group[key_group][left_table])\n",
    "            #print(bin_mode_left)\n",
    "            # print(\"==========================================\")\n",
    "            #print(equivalent_key_group[key_group][right_table])\n",
    "            #print(bin_mode_right)\n",
    "            if len(equivalent_key_group[key_group][left_table]) > 1:\n",
    "                print(len(equivalent_key_group[key_group][left_table]), sub_plan_query_str)\n",
    "            if len(equivalent_key_group[key_group][right_table]) > 1:\n",
    "                print(len(equivalent_key_group[key_group][right_table]), sub_plan_query_str)\n",
    "            all_pdfs = [cond_factor_left.pdfs[key] * cond_factor_left.table_len * cond_factor_left.na_values[key]\n",
    "                        for key in equivalent_key_group[key_group][left_table]] + \\\n",
    "                       [cond_factor_right.pdfs[key] * res * cond_factor_right.na_values[key]\n",
    "                        for key in equivalent_key_group[key_group][right_table]]\n",
    "            all_bin_modes = [bin_mode_left[key] for key in equivalent_key_group[key_group][left_table]] + \\\n",
    "                            [bin_mode_right[key] for key in equivalent_key_group[key_group][right_table]]\n",
    "            # print(\"====================================================\")\n",
    "            # print(equivalent_key_group[key_group][left_table])\n",
    "            # print(equivalent_key_group[key_group][right_table])\n",
    "            for key in equivalent_key_group[key_group][left_table] + equivalent_key_group[key_group][right_table]:\n",
    "                new_variables[key] = key_group\n",
    "            new_pdf, new_bin_mode = self.compute_bound_oned(all_pdfs, all_bin_modes, return_factor=True)\n",
    "            res = np.sum(new_pdf)\n",
    "            key_group_pdf[key_group] = new_pdf / res\n",
    "            key_group_bin_mode[key_group] = new_bin_mode\n",
    "            new_union_key_group[key_group] = [key_group]\n",
    "            new_na_values[key_group] = 1.0\n",
    "\n",
    "        for group in union_key_group:\n",
    "            if group not in new_union_key_group:\n",
    "                new_union_key_group[group] = []\n",
    "            for table, keys in union_key_group[group]:\n",
    "                for key in keys:\n",
    "                    new_union_key_group[group].append(key)\n",
    "                    key_group_pdf[key] = conditional_factors[table].pdfs[key]\n",
    "                    key_group_bin_mode[key] = self.table_buckets[tables_all[table]].oned_bin_modes[key]\n",
    "                    new_na_values[key] = conditional_factors[table].na_values[key]\n",
    "        \n",
    "        #print(\"!!!!!!!\", union_key_group)\n",
    "        #print(new_union_key_group)\n",
    "        new_factor = Group_Factor(sub_plan_query_str, res, new_variables, key_group_pdf, key_group_bin_mode,\n",
    "                                  union_key_group_set, new_union_key_group, new_na_values, new_join_cond)\n",
    "        return new_factor, res\n",
    "\n",
    "    def get_join_keys_two_tables(self, left_table, right_table, table_equivalent_group, table_key_equivalent_group,\n",
    "                                 table_key_group_map, join_cond, join_keys, tables_all):\n",
    "        \"\"\"\n",
    "            Get the join keys between two tables\n",
    "        \"\"\"\n",
    "        actual_join_cond = []\n",
    "        for cond in join_cond[left_table]:\n",
    "            if cond in join_cond[right_table]:\n",
    "                actual_join_cond.append(cond)\n",
    "        equivalent_key_group = dict()\n",
    "        union_key_group_set = table_equivalent_group[left_table].union(table_equivalent_group[right_table])\n",
    "        union_key_group = dict()\n",
    "        new_join_cond = join_cond[left_table].union(join_cond[right_table])\n",
    "        if len(actual_join_cond) != 0:\n",
    "            for cond in actual_join_cond:\n",
    "                key1 = cond.split(\"=\")[0].strip()\n",
    "                key2 = cond.split(\"=\")[1].strip()\n",
    "                if key1.split(\".\")[0] == left_table:\n",
    "                    key_left = tables_all[left_table] + \".\" + key1.split(\".\")[-1]\n",
    "                    key_group = table_key_group_map[left_table][key_left]\n",
    "                    if key_group not in equivalent_key_group:\n",
    "                        equivalent_key_group[key_group] = dict()\n",
    "                    if left_table in equivalent_key_group[key_group]:\n",
    "                        equivalent_key_group[key_group][left_table].append(key_left)\n",
    "                    else:\n",
    "                        equivalent_key_group[key_group][left_table] = [key_left]\n",
    "                    assert key2.split(\".\")[0] == right_table, f\"unrecognized table alias\"\n",
    "                    key_right = tables_all[right_table] + \".\" + key2.split(\".\")[-1]\n",
    "                    key_group_t = table_key_group_map[right_table][key_right]\n",
    "                    assert key_group_t == key_group, f\"key group mismatch for join {cond}\"\n",
    "                    if right_table in equivalent_key_group[key_group]:\n",
    "                        equivalent_key_group[key_group][right_table].append(key_right)\n",
    "                    else:\n",
    "                        equivalent_key_group[key_group][right_table] = [key_right]\n",
    "                else:\n",
    "                    assert key2.split(\".\")[0] == left_table, f\"unrecognized table alias\"\n",
    "                    key_left = tables_all[left_table] + \".\" + key2.split(\".\")[-1]\n",
    "                    key_group = table_key_group_map[left_table][key_left]\n",
    "                    if key_group not in equivalent_key_group:\n",
    "                        equivalent_key_group[key_group] = dict()\n",
    "                    if left_table in equivalent_key_group[key_group]:\n",
    "                        equivalent_key_group[key_group][left_table].append(key_left)\n",
    "                    else:\n",
    "                        equivalent_key_group[key_group][left_table] = [key_left]\n",
    "                    assert key1.split(\".\")[0] == right_table, f\"unrecognized table alias\"\n",
    "                    key_right = tables_all[right_table] + \".\" + key1.split(\".\")[-1]\n",
    "                    key_group_t = table_key_group_map[right_table][key_right]\n",
    "                    assert key_group_t == key_group, f\"key group mismatch for join {cond}\"\n",
    "                    if right_table in equivalent_key_group[key_group]:\n",
    "                        equivalent_key_group[key_group][right_table].append(key_right)\n",
    "                    else:\n",
    "                        equivalent_key_group[key_group][right_table] = [key_right]\n",
    "\n",
    "            for group in union_key_group_set:\n",
    "                if group in equivalent_key_group:\n",
    "                    new_left_key = []\n",
    "                    for key in table_key_equivalent_group[left_table][group]:\n",
    "                        if key not in equivalent_key_group[group][left_table]:\n",
    "                            new_left_key.append(key)\n",
    "                    if len(new_left_key) != 0:\n",
    "                        union_key_group[group] = [(left_table, new_left_key)]\n",
    "                    new_right_key = []\n",
    "                    for key in table_key_equivalent_group[right_table][group]:\n",
    "                        if key not in equivalent_key_group[group][right_table]:\n",
    "                            new_right_key.append(key)\n",
    "                    if len(new_right_key) != 0:\n",
    "                        if group in union_key_group:\n",
    "                            union_key_group[group].append((right_table, new_right_key))\n",
    "                        else:\n",
    "                            union_key_group[group] = [(right_table, new_right_key)]\n",
    "                else:\n",
    "                    if group in table_key_equivalent_group[left_table]:\n",
    "                        if group in union_key_group:\n",
    "                            union_key_group[group].append((left_table, table_key_equivalent_group[left_table][group]))\n",
    "                        else:\n",
    "                            union_key_group[group] = [(left_table, table_key_equivalent_group[left_table][group])]\n",
    "                    if group in table_key_equivalent_group[right_table]:\n",
    "                        if group in union_key_group:\n",
    "                            union_key_group[group].append((right_table, table_key_equivalent_group[right_table][group]))\n",
    "                        else:\n",
    "                            union_key_group[group] = [(right_table, table_key_equivalent_group[right_table][group])]\n",
    "\n",
    "        else:\n",
    "            common_key_group = table_equivalent_group[left_table].intersection(table_equivalent_group[right_table])\n",
    "            common_key_group = list(common_key_group)[0]\n",
    "            for group in union_key_group_set:\n",
    "                if group == common_key_group:\n",
    "                    equivalent_key_group[group] = dict()\n",
    "                    equivalent_key_group[group][left_table] = table_key_equivalent_group[left_table][group]\n",
    "                    equivalent_key_group[group][right_table] = table_key_equivalent_group[right_table][group]\n",
    "                elif group in table_key_equivalent_group[left_table]:\n",
    "                    if group in union_key_group:\n",
    "                        union_key_group[group].append((left_table, table_key_equivalent_group[left_table][group]))\n",
    "                    else:\n",
    "                        union_key_group[group] = [(left_table, table_key_equivalent_group[left_table][group])]\n",
    "                else:\n",
    "                    if group in union_key_group:\n",
    "                        union_key_group[group].append((right_table, table_key_equivalent_group[right_table][group]))\n",
    "                    else:\n",
    "                        union_key_group[group] = [(right_table, table_key_equivalent_group[right_table][group])]\n",
    "\n",
    "        return equivalent_key_group, union_key_group_set, union_key_group, new_join_cond\n",
    "\n",
    "    def get_additional_joins_two_tables(self, left_table, right_table, table_equivalent_group,\n",
    "                                        table_key_equivalent_group):\n",
    "        common_key_group = table_equivalent_group[left_table].intersection(table_equivalent_group[right_table])\n",
    "        union_key_group_set = table_equivalent_group[left_table].union(table_equivalent_group[right_table])\n",
    "        union_key_group = dict()\n",
    "        all_join_predicates = []\n",
    "        for group in union_key_group_set:\n",
    "            if group in common_key_group:\n",
    "                left_key = table_key_equivalent_group[left_table][group][0]\n",
    "                left_key = left_table + \".\" + left_key.split(\".\")[-1]\n",
    "                right_key = table_key_equivalent_group[right_table][group][0]\n",
    "                right_key = right_table + \".\" + right_key.split(\".\")[-1]\n",
    "                join_predicate = left_key + \" = \" + right_key\n",
    "                all_join_predicates.append(join_predicate)\n",
    "            if group in table_key_equivalent_group[left_table]:\n",
    "                left_key = table_key_equivalent_group[left_table][group][0]\n",
    "                left_key = left_table + \".\" + left_key.split(\".\")[-1]\n",
    "                union_key_group[group] = left_key\n",
    "            else:\n",
    "                right_key = table_key_equivalent_group[right_table][group][0]\n",
    "                right_key = right_table + \".\" + right_key.split(\".\")[-1]\n",
    "                union_key_group[group] = right_key\n",
    "\n",
    "        return all_join_predicates, union_key_group\n",
    "\n",
    "    def get_sub_plan_join_key(self, sub_plan_query, join_cond):\n",
    "        # returning a subset of join_keys covered by the tables in sub_plan_query\n",
    "        touched_join_cond = set()\n",
    "        untouched_join_cond = set()\n",
    "        for tab in join_cond:\n",
    "            if tab in sub_plan_query:\n",
    "                touched_join_cond = touched_join_cond.union(join_cond[tab])\n",
    "            else:\n",
    "                untouched_join_cond = untouched_join_cond.union(join_cond[tab])\n",
    "        touched_join_cond -= untouched_join_cond\n",
    "\n",
    "        join_keys = dict()\n",
    "        for cond in touched_join_cond:\n",
    "            key1 = cond.split(\"=\")[0].strip()\n",
    "            table1 = key1.split(\".\")[0].strip()\n",
    "            if table1 not in join_keys:\n",
    "                join_keys[table1] = set([key1])\n",
    "            else:\n",
    "                join_keys[table1].add(key1)\n",
    "\n",
    "            key2 = cond.split(\"=\")[1].strip()\n",
    "            table2 = key2.split(\".\")[0].strip()\n",
    "            if table2 not in join_keys:\n",
    "                join_keys[table2] = set([key2])\n",
    "            else:\n",
    "                join_keys[table2].add(key2)\n",
    "\n",
    "        return join_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6d9ebfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"ground_truth_factors_no_filter.pkl\", \"rb\") as f:\n",
    "    ground_truth_factors_no_filter = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb5daf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle5 as pickle\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def load_sample_imdb(table_buckets, tables_alias, query_file_orders, join_keys, table_key_equivalent_group,\n",
    "                     SPERCENTAGE=1.0, qdir=\"/home/ubuntu/data_CE/saved_models/binned_cards/{}/job/all_job/\"):\n",
    "    qdir = qdir.format(SPERCENTAGE)\n",
    "    all_sample_factors = []\n",
    "    for fn in query_file_orders:\n",
    "        conditional_factors = load_sample_imdb_one_query(table_buckets, tables_alias, fn, join_keys,\n",
    "                                                         table_key_equivalent_group, SPERCENTAGE, qdir)\n",
    "        all_sample_factors.append(conditional_factors)\n",
    "    return all_sample_factors\n",
    "\n",
    "\n",
    "def load_sample_imdb_one_query(table_buckets, tables_alias, query_file_name, join_keys, table_key_equivalent_group,\n",
    "                               SPERCENTAGE=10.0, qdir=\"/home/ubuntu/data_CE/saved_models/binned_cards/{}/job/all_job/\"):\n",
    "    qdir = qdir.format(SPERCENTAGE)\n",
    "    fpath = os.path.join(qdir, query_file_name)\n",
    "    with open(fpath, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "    conditional_factors = dict()\n",
    "    table_pdfs = dict()\n",
    "    filter_size = dict()\n",
    "    for i, alias in enumerate(data[\"all_aliases\"]):\n",
    "        column = data[\"all_columns\"][i]\n",
    "        alias = alias[0]\n",
    "        key = tables_alias[alias] + \".\" + column\n",
    "        cards = data[\"results\"][i][0]\n",
    "        n_bins = table_buckets[tables_alias[alias]].bin_sizes[key]\n",
    "        pdfs = np.zeros(n_bins)\n",
    "        for (j, val) in cards:\n",
    "            if j is None:\n",
    "                j = 0\n",
    "            pdfs[j] += val\n",
    "        table_len = np.sum(pdfs)\n",
    "        #print(alias+\".\"+column, table_len, pdfs)\n",
    "        if table_len == 0:\n",
    "            # no sample satisfy the filter, set it with a small value\n",
    "            #print(\"========================\", alias+\".\"+column)\n",
    "            table_len = 1\n",
    "            pdfs = table_key_equivalent_group[tables_alias[alias]].pdfs[key]\n",
    "        else:\n",
    "            pdfs /= table_len\n",
    "        if alias not in table_pdfs:\n",
    "            table_pdfs[alias] = dict()\n",
    "            filter_size[alias] = table_len\n",
    "        table_pdfs[alias][key] = pdfs\n",
    "\n",
    "    for alias in tables_alias:\n",
    "        if alias in table_pdfs:\n",
    "            table_len = min(table_key_equivalent_group[tables_alias[alias]].table_len,\n",
    "                            filter_size[alias]/(SPERCENTAGE/100))\n",
    "            na_values = table_key_equivalent_group[tables_alias[alias]].na_values\n",
    "            conditional_factors[alias] = Factor(tables_alias[alias], table_len, list(table_pdfs[alias].keys()),\n",
    "                                                table_pdfs[alias], na_values)\n",
    "        else:\n",
    "            #TODO: ground-truth distribution\n",
    "            conditional_factors[alias] = table_key_equivalent_group[tables_alias[alias]]\n",
    "    return conditional_factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7745d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT MIN(k.keyword) AS movie_keyword, MIN(n.name) AS actor_name, MIN(t.title) AS hero_movie FROM cast_info AS ci, keyword AS k, movie_keyword AS mk, name AS n, title AS t WHERE k.keyword in ('superhero', 'sequel', 'second-part', 'marvel-comics', 'based-on-comic', 'tv-special', 'fight', 'violence') AND n.name LIKE '%Downey%Robert%' AND t.production_year > 2014 AND k.id = mk.keyword_id AND t.id = mk.movie_id AND t.id = ci.movie_id AND ci.movie_id = mk.movie_id AND n.id = ci.person_id;\n",
      "\n",
      "========================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'true_card' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-8939366a77a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m temp = BE.get_cardinality_bound_all(queries[idx], sub_plan_queries_str_all[idx], \n\u001b[0;32m----> 8\u001b[0;31m                                     q_file_names[idx], True, true_card[q_file.split(\".\")[0]])\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;31m#print(time.time() - t)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#print(len(temp))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'true_card' is not defined"
     ]
    }
   ],
   "source": [
    "q_file = \"6b.pkl\"\n",
    "idx = q_file_names.index(q_file)\n",
    "print(queries[idx])\n",
    "print(\"========================================\")\n",
    "BE = Bound_ensemble(table_buckets, schema, ground_truth_factors_no_filter)\n",
    "t = time.time()\n",
    "temp = BE.get_cardinality_bound_all(queries[idx], sub_plan_queries_str_all[idx], \n",
    "                                    q_file_names[idx], True, true_card[q_file.split(\".\")[0]])\n",
    "#print(time.time() - t)\n",
    "#print(len(temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10a94efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_true_card = np.load(\"/home/ubuntu/CEB/all_true_card_job.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d6bf438d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/bayescard/lib/python3.7/site-packages/ipykernel_launcher.py:566: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 mk ml\n",
      "2 mk t1\n",
      "2 mk t1\n",
      "2 mk ml\n",
      "2 mk t1\n",
      "2 mk t1\n",
      "11.219444036483765\n"
     ]
    }
   ],
   "source": [
    "BE = Bound_ensemble(table_buckets, schema, ground_truth_factors_no_filter)\n",
    "res = dict()\n",
    "t = time.time()\n",
    "for q_file_id in range(len(queries)):\n",
    "    #print(q_file_id, q_file_names[q_file_id])\n",
    "    temp = BE.get_cardinality_bound_all(queries[q_file_id], sub_plan_queries_str_all[q_file_id], \n",
    "                                        q_file_names[q_file_id])\n",
    "    res[q_file_names[q_file_id].split(\".pkl\")[0]] = temp\n",
    "print(time.time() - t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9af322f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 153740.0,\n",
       " 8600510.0,\n",
       " 23340.0,\n",
       " 145050.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 8600510.0,\n",
       " 145050.0,\n",
       " 303538173280.0,\n",
       " 10.0,\n",
       " 10.0,\n",
       " 303538173280.0,\n",
       " 10.0]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res['5a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6877d8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(true_card['12a'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f547592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================\n",
      "query no 0, 1a with 14 number of sub-plan queries\n",
      "q-error 50% percentile is 1.6523645051895128\n",
      "q-error 90% percentile is 568.5505611419792\n",
      "q-error 95% percentile is 12146.071923986441\n",
      "q-error 99% percentile is 29346.374443981033\n",
      "q-error 100% percentile is 33646.45007397971\n",
      "=============================================\n",
      "query no 1, 1b with 14 number of sub-plan queries\n",
      "q-error 50% percentile is 24.95164467124468\n",
      "q-error 90% percentile is 2050.3512568124197\n",
      "q-error 95% percentile is 27662.536566712795\n",
      "q-error 99% percentile is 65487.24445549219\n",
      "q-error 100% percentile is 74943.4214276871\n",
      "=============================================\n",
      "query no 2, 1c with 14 number of sub-plan queries\n",
      "q-error 50% percentile is 5.989359185221234\n",
      "q-error 90% percentile is 2784.0590651398784\n",
      "q-error 95% percentile is 578446.1601620127\n",
      "q-error 99% percentile is 1433714.558402175\n",
      "q-error 100% percentile is 1647531.657962217\n",
      "=============================================\n",
      "query no 3, 1d with 14 number of sub-plan queries\n",
      "q-error 50% percentile is 24.95164467124468\n",
      "q-error 90% percentile is 1205.9050824553522\n",
      "q-error 95% percentile is 23040.80530754438\n",
      "q-error 99% percentile is 55339.17258574186\n",
      "q-error 100% percentile is 63413.76440529128\n",
      "=============================================\n",
      "query no 4, 2a with 14 number of sub-plan queries\n",
      "q-error 50% percentile is 5.175538798825727\n",
      "q-error 90% percentile is 2524.730642421584\n",
      "q-error 95% percentile is 4755.931939013436\n",
      "q-error 99% percentile is 6542.733940163408\n",
      "q-error 100% percentile is 6989.434440450904\n",
      "=============================================\n",
      "query no 5, 2b with 14 number of sub-plan queries\n",
      "q-error 50% percentile is 4.8059153943026764\n",
      "q-error 90% percentile is 1232.5990436689717\n",
      "q-error 95% percentile is 3556.09545445744\n",
      "q-error 99% percentile is 6302.766643252207\n",
      "q-error 100% percentile is 6989.434440450904\n",
      "=============================================\n",
      "query no 6, 2c with 14 number of sub-plan queries\n",
      "q-error 50% percentile is 54.39091376253168\n",
      "q-error 90% percentile is 5128.4575743982095\n",
      "q-error 95% percentile is 13866.814315713262\n",
      "q-error 99% percentile is 24084.635844674478\n",
      "q-error 100% percentile is 26639.0912269148\n",
      "=============================================\n",
      "query no 7, 2d with 14 number of sub-plan queries\n",
      "q-error 50% percentile is 17.135858021491174\n",
      "q-error 90% percentile is 4853.069978787751\n",
      "q-error 95% percentile is 6896.583970562067\n",
      "q-error 99% percentile is 6970.864346473137\n",
      "q-error 100% percentile is 6989.434440450904\n",
      "=============================================\n",
      "query no 8, 3a with 8 number of sub-plan queries\n",
      "q-error 50% percentile is 22.47237396670067\n",
      "q-error 90% percentile is 2019279.767656364\n",
      "q-error 95% percentile is 4332431.883176146\n",
      "q-error 99% percentile is 6182953.575591975\n",
      "q-error 100% percentile is 6645583.998695933\n",
      "=============================================\n",
      "query no 9, 3b with 8 number of sub-plan queries\n",
      "q-error 50% percentile is 24.391881399753117\n",
      "q-error 90% percentile is 485057.4542339378\n",
      "q-error 95% percentile is 1037923.6146083144\n",
      "q-error 99% percentile is 1480216.5429078164\n",
      "q-error 100% percentile is 1590789.7749826922\n",
      "=============================================\n",
      "query no 10, 3c with 8 number of sub-plan queries\n",
      "q-error 50% percentile is 20.747524365578403\n",
      "q-error 90% percentile is 127614.50965254965\n",
      "q-error 95% percentile is 211245.1684470359\n",
      "q-error 99% percentile is 278149.695482625\n",
      "q-error 100% percentile is 294875.8272415223\n",
      "=============================================\n",
      "query no 11, 4a with 14 number of sub-plan queries\n",
      "q-error 50% percentile is 6.15361585492019\n",
      "q-error 90% percentile is 875.4709814536698\n",
      "q-error 95% percentile is 1399.8705290094026\n",
      "q-error 99% percentile is 1648.5652689451406\n",
      "q-error 100% percentile is 1710.7389539290755\n",
      "=============================================\n",
      "query no 12, 4b with 14 number of sub-plan queries\n",
      "q-error 50% percentile is 27.73083989215985\n",
      "q-error 90% percentile is 3824.7154120371097\n",
      "q-error 95% percentile is 14990.871865464254\n",
      "q-error 99% percentile is 30642.23418617694\n",
      "q-error 100% percentile is 34555.07476635514\n",
      "=============================================\n",
      "query no 13, 4c with 14 number of sub-plan queries\n",
      "q-error 50% percentile is 4.233060078216475\n",
      "q-error 90% percentile is 413.3976825276413\n",
      "q-error 95% percentile is 734.7593599870506\n",
      "q-error 99% percentile is 973.316665744552\n",
      "q-error 100% percentile is 1032.9559921839277\n",
      "=============================================\n",
      "query no 14, 5a with 14 number of sub-plan queries\n",
      "q-error 50% percentile is 9.09090909090909\n",
      "q-error 90% percentile is 271363309.3318504\n",
      "q-error 95% percentile is 387660502.2733078\n",
      "q-error 99% percentile is 387660502.2733078\n",
      "q-error 100% percentile is 387660502.2733078\n",
      "=============================================\n",
      "query no 15, 5b with 14 number of sub-plan queries\n",
      "q-error 50% percentile is 9.09090909090909\n",
      "q-error 90% percentile is 33545246482.727318\n",
      "q-error 95% percentile is 47921780136.36363\n",
      "q-error 99% percentile is 47921780136.36363\n",
      "q-error 100% percentile is 47921780136.36363\n",
      "=============================================\n",
      "query no 16, 5c with 14 number of sub-plan queries\n",
      "q-error 50% percentile is 141.03290316893052\n",
      "q-error 90% percentile is 20615718.314927574\n",
      "q-error 95% percentile is 22846247.01903632\n",
      "q-error 99% percentile is 22846247.01903632\n",
      "q-error 100% percentile is 22846247.01903632\n",
      "=============================================\n",
      "query no 17, 6a with 14 number of sub-plan queries\n",
      "q-error 50% percentile is 29.519174852532643\n",
      "q-error 90% percentile is 50339.04089066707\n",
      "q-error 95% percentile is 135265.2233188019\n",
      "q-error 99% percentile is 229895.14181777855\n",
      "q-error 100% percentile is 253552.62144252288\n",
      "=============================================\n",
      "query no 18, 6b with 14 number of sub-plan queries\n",
      "q-error 50% percentile is 97.75609518463065\n",
      "q-error 90% percentile is 395963.5181861974\n",
      "q-error 95% percentile is 1550096.079636491\n",
      "q-error 99% percentile is 3022256.3110031486\n",
      "q-error 100% percentile is 3390296.3688448155\n",
      "=============================================\n",
      "query no 19, 6c with 14 number of sub-plan queries\n",
      "q-error 50% percentile is 87.91343260168188\n",
      "q-error 90% percentile is 395963.5181861974\n",
      "q-error 95% percentile is 1550096.079636491\n",
      "q-error 99% percentile is 3022256.3110031486\n",
      "q-error 100% percentile is 3390296.3688448155\n",
      "=============================================\n",
      "query no 20, 6d with 14 number of sub-plan queries\n",
      "q-error 50% percentile is 107.15464158716826\n",
      "q-error 90% percentile is 13411.779491878742\n",
      "q-error 95% percentile is 45483.90305267979\n",
      "q-error 99% percentile is 87532.46189974429\n",
      "q-error 100% percentile is 98044.60161151049\n",
      "=============================================\n",
      "query no 21, 6e with 14 number of sub-plan queries\n",
      "q-error 50% percentile is 31.198278475747184\n",
      "q-error 90% percentile is 12198.32360105598\n",
      "q-error 95% percentile is 45483.90305267979\n",
      "q-error 99% percentile is 87532.46189974429\n",
      "q-error 100% percentile is 98044.60161151049\n",
      "=============================================\n",
      "query no 22, 6f with 14 number of sub-plan queries\n",
      "q-error 50% percentile is 138.78108276014825\n",
      "q-error 90% percentile is 66571.47793821675\n",
      "q-error 95% percentile is 94778.93214475947\n",
      "q-error 99% percentile is 97391.46771816027\n",
      "q-error 100% percentile is 98044.60161151049\n",
      "=============================================\n",
      "query no 23, 7a with 82 number of sub-plan queries\n",
      "q-error 50% percentile is 9019.599398143817\n",
      "q-error 90% percentile is 46390234877.537056\n",
      "q-error 95% percentile is 1322317030843.3076\n",
      "q-error 99% percentile is 1313208161724406.2\n",
      "q-error 100% percentile is 3055226323979059.5\n",
      "=============================================\n",
      "query no 24, 7b with 82 number of sub-plan queries\n",
      "q-error 50% percentile is 32218.88203831303\n",
      "q-error 90% percentile is 91621544646.69199\n",
      "q-error 95% percentile is 938489322170.8379\n",
      "q-error 99% percentile is 4028740077433685.0\n",
      "q-error 100% percentile is 1.0029112498279088e+16\n",
      "=============================================\n",
      "query no 25, 7c with 82 number of sub-plan queries\n",
      "q-error 50% percentile is 61.74316821943693\n",
      "q-error 90% percentile is 33564674.24194549\n",
      "q-error 95% percentile is 187147016.52779517\n",
      "q-error 99% percentile is 835050373773509.5\n",
      "q-error 100% percentile is 2997818325169438.5\n",
      "=============================================\n",
      "query no 26, 8a with 51 number of sub-plan queries\n",
      "q-error 50% percentile is 25867.61076087615\n",
      "q-error 90% percentile is 2393451892.578105\n",
      "q-error 95% percentile is 62002385793.24161\n",
      "q-error 99% percentile is 3778942071839.5195\n",
      "q-error 100% percentile is 7265854422147.937\n",
      "=============================================\n",
      "query no 27, 8b with 51 number of sub-plan queries\n",
      "q-error 50% percentile is 2481.083654257595\n",
      "q-error 90% percentile is 3098439811.1917205\n",
      "q-error 95% percentile is 3793714895.9170284\n",
      "q-error 99% percentile is 7469935819.035944\n",
      "q-error 100% percentile is 10296402265.71468\n",
      "=============================================\n",
      "query no 28, 8c with 51 number of sub-plan queries\n",
      "q-error 50% percentile is 21.50923664341055\n",
      "q-error 90% percentile is 1071219.648213709\n",
      "q-error 95% percentile is 6303699.098372763\n",
      "q-error 99% percentile is 85618775.38051781\n",
      "q-error 100% percentile is 116211867.74116822\n",
      "=============================================\n",
      "query no 29, 8d with 51 number of sub-plan queries\n",
      "q-error 50% percentile is 24.79570787891351\n",
      "q-error 90% percentile is 1071219.648213709\n",
      "q-error 95% percentile is 6303699.098372763\n",
      "q-error 99% percentile is 85618775.38051781\n",
      "q-error 100% percentile is 116211867.74116822\n",
      "=============================================\n",
      "query no 30, 9a with 99 number of sub-plan queries\n",
      "q-error 50% percentile is 35377.849529839455\n",
      "q-error 90% percentile is 17075972923.392078\n",
      "q-error 95% percentile is 174514661081.72534\n",
      "q-error 99% percentile is 16760051992474.816\n",
      "q-error 100% percentile is 17031447414692.684\n",
      "=============================================\n",
      "query no 31, 9b with 99 number of sub-plan queries\n",
      "q-error 50% percentile is 116310.67247013544\n",
      "q-error 90% percentile is 42481076838.35577\n",
      "q-error 95% percentile is 244381584330.5928\n",
      "q-error 99% percentile is 24374700762394.902\n",
      "q-error 100% percentile is 24377238129586.59\n",
      "=============================================\n",
      "query no 32, 9c with 99 number of sub-plan queries\n",
      "q-error 50% percentile is 1518.7143462705817\n",
      "q-error 90% percentile is 224253050.5644498\n",
      "q-error 95% percentile is 7911009734.169243\n",
      "q-error 99% percentile is 564311307263.3823\n",
      "q-error 100% percentile is 589395177916.5476\n",
      "=============================================\n",
      "query no 33, 9d with 99 number of sub-plan queries\n",
      "q-error 50% percentile is 81.67291245854149\n",
      "q-error 90% percentile is 7463005.429423509\n",
      "q-error 95% percentile is 174470238.26710784\n",
      "q-error 99% percentile is 9502611327.254993\n",
      "q-error 100% percentile is 9936272369.809607\n",
      "=============================================\n",
      "query no 34, 10a with 46 number of sub-plan queries\n",
      "q-error 50% percentile is 35.3315649867374\n",
      "q-error 90% percentile is 1966335.4660287292\n",
      "q-error 95% percentile is 4500389.864990918\n",
      "q-error 99% percentile is 4968498.943915616\n",
      "q-error 100% percentile is 4968498.943915616\n",
      "=============================================\n",
      "query no 35, 10b with 46 number of sub-plan queries\n",
      "q-error 50% percentile is 10.047089304922679\n",
      "q-error 90% percentile is 5877597.3675324675\n",
      "q-error 95% percentile is 96170638.82800141\n",
      "q-error 99% percentile is 1036467338085.2593\n",
      "q-error 100% percentile is 1792574906525.951\n",
      "=============================================\n",
      "query no 36, 10c with 46 number of sub-plan queries\n",
      "q-error 50% percentile is 63612.690001433\n",
      "q-error 90% percentile is 7060312446.832342\n",
      "q-error 95% percentile is 7060312446.832342\n",
      "q-error 99% percentile is 50986822690.83366\n",
      "q-error 100% percentile is 85839865103.93535\n",
      "=============================================\n",
      "query no 37, 11a with 85 number of sub-plan queries\n",
      "q-error 50% percentile is 650.1074608749032\n",
      "q-error 90% percentile is 5713115192.813307\n",
      "q-error 95% percentile is 79030249954.79059\n",
      "q-error 99% percentile is 31617562947444.223\n",
      "q-error 100% percentile is 197005548207664.97\n",
      "=============================================\n",
      "query no 38, 11b with 85 number of sub-plan queries\n",
      "q-error 50% percentile is 1889.4572265384923\n",
      "q-error 90% percentile is 124375570.71647394\n",
      "q-error 95% percentile is 372200434.94736844\n",
      "q-error 99% percentile is 1831673905279.7935\n",
      "q-error 100% percentile is 11442658051800.953\n",
      "=============================================\n",
      "query no 39, 11c with 85 number of sub-plan queries\n",
      "q-error 50% percentile is 58.1899347056741\n",
      "q-error 90% percentile is 4092691.9034991027\n",
      "q-error 95% percentile is 477882396.0274677\n",
      "q-error 99% percentile is 738758199956.9366\n",
      "q-error 100% percentile is 4584520909490.439\n",
      "=============================================\n",
      "query no 40, 11d with 85 number of sub-plan queries\n",
      "q-error 50% percentile is 133.1162449578291\n",
      "q-error 90% percentile is 260668208.34975022\n",
      "q-error 95% percentile is 2706860558.5349293\n",
      "q-error 99% percentile is 738758199956.9366\n",
      "q-error 100% percentile is 4584520909490.439\n",
      "=============================================\n",
      "query no 41, 12a with 85 number of sub-plan queries\n",
      "q-error 50% percentile is 49347.387741490376\n",
      "q-error 90% percentile is 16430150131482.834\n",
      "q-error 95% percentile is 5873789826330170.0\n",
      "q-error 99% percentile is 1.5451067849325892e+16\n",
      "q-error 100% percentile is 1.5451067849325892e+16\n",
      "=============================================\n",
      "query no 42, 12b with 85 number of sub-plan queries\n",
      "q-error 50% percentile is 11609.898341975044\n",
      "q-error 90% percentile is 399261128.80815357\n",
      "q-error 95% percentile is 1440758222329.6465\n",
      "q-error 99% percentile is 167623376036108.28\n",
      "q-error 100% percentile is 167623376036108.28\n",
      "=============================================\n",
      "query no 43, 12c with 85 number of sub-plan queries\n",
      "q-error 50% percentile is 10636.07166146951\n",
      "q-error 90% percentile is 1408369444209.3894\n",
      "q-error 95% percentile is 1741389829816591.0\n",
      "q-error 99% percentile is 5627964585801482.0\n",
      "q-error 100% percentile is 5627964585801482.0\n",
      "=============================================\n",
      "query no 44, 13a with 130 number of sub-plan queries\n",
      "q-error 50% percentile is 11.850329010997864\n",
      "q-error 90% percentile is 3027519771.428798\n",
      "q-error 95% percentile is 3977906141307.213\n",
      "q-error 99% percentile is 255011679538196.9\n",
      "q-error 100% percentile is 1.9259459880091376e+16\n",
      "=============================================\n",
      "query no 45, 13b with 130 number of sub-plan queries\n",
      "q-error 50% percentile is 3246.882465257948\n",
      "q-error 90% percentile is 1303705909566.1057\n",
      "q-error 95% percentile is 10221056569303.225\n",
      "q-error 99% percentile is 67311188279371.625\n",
      "q-error 100% percentile is 8.240698389259667e+17\n",
      "=============================================\n",
      "query no 46, 13c with 130 number of sub-plan queries\n",
      "q-error 50% percentile is 14852.728166069295\n",
      "q-error 90% percentile is 2091303438690.3953\n",
      "q-error 95% percentile is 10867023122144.033\n",
      "q-error 99% percentile is 47201884064066.91\n",
      "q-error 100% percentile is 5.026491512201792e+17\n",
      "=============================================\n",
      "query no 47, 13d with 130 number of sub-plan queries\n",
      "q-error 50% percentile is 8.177839391558743\n",
      "q-error 90% percentile is 564026001.5285873\n",
      "q-error 95% percentile is 1575773765930.3506\n",
      "q-error 99% percentile is 35419231753554.375\n",
      "q-error 100% percentile is 1.9259459880091376e+16\n",
      "=============================================\n",
      "query no 48, 14a with 76 number of sub-plan queries\n",
      "q-error 50% percentile is 66.68164596780045\n",
      "q-error 90% percentile is 475381.75573283155\n",
      "q-error 95% percentile is 19676901606.08465\n",
      "q-error 99% percentile is 5.19305183802964e+17\n",
      "q-error 100% percentile is 2.0771667572099392e+18\n",
      "=============================================\n",
      "query no 49, 14b with 76 number of sub-plan queries\n",
      "q-error 50% percentile is 2671.6054019977355\n",
      "q-error 90% percentile is 75536429.26724139\n",
      "q-error 95% percentile is 18234146833.818916\n",
      "q-error 99% percentile is 2.3727324216853705e+19\n",
      "q-error 100% percentile is 9.490929058048256e+19\n",
      "=============================================\n",
      "query no 50, 14c with 76 number of sub-plan queries\n",
      "q-error 50% percentile is 62.21948701102702\n",
      "q-error 90% percentile is 132940.72446485207\n",
      "q-error 95% percentile is 10713890951.175503\n",
      "q-error 99% percentile is 8.685779605586568e+16\n",
      "q-error 100% percentile is 3.4739750177032826e+17\n",
      "=============================================\n",
      "query no 51, 15a with 174 number of sub-plan queries\n",
      "q-error 50% percentile is 399197.8947368421\n",
      "q-error 90% percentile is 102172307052259.25\n",
      "q-error 95% percentile is 3972576468975000.5\n",
      "q-error 99% percentile is 2.014129965667722e+16\n",
      "q-error 100% percentile is 2.014129965667722e+16\n",
      "=============================================\n",
      "query no 52, 15b with 174 number of sub-plan queries\n",
      "q-error 50% percentile is 399197.8947368421\n",
      "q-error 90% percentile is 405833848116081.94\n",
      "q-error 95% percentile is 4955278528030691.0\n",
      "q-error 99% percentile is 9.341692233727805e+16\n",
      "q-error 100% percentile is 9.341692233727805e+16\n",
      "=============================================\n",
      "query no 53, 15c with 174 number of sub-plan queries\n",
      "q-error 50% percentile is 75847.6\n",
      "q-error 90% percentile is 77059909390740.23\n",
      "q-error 95% percentile is 700609362888922.6\n",
      "q-error 99% percentile is 3700354360655802.0\n",
      "q-error 100% percentile is 3700354360655802.0\n",
      "=============================================\n",
      "query no 54, 15d with 174 number of sub-plan queries\n",
      "q-error 50% percentile is 31646.312121176845\n",
      "q-error 90% percentile is 61236606002185.09\n",
      "q-error 95% percentile is 672247575047536.4\n",
      "q-error 99% percentile is 1536850302827785.8\n",
      "q-error 100% percentile is 1794021799221022.0\n",
      "=============================================\n",
      "query no 55, 16a with 86 number of sub-plan queries\n",
      "q-error 50% percentile is 6538.051208948632\n",
      "q-error 90% percentile is 28655445698.718796\n",
      "q-error 95% percentile is 60635695138398.43\n",
      "q-error 99% percentile is 248246369456357.5\n",
      "q-error 100% percentile is 248246369456357.5\n",
      "=============================================\n",
      "query no 56, 16b with 86 number of sub-plan queries\n",
      "q-error 50% percentile is 229.2803649529143\n",
      "q-error 90% percentile is 89123659.5070503\n",
      "q-error 95% percentile is 1713167094.7444959\n",
      "q-error 99% percentile is 239205297742.93973\n",
      "q-error 100% percentile is 239205297742.93973\n",
      "=============================================\n",
      "query no 57, 16c with 86 number of sub-plan queries\n",
      "q-error 50% percentile is 373.9986248269892\n",
      "q-error 90% percentile is 97220982.77350834\n",
      "q-error 95% percentile is 1120562215224.1665\n",
      "q-error 99% percentile is 1907150538728.9084\n",
      "q-error 100% percentile is 2189840951253.217\n",
      "=============================================\n",
      "query no 58, 16d with 86 number of sub-plan queries\n",
      "q-error 50% percentile is 433.7481785051175\n",
      "q-error 90% percentile is 92314122.01002708\n",
      "q-error 95% percentile is 1285371693730.6514\n",
      "q-error 99% percentile is 2000747719025.752\n",
      "q-error 100% percentile is 2378691440734.391\n",
      "=============================================\n",
      "query no 59, 17a with 49 number of sub-plan queries\n",
      "q-error 50% percentile is 123.44617419899348\n",
      "q-error 90% percentile is 2308573.083791701\n",
      "q-error 95% percentile is 1713167094.7444959\n",
      "q-error 99% percentile is 41353993700.90963\n",
      "q-error 100% percentile is 62469811165.74175\n",
      "=============================================\n",
      "query no 60, 17b with 49 number of sub-plan queries\n",
      "q-error 50% percentile is 124.7897342312435\n",
      "q-error 90% percentile is 6492801.178037236\n",
      "q-error 95% percentile is 14428463581.511257\n",
      "q-error 99% percentile is 54081568161.97424\n",
      "q-error 100% percentile is 82859636090.54543\n",
      "=============================================\n",
      "query no 61, 17c with 49 number of sub-plan queries\n",
      "q-error 50% percentile is 210.76182496522068\n",
      "q-error 90% percentile is 11372205.661067683\n",
      "q-error 95% percentile is 23517357093.40371\n",
      "q-error 99% percentile is 61352682971.48834\n",
      "q-error 100% percentile is 82859636090.54543\n",
      "=============================================\n",
      "query no 62, 17d with 49 number of sub-plan queries\n",
      "q-error 50% percentile is 124.7897342312435\n",
      "q-error 90% percentile is 5058960.169708178\n",
      "q-error 95% percentile is 12215845652.205418\n",
      "q-error 99% percentile is 52311473818.52954\n",
      "q-error 100% percentile is 82859636090.54543\n",
      "=============================================\n",
      "query no 63, 17e with 49 number of sub-plan queries\n",
      "q-error 50% percentile is 75.56393884037244\n",
      "q-error 90% percentile is 970951.9398648901\n",
      "q-error 95% percentile is 1713167094.7444959\n",
      "q-error 99% percentile is 44397340309.228775\n",
      "q-error 100% percentile is 67082413891.757706\n",
      "=============================================\n",
      "query no 64, 17f with 49 number of sub-plan queries\n",
      "q-error 50% percentile is 75.56393884037244\n",
      "q-error 90% percentile is 1849743.2800679733\n",
      "q-error 95% percentile is 11636968873.552313\n",
      "q-error 99% percentile is 51848372395.607056\n",
      "q-error 100% percentile is 82859636090.54543\n",
      "=============================================\n",
      "query no 65, 18a with 49 number of sub-plan queries\n",
      "q-error 50% percentile is 1456.9642557463787\n",
      "q-error 90% percentile is 20968447578133.844\n",
      "q-error 95% percentile is 4595450623168526.0\n",
      "q-error 99% percentile is 5.608195760171635e+17\n",
      "q-error 100% percentile is 1.0634465539843452e+18\n",
      "=============================================\n",
      "query no 66, 18b with 49 number of sub-plan queries\n",
      "q-error 50% percentile is 277929.04764504184\n",
      "q-error 90% percentile is 5.660048481611687e+16\n",
      "q-error 95% percentile is 5.217144573075189e+19\n",
      "q-error 99% percentile is 1.0582217413953584e+20\n",
      "q-error 100% percentile is 1.0582217413953584e+20\n",
      "=============================================\n",
      "query no 67, 18c with 49 number of sub-plan queries\n",
      "q-error 50% percentile is 2625.2956554393677\n",
      "q-error 90% percentile is 21437670329963.203\n",
      "q-error 95% percentile is 4.3144973207180504e+16\n",
      "q-error 99% percentile is 2.0435734061407022e+18\n",
      "q-error 100% percentile is 2.0767811647417554e+18\n",
      "=============================================\n",
      "query no 68, 19a with 302 number of sub-plan queries\n",
      "q-error 50% percentile is 3293747.1722080135\n",
      "q-error 90% percentile is 238700148003123.75\n",
      "q-error 95% percentile is 1079264424263239.6\n",
      "q-error 99% percentile is 3.1334238203649664e+17\n",
      "q-error 100% percentile is 3.219317487014624e+17\n",
      "=============================================\n",
      "query no 69, 19b with 302 number of sub-plan queries\n",
      "q-error 50% percentile is 2477264.6666666665\n",
      "q-error 90% percentile is 20140832646148.594\n",
      "q-error 95% percentile is 36610220934000.0\n",
      "q-error 99% percentile is 63853108607888.48\n",
      "q-error 100% percentile is 64128289291462.85\n",
      "=============================================\n",
      "query no 70, 19c with 302 number of sub-plan queries\n",
      "q-error 50% percentile is 210594.3340870641\n",
      "q-error 90% percentile is 27308644846813.773\n",
      "q-error 95% percentile is 112976835894880.83\n",
      "q-error 99% percentile is 3.671965173368218e+16\n",
      "q-error 100% percentile is 3.871780461845861e+16\n",
      "=============================================\n",
      "query no 71, 19d with 302 number of sub-plan queries\n",
      "q-error 50% percentile is 212518.3841107061\n",
      "q-error 90% percentile is 45028835792.78101\n",
      "q-error 95% percentile is 2058260696133.5205\n",
      "q-error 99% percentile is 20513649026204.297\n",
      "q-error 100% percentile is 78616729386667.69\n",
      "=============================================\n",
      "query no 72, 20a with 220 number of sub-plan queries\n",
      "q-error 50% percentile is 125.88523363583704\n",
      "q-error 90% percentile is 357049.8136733503\n",
      "q-error 95% percentile is 67666058.7359438\n",
      "q-error 99% percentile is 9116454708.719643\n",
      "q-error 100% percentile is 9597821322.883469\n",
      "=============================================\n",
      "query no 73, 20b with 220 number of sub-plan queries\n",
      "q-error 50% percentile is 449.85020523152014\n",
      "q-error 90% percentile is 483990.7642154772\n",
      "q-error 95% percentile is 69114429.15573224\n",
      "q-error 99% percentile is 8755698105.006487\n",
      "q-error 100% percentile is 9867184577.101732\n",
      "=============================================\n",
      "query no 74, 20c with 220 number of sub-plan queries\n",
      "q-error 50% percentile is 60.2090415787485\n",
      "q-error 90% percentile is 59330.84720055518\n",
      "q-error 95% percentile is 85912617.57152405\n",
      "q-error 99% percentile is 3173130361.4948273\n",
      "q-error 100% percentile is 3173130361.4948273\n",
      "=============================================\n",
      "query no 75, 21a with 174 number of sub-plan queries\n",
      "q-error 50% percentile is 548078.9877802606\n",
      "q-error 90% percentile is 221466997864495.53\n",
      "q-error 95% percentile is 1.3049203668720477e+17\n",
      "q-error 99% percentile is 1.5329903974246177e+20\n",
      "q-error 100% percentile is 2.593766954296349e+20\n",
      "=============================================\n",
      "query no 76, 21b with 174 number of sub-plan queries\n",
      "q-error 50% percentile is 999327.8977780575\n",
      "q-error 90% percentile is 2455091830429213.5\n",
      "q-error 95% percentile is 5.888166891405473e+17\n",
      "q-error 99% percentile is 3.309321757150879e+21\n",
      "q-error 100% percentile is 6.932388387813422e+21\n",
      "=============================================\n",
      "query no 77, 21c with 174 number of sub-plan queries\n",
      "q-error 50% percentile is 300953.14209598675\n",
      "q-error 90% percentile is 143891773868177.5\n",
      "q-error 95% percentile is 9.110160758829291e+16\n",
      "q-error 99% percentile is 1.2860657616010969e+20\n",
      "q-error 100% percentile is 2.1111734299378496e+20\n",
      "=============================================\n",
      "query no 78, 22a with 399 number of sub-plan queries\n",
      "q-error 50% percentile is 1122.074356737898\n",
      "q-error 90% percentile is 19256748769873.363\n",
      "q-error 95% percentile is 1468672259086101.2\n",
      "q-error 99% percentile is 5.356552991995683e+18\n",
      "q-error 100% percentile is 8.509120075040789e+81\n",
      "=============================================\n",
      "query no 79, 22b with 399 number of sub-plan queries\n",
      "q-error 50% percentile is 27274.7332155477\n",
      "q-error 90% percentile is 51461261596866.88\n",
      "q-error 95% percentile is 6.859771629193333e+16\n",
      "q-error 99% percentile is 4.500256208699937e+20\n",
      "q-error 100% percentile is 9.293057975503812e+83\n",
      "=============================================\n",
      "query no 80, 22c with 399 number of sub-plan queries\n",
      "q-error 50% percentile is 313.61732081911265\n",
      "q-error 90% percentile is 14980730953226.934\n",
      "q-error 95% percentile is 379585974039704.5\n",
      "q-error 99% percentile is 1.0415276542744358e+18\n",
      "q-error 100% percentile is 1.395346071817679e+81\n",
      "=============================================\n",
      "query no 81, 22d with 399 number of sub-plan queries\n",
      "q-error 50% percentile is 268.1226777885279\n",
      "q-error 90% percentile is 5676748590969.996\n",
      "q-error 95% percentile is 134578013588927.28\n",
      "q-error 99% percentile is 4.4614364229804006e+17\n",
      "q-error 100% percentile is 5.015615478235451e+80\n",
      "=============================================\n",
      "query no 82, 23a with 399 number of sub-plan queries\n",
      "q-error 50% percentile is 6677.265804597701\n",
      "q-error 90% percentile is 62927918001.33556\n",
      "q-error 95% percentile is 470013530715.5654\n",
      "q-error 99% percentile is 726233429915332.4\n",
      "q-error 100% percentile is 1.2871360545442768e+16\n",
      "=============================================\n",
      "query no 83, 23b with 399 number of sub-plan queries\n",
      "q-error 50% percentile is 7520.027508090615\n",
      "q-error 90% percentile is 10232765006.26499\n",
      "q-error 95% percentile is 346209473676.2813\n",
      "q-error 99% percentile is 722850696814662.4\n",
      "q-error 100% percentile is 1.2838120478767104e+16\n",
      "=============================================\n",
      "query no 84, 23c with 399 number of sub-plan queries\n",
      "q-error 50% percentile is 2575.6040682678913\n",
      "q-error 90% percentile is 55925407509.20268\n",
      "q-error 95% percentile is 466728908651.29663\n",
      "q-error 99% percentile is 700609362888922.6\n",
      "q-error 100% percentile is 7918236043511459.0\n",
      "=============================================\n",
      "query no 85, 24a with 913 number of sub-plan queries\n",
      "q-error 50% percentile is 26792.45287422465\n",
      "q-error 90% percentile is 2847624104546519.0\n",
      "q-error 95% percentile is 1.4167968443766821e+17\n",
      "q-error 99% percentile is 6.792562607391153e+18\n",
      "q-error 100% percentile is 7.653639292745341e+19\n",
      "=============================================\n",
      "query no 86, 24b with 913 number of sub-plan queries\n",
      "q-error 50% percentile is 53642.51582278481\n",
      "q-error 90% percentile is 250629042542809.28\n",
      "q-error 95% percentile is 4399046471883162.0\n",
      "q-error 99% percentile is 7.154068128430215e+16\n",
      "q-error 100% percentile is 3.571660069189662e+17\n",
      "=============================================\n",
      "query no 87, 25a with 156 number of sub-plan queries\n",
      "q-error 50% percentile is 2394.85106961202\n",
      "q-error 90% percentile is 7.656775122380202e+16\n",
      "q-error 95% percentile is 7.778499102992275e+20\n",
      "q-error 99% percentile is 2.5705483227880252e+23\n",
      "q-error 100% percentile is 4.43063049560324e+23\n",
      "=============================================\n",
      "query no 88, 25b with 156 number of sub-plan queries\n",
      "q-error 50% percentile is 65293.34579301971\n",
      "q-error 90% percentile is 1.1845491091027565e+17\n",
      "q-error 95% percentile is 3.2869197836502523e+21\n",
      "q-error 99% percentile is 3.943572270487184e+22\n",
      "q-error 100% percentile is 3.943572270487184e+22\n",
      "=============================================\n",
      "query no 89, 25c with 156 number of sub-plan queries\n",
      "q-error 50% percentile is 3030.314547611969\n",
      "q-error 90% percentile is 2.28751905883752e+16\n",
      "q-error 95% percentile is 4.203374201100931e+20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q-error 99% percentile is 1.879750715678719e+23\n",
      "q-error 100% percentile is 3.2136405289256566e+23\n",
      "=============================================\n",
      "query no 90, 26a with 669 number of sub-plan queries\n",
      "q-error 50% percentile is 345.3758174791914\n",
      "q-error 90% percentile is 1518287812.532805\n",
      "q-error 95% percentile is 275490561139.33325\n",
      "q-error 99% percentile is 11099288297355.049\n",
      "q-error 100% percentile is 15244695886766.25\n",
      "=============================================\n",
      "query no 91, 26b with 669 number of sub-plan queries\n",
      "q-error 50% percentile is 768.3530454746523\n",
      "q-error 90% percentile is 4653657554.351193\n",
      "q-error 95% percentile is 1060683722029.5021\n",
      "q-error 99% percentile is 61912976777564.92\n",
      "q-error 100% percentile is 104798165121167.48\n",
      "=============================================\n",
      "query no 92, 26c with 669 number of sub-plan queries\n",
      "q-error 50% percentile is 142.2671078426895\n",
      "q-error 90% percentile is 579607562.9937037\n",
      "q-error 95% percentile is 44452093448.00943\n",
      "q-error 99% percentile is 3971458153379.85\n",
      "q-error 100% percentile is 4978217551028.193\n",
      "=============================================\n",
      "query no 93, 27a with 893 number of sub-plan queries\n",
      "q-error 50% percentile is 1362861.6571787314\n",
      "q-error 90% percentile is 343257882230.13696\n",
      "q-error 95% percentile is 24256797197883.254\n",
      "q-error 99% percentile is 6.091953866012578e+16\n",
      "q-error 100% percentile is 3.6037953826542846e+20\n",
      "=============================================\n",
      "query no 94, 27b with 893 number of sub-plan queries\n",
      "q-error 50% percentile is 2747600.3121412233\n",
      "q-error 90% percentile is 62834040872.63158\n",
      "q-error 95% percentile is 3060812734838.66\n",
      "q-error 99% percentile is 5922940167066188.0\n",
      "q-error 100% percentile is 1.9434093458471495e+20\n",
      "=============================================\n",
      "query no 95, 27c with 893 number of sub-plan queries\n",
      "q-error 50% percentile is 1309474.9119445588\n",
      "q-error 90% percentile is 261706433883.91632\n",
      "q-error 95% percentile is 21233679445108.05\n",
      "q-error 99% percentile is 3.640276550396324e+16\n",
      "q-error 100% percentile is 2.1111734299378496e+20\n",
      "=============================================\n",
      "query no 96, 28a with 2018 number of sub-plan queries\n",
      "q-error 50% percentile is 707.5579338161151\n",
      "q-error 90% percentile is 1584454531.225168\n",
      "q-error 95% percentile is 16357916835.626127\n",
      "q-error 99% percentile is 262943183360858.66\n",
      "q-error 100% percentile is 6.889102611505359e+80\n",
      "=============================================\n",
      "query no 97, 28b with 2018 number of sub-plan queries\n",
      "q-error 50% percentile is 6079.343926890813\n",
      "q-error 90% percentile is 31621303539.070225\n",
      "q-error 95% percentile is 909458852112.0421\n",
      "q-error 99% percentile is 8430998048681285.0\n",
      "q-error 100% percentile is 8.209270469057875e+80\n",
      "=============================================\n",
      "query no 98, 28c with 2018 number of sub-plan queries\n",
      "q-error 50% percentile is 598.8398996238085\n",
      "q-error 90% percentile is 1634686745.8469815\n",
      "q-error 95% percentile is 13019002064.31924\n",
      "q-error 99% percentile is 369251059734913.3\n",
      "q-error 100% percentile is 1.395346071817679e+81\n",
      "=============================================\n",
      "query no 99, 29a with 13231 number of sub-plan queries\n",
      "q-error 50% percentile is 760220.6072568977\n",
      "q-error 90% percentile is 401896437936.729\n",
      "q-error 95% percentile is 7469779193585.484\n",
      "q-error 99% percentile is 401391768892887.9\n",
      "q-error 100% percentile is 5.357477294798819e+16\n",
      "=============================================\n",
      "query no 100, 29b with 13231 number of sub-plan queries\n",
      "q-error 50% percentile is 2337236.4153744746\n",
      "q-error 90% percentile is 7970888285292.964\n",
      "q-error 95% percentile is 43411479893881.4\n",
      "q-error 99% percentile is 4687290294164217.0\n",
      "q-error 100% percentile is 3.8410793795073695e+18\n",
      "=============================================\n",
      "query no 101, 29c with 13231 number of sub-plan queries\n",
      "q-error 50% percentile is 385107.48652537516\n",
      "q-error 90% percentile is 13435145119945.896\n",
      "q-error 95% percentile is 228314828975281.84\n",
      "q-error 99% percentile is 2.143387800092871e+16\n",
      "q-error 100% percentile is 3.919717339259925e+19\n",
      "=============================================\n",
      "query no 102, 30a with 803 number of sub-plan queries\n",
      "q-error 50% percentile is 68974.66922213072\n",
      "q-error 90% percentile is 60853582015816.72\n",
      "q-error 95% percentile is 3.207498635218449e+16\n",
      "q-error 99% percentile is 4.818368826187634e+18\n",
      "q-error 100% percentile is 8.191865316538038e+23\n",
      "=============================================\n",
      "query no 103, 30b with 803 number of sub-plan queries\n",
      "q-error 50% percentile is 1239996.1461126006\n",
      "q-error 90% percentile is 32060676201954.133\n",
      "q-error 95% percentile is 1276834837392873.8\n",
      "q-error 99% percentile is 1.4177357987154e+18\n",
      "q-error 100% percentile is 7.463408117571338e+22\n",
      "=============================================\n",
      "query no 104, 30c with 803 number of sub-plan queries\n",
      "q-error 50% percentile is 27808.287589283565\n",
      "q-error 90% percentile is 16745826393742.307\n",
      "q-error 95% percentile is 8351871760952010.0\n",
      "q-error 99% percentile is 2.0753975081333793e+18\n",
      "q-error 100% percentile is 3.2136405289256566e+23\n",
      "=============================================\n",
      "query no 105, 31a with 479 number of sub-plan queries\n",
      "q-error 50% percentile is 36285.28263931523\n",
      "q-error 90% percentile is 2.1598266587591365e+18\n",
      "q-error 95% percentile is 4.330409253153239e+21\n",
      "q-error 99% percentile is 1.981588059145385e+24\n",
      "q-error 100% percentile is 1.756340910913624e+25\n",
      "=============================================\n",
      "query no 106, 31b with 479 number of sub-plan queries\n",
      "q-error 50% percentile is 318903.8255850603\n",
      "q-error 90% percentile is 1.5850908809717596e+18\n",
      "q-error 95% percentile is 2.7448550001842685e+21\n",
      "q-error 99% percentile is 7.463408117571338e+22\n",
      "q-error 100% percentile is 1.2004266772511645e+23\n",
      "=============================================\n",
      "query no 107, 31c with 479 number of sub-plan queries\n",
      "q-error 50% percentile is 25602.718520018996\n",
      "q-error 90% percentile is 9.362846869417715e+17\n",
      "q-error 95% percentile is 2.606073221919026e+21\n",
      "q-error 99% percentile is 2.0342302964470895e+24\n",
      "q-error 100% percentile is 9.532441116957972e+24\n",
      "=============================================\n",
      "query no 108, 32a with 26 number of sub-plan queries\n",
      "q-error 50% percentile is 9.09090909090909\n",
      "q-error 90% percentile is 1413811899501.5083\n",
      "q-error 95% percentile is 4712705958928.486\n",
      "q-error 99% percentile is 4712705958928.486\n",
      "q-error 100% percentile is 4712705958928.486\n",
      "=============================================\n",
      "query no 109, 32b with 26 number of sub-plan queries\n",
      "q-error 50% percentile is 87.65248384827885\n",
      "q-error 90% percentile is 3734341926001.698\n",
      "q-error 95% percentile is 4712705958928.486\n",
      "q-error 99% percentile is 4712705958928.486\n",
      "q-error 100% percentile is 4712705958928.486\n",
      "=============================================\n",
      "query no 110, 33a with 2456 number of sub-plan queries\n",
      "q-error 50% percentile is 74036.38578397255\n",
      "q-error 90% percentile is 8511353918.006133\n",
      "q-error 95% percentile is 289100449044.1667\n",
      "q-error 99% percentile is 9.973134202495754e+16\n",
      "q-error 100% percentile is 2.2131327667195762e+45\n",
      "=============================================\n",
      "query no 111, 33b with 2456 number of sub-plan queries\n",
      "q-error 50% percentile is 190882.0343261549\n",
      "q-error 90% percentile is 32648236270.184616\n",
      "q-error 95% percentile is 1514201881894.9712\n",
      "q-error 99% percentile is 5.3161534773962776e+16\n",
      "q-error 100% percentile is 2.305750121986051e+44\n",
      "=============================================\n",
      "query no 112, 33c with 1927 number of sub-plan queries\n",
      "q-error 50% percentile is 8735.62003058104\n",
      "q-error 90% percentile is 669427843.8075627\n",
      "q-error 95% percentile is 16287125430.178705\n",
      "q-error 99% percentile is 342297683563792.75\n",
      "q-error 100% percentile is 1.585995653738051e+44\n"
     ]
    }
   ],
   "source": [
    "q_file_names_real = []\n",
    "sub_query_len = []\n",
    "for query_no in range(1, 34):\n",
    "    for suffix in ['a', 'b', 'c', 'd', 'e', 'f', 'g']:\n",
    "        file = f\"{query_no}{suffix}.sql\"\n",
    "        if file in os.listdir(query_path):\n",
    "            q_file_names_real.append(file.split(\".sql\")[0])\n",
    "            sub_query_len.append(len(res[file.split(\".sql\")[0]]))\n",
    "            \n",
    "start_idx = 0\n",
    "true_card = dict()\n",
    "all_true_card_new = []\n",
    "for i, q_file in enumerate(q_file_names_real):\n",
    "    print(\"=============================================\")\n",
    "    print(f\"query no {i}, {q_file} with {len(res[q_file])} number of sub-plan queries\")\n",
    "    pred = res[q_file]\n",
    "    pred = np.asarray(pred)\n",
    "    end_idx = start_idx + sub_query_len[i]\n",
    "    true = all_true_card[start_idx: end_idx]\n",
    "    temp_true = copy.deepcopy(true)\n",
    "    temp_true[true == -1] = pred[true == -1]\n",
    "    all_true_card_new.append(temp_true)\n",
    "    true_card[q_file] = true\n",
    "    assert len(pred) == len(true)\n",
    "    pred = pred[true != -1]\n",
    "    pred[pred <= 0] = 1\n",
    "    true = true[true != -1]\n",
    "    q_errors = np.maximum(pred/true, true/pred)\n",
    "    for i in [50, 90, 95, 99, 100]:\n",
    "        print(f\"q-error {i}% percentile is {np.percentile(q_errors, i)}\")\n",
    "    start_idx = end_idx\n",
    "all_true_card_new = np.concatenate(all_true_card_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "99f92b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"CE_scheme_est.txt\", \"w\") as f:\n",
    "    for t in q_file_names_real:\n",
    "        for val in res[t]:\n",
    "            f.write(str(val)+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb09ff83",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"true_est_job.txt\", \"w\") as f:\n",
    "    for i in all_true_card_new:\n",
    "        f.write(str(i)+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523c5232",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_factors_no_filter[\"char_name\"].pdfs['char_name.id'] * ground_truth_factors_no_filter[\"char_name\"].table_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdb39cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056a9aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/Users/ziniuw/Desktop/research/Learned_QO/CC_model/CE_scheme_models/stats/model_200.pkl\"\n",
    "with open(model_path, \"rb\") as f:\n",
    "    new_BE = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de13cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_str = \"SELECT COUNT(*) FROM users as u, comments as c, posts as p WHERE p.OwnerUserId = u.Id AND p.Id = c.PostId AND u.UpVotes>=0 AND u.CreationDate>='2010-08-21 21:27:38'::timestamp AND c.CreationDate>='2010-07-21 11:05:37'::timestamp AND c.CreationDate<='2014-08-25 17:59:25'::timestamp\"\n",
    "t = time.time()\n",
    "res = new_BE.get_cardinality_bound(query57)\n",
    "print(time.time() - t)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ff26f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_file = \"/Users/ziniuw/Desktop/past_research/End-to-End-CardEst-Benchmark/workloads/stats_CEB/sub_plan_queries/stats_CEB_sub_queries.sql\"\n",
    "with open(query_file, \"r\") as f:\n",
    "    queries = f.readlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef76e71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "qerror = []\n",
    "latency = []\n",
    "pred = []\n",
    "for i, query_str in enumerate(queries):\n",
    "    query = query_str.split(\"||\")[0][:-1]\n",
    "    print(\"========================\")\n",
    "    true_card = int(query_str.split(\"||\")[-1])\n",
    "    t = time.time()\n",
    "    res = new_BE.get_cardinality_bound(query)\n",
    "    pred.append(res)\n",
    "    latency.append(time.time() - t)\n",
    "    qerror.append(res/true_card)\n",
    "    print(f\"estimating query {i}: predicted {res}, true_card {true_card}, qerror {res/true_card}, latency {time.time() - t}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e595aff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "qerror = np.asarray(qerror)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e6d0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_qerror = copy.deepcopy(qerror)\n",
    "temp_qerror[temp_qerror < 1] = 1/temp_qerror[temp_qerror < 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73add80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [50, 90, 95, 99, 100]:\n",
    "    print(f\"q-error {i}% percentile is {np.percentile(temp_qerror, i)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f81d047",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"stats_CEB_sub_queries_CE_scheme.txt\", \"w\") as f:\n",
    "    for p in pred:\n",
    "        f.write(str(p)+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301c046f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"stats_CEB_exec.sql\", \"r\") as f:\n",
    "    queries = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e23ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"stats_CEB_exec.sql\", \"w\") as f:\n",
    "    for q in queries:\n",
    "        q = q.split(\"||\")[-1]\n",
    "        f.write(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e38019fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuple([1]) == (1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "57256527",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1,2).index(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0a104929",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.update(dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "69f7824c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d953fdc1",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'replace'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c83140bbefd7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'replace'"
     ]
    }
   ],
   "source": [
    "a = (1,2)\n",
    "a.replace(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18539141",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
