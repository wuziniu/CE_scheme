{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6025f1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "sys.path.append(\"/home/ubuntu/CE_scheme/\")\n",
    "from Schemas.imdb.schema import gen_imdb_schema\n",
    "from Join_scheme.data_prepare import read_table_csv\n",
    "from Join_scheme.binning import identify_key_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "021c79b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 3}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = dict()\n",
    "a[(1)] = 3\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e6f2bce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['asd.sad', 'sdasew']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted((\"asd.sad\", \"sdasew\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c1cf74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "\n",
    "class Bucket:\n",
    "    \"\"\"\n",
    "    The class of bucketization of a key attribute\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name, bins=[], bin_modes=[], bin_vars=[], bin_means=[], rest_bins_remaining=None):\n",
    "        self.name = name\n",
    "        self.bins = bins\n",
    "        self.bin_modes = bin_modes\n",
    "        self.bin_vars = bin_vars\n",
    "        self.bin_means = bin_means\n",
    "        self.rest_bins_remaining = rest_bins_remaining\n",
    "        if len(bins) != 0:\n",
    "            assert len(bins) == len(bin_modes)\n",
    "\n",
    "\n",
    "class Table_bucket:\n",
    "    \"\"\"\n",
    "    The class of bucketization for all key attributes in a table.\n",
    "    Supporting more than three dimensional bin modes requires simplifying the causal structure\n",
    "    \"\"\"\n",
    "    def __init__(self, table_name, id_attributes, bin_sizes, oned_bin_modes=None):\n",
    "        self.table_name = table_name\n",
    "        self.id_attributes = id_attributes\n",
    "        self.bin_sizes = bin_sizes\n",
    "        if oned_bin_modes:\n",
    "            self.oned_bin_modes = oned_bin_modes\n",
    "        else:\n",
    "            self.oned_bin_modes = dict()\n",
    "        self.twod_bin_modes = dict()\n",
    "\n",
    "\n",
    "class Bucket_group:\n",
    "    \"\"\"\n",
    "    The class of bucketization for a group of equivalent join keys\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, buckets, start_key, sample_rate, bins=None, primary_keys=[]):\n",
    "        self.buckets = buckets\n",
    "        self.start_key = start_key\n",
    "        self.sample_rate = sample_rate\n",
    "        self.bins = bins\n",
    "        self.primary_keys = primary_keys\n",
    "\n",
    "    def bucketize(self, data):\n",
    "        \"\"\"\n",
    "        Discretize data based on the bucket\n",
    "        \"\"\"\n",
    "        res = dict()\n",
    "        seen_remain_key = np.array([])\n",
    "        cumulative_bin = copy.deepcopy(self.buckets[self.start_key].bins)\n",
    "        start_means = np.asarray(self.buckets[self.start_key].bin_means)\n",
    "\n",
    "        for key in data:\n",
    "            if key in self.primary_keys:\n",
    "                continue\n",
    "            res[key] = copy.deepcopy(data[key])\n",
    "            if key != self.start_key:\n",
    "                unique_remain = np.setdiff1d(self.buckets[key].rest_bins_remaining, seen_remain_key)\n",
    "                assert sum([np.sum(np.isin(unique_remain, b) == 1) for b in cumulative_bin]) == 0\n",
    "\n",
    "                if len(unique_remain) != 0:\n",
    "                    remaining_data = data[key][np.isin(data[key], unique_remain)]\n",
    "                    unique_remain, count_remain = np.unique(remaining_data, return_counts=True)\n",
    "                    unique_counts = np.unique(count_remain)\n",
    "                    for u in unique_counts:\n",
    "                        temp_idx = np.searchsorted(start_means, u)\n",
    "                        if temp_idx == len(cumulative_bin):\n",
    "                            idx = -1\n",
    "                            if u > self.buckets[key].bin_modes[-1]:\n",
    "                                self.buckets[key].bin_modes[-1] = u\n",
    "                        elif temp_idx == 0:\n",
    "                            idx = 0\n",
    "                        else:\n",
    "                            if (u - start_means[temp_idx - 1]) >= (start_means[temp_idx] - u):\n",
    "                                idx = temp_idx - 1\n",
    "                            else:\n",
    "                                idx = temp_idx\n",
    "                        temp_unique = unique_remain[count_remain == u]\n",
    "                        cumulative_bin[idx] = np.concatenate((cumulative_bin[idx], temp_unique))\n",
    "                        seen_remain_key = np.concatenate((seen_remain_key, temp_unique))\n",
    "                        if u > self.buckets[key].bin_modes[idx]:\n",
    "                            self.buckets[key].bin_modes[idx] = u\n",
    "            res[key] = copy.deepcopy(data[key])\n",
    "            count = 0\n",
    "            for i, b in enumerate(cumulative_bin):\n",
    "                count += len(data[key][np.isin(data[key], b)])\n",
    "                res[key][np.isin(data[key], b)] = i\n",
    "\n",
    "        self.bins = cumulative_bin\n",
    "\n",
    "        for key in data:\n",
    "            if key in self.primary_keys:\n",
    "                res[key] = self.bucketize_PK(data[key])\n",
    "                self.buckets[key] = Bucket(key, bin_modes=np.ones(len(self.bins)))\n",
    "        \n",
    "        for key in data:\n",
    "            if key in self.primary_keys:\n",
    "                continue\n",
    "            if self.sample_rate[key] < 1.0:\n",
    "                bin_modes = np.asarray(self.buckets[key].bin_modes)\n",
    "                bin_modes[bin_modes != 1] = bin_modes[bin_modes != 1] / self.sample_rate[key]\n",
    "                self.buckets[key].bin_modes = bin_modes\n",
    "        \n",
    "        return res\n",
    "\n",
    "    def bucketize_PK(self, data):\n",
    "        res = copy.deepcopy(data)\n",
    "        remaining_data = np.unique(data)\n",
    "        for i, b in enumerate(self.bins):\n",
    "            res[np.isin(data, b)] = i\n",
    "            remaining_data = np.setdiff1d(remaining_data, b)\n",
    "        if len(remaining_data) != 0:\n",
    "            self.bins.append(list(remaining_data))\n",
    "            for key in self.buckets:\n",
    "                if key not in self.primary_keys:\n",
    "                    self.buckets[key].bin_modes = np.append(self.buckets[key].bin_modes, 0)\n",
    "        res[np.isin(data, remaining_data)] = len(self.bins)\n",
    "        return res\n",
    "\n",
    "\n",
    "\n",
    "def identify_key_values(schema):\n",
    "    \"\"\"\n",
    "    identify all the key attributes from the schema of a DB, currently we assume all possible joins are known\n",
    "    It is also easy to support unseen joins, which we left as a future work.\n",
    "    :param schema: the schema of a DB\n",
    "    :return: a dict of all keys, {table: [keys]};\n",
    "             a dict of set, each indicating which keys on different tables are considered the same key.\n",
    "    \"\"\"\n",
    "    all_keys = set()\n",
    "    equivalent_keys = dict()\n",
    "    for i, join in enumerate(schema.relationships):\n",
    "        keys = join.identifier.split(\" = \")\n",
    "        all_keys.add(keys[0])\n",
    "        all_keys.add(keys[1])\n",
    "        seen = False\n",
    "        for k in equivalent_keys:\n",
    "            if keys[0] in equivalent_keys[k]:\n",
    "                equivalent_keys[k].add(keys[1])\n",
    "                seen = True\n",
    "                break\n",
    "            elif keys[1] in equivalent_keys[k]:\n",
    "                equivalent_keys[k].add(keys[0])\n",
    "                seen = True\n",
    "                break\n",
    "        if not seen:\n",
    "            # set the keys[-1] as the identifier of this equivalent join key group for convenience.\n",
    "            equivalent_keys[keys[-1]] = set(keys)\n",
    "\n",
    "    assert len(all_keys) == sum([len(equivalent_keys[k]) for k in equivalent_keys])\n",
    "    return all_keys, equivalent_keys\n",
    "\n",
    "\n",
    "def equal_freq_binning(name, data, n_bins, data_len):\n",
    "    uniques, counts = data\n",
    "    if len(uniques) <= n_bins:\n",
    "        bins = []\n",
    "        bin_modes = []\n",
    "        bin_vars = []\n",
    "        bin_means = []\n",
    "        \n",
    "        for i, uni in enumerate(uniques):\n",
    "            bins.append([uni])\n",
    "            bin_modes.append(counts[i])\n",
    "            bin_vars.append(0)\n",
    "            bin_means.append(counts[i])\n",
    "        return Bucket(name, bins, bin_modes, bin_vars, bin_means)\n",
    "    \n",
    "    unique_counts, count_counts = np.unique(counts, return_counts=True)\n",
    "    idx = np.argsort(unique_counts)\n",
    "    unique_counts = unique_counts[idx]\n",
    "    count_counts = count_counts[idx]\n",
    "\n",
    "    bins = []\n",
    "    bin_modes = []\n",
    "    bin_vars = []\n",
    "    bin_means = []\n",
    "\n",
    "    bin_freq = data_len / n_bins\n",
    "    cur_freq = 0\n",
    "    cur_bin = []\n",
    "    cur_bin_count = []\n",
    "    for i, uni_c in enumerate(unique_counts):\n",
    "        cur_freq += count_counts[i] * uni_c\n",
    "        cur_bin.append(uniques[np.where(counts == uni_c)[0]])\n",
    "        cur_bin_count.extend([uni_c] * count_counts[i])\n",
    "        if (cur_freq > bin_freq) or (i == (len(unique_counts) - 1)):\n",
    "            bins.append(np.concatenate(cur_bin))\n",
    "            cur_bin_count = np.asarray(cur_bin_count)\n",
    "            bin_modes.append(uni_c)\n",
    "            bin_means.append(np.mean(cur_bin_count))\n",
    "            bin_vars.append(np.var(cur_bin_count))\n",
    "            cur_freq = 0\n",
    "            cur_bin = []\n",
    "            cur_bin_count = []\n",
    "    assert len(uniques) == sum([len(b) for b in bins]), f\"some unique values missed or duplicated\"\n",
    "    return Bucket(name, bins, bin_modes, bin_vars, bin_means)\n",
    "\n",
    "\n",
    "def compute_variance_score(buckets):\n",
    "    \"\"\"\n",
    "    compute the variance of products of random variables\n",
    "    \"\"\"\n",
    "    all_mean = np.asarray([buckets[k].bin_means for k in buckets])\n",
    "    all_var = np.asarray([buckets[k].bin_vars for k in buckets])\n",
    "    return np.sum(np.prod(all_var + all_mean ** 2, axis=0) - np.prod(all_mean, axis=0) ** 2)\n",
    "\n",
    "\n",
    "def sub_optimal_bucketize(data, sample_rate, n_bins=30, primary_keys=[]):\n",
    "    \"\"\"\n",
    "    Perform sub-optimal bucketization on a group of equivalent join keys.\n",
    "    :param data: a dict of (potentially sampled) table data of the keys\n",
    "                 the keys of this dict are one group of equivalent join keys\n",
    "    :param sample_rate: the sampling rate the data, could be all 1 if no sampling is performed\n",
    "    :param n_bins: how many bins can we allocate\n",
    "    :param primary_keys: the primary keys in the equivalent group since we don't need to bucketize PK.\n",
    "    :return: new data, where the keys are bucketized\n",
    "             the mode of each bucket\n",
    "    \"\"\"\n",
    "    unique_values = dict()\n",
    "    for key in data:\n",
    "        if key not in primary_keys:\n",
    "            unique_values[key] = np.unique(data[key], return_counts=True)\n",
    "\n",
    "    best_variance_score = np.infty\n",
    "    best_bin_len = 0\n",
    "    best_start_key = None\n",
    "    best_buckets = None\n",
    "    for start_key in data:\n",
    "        if start_key in primary_keys:\n",
    "            continue\n",
    "        start_bucket = equal_freq_binning(start_key, unique_values[start_key], n_bins, len(data[start_key]))\n",
    "        rest_buckets = dict()\n",
    "        for key in data:\n",
    "            if key == start_key or key in primary_keys:\n",
    "                continue\n",
    "            uniques = unique_values[key][0]\n",
    "            counts = unique_values[key][1]\n",
    "            rest_buckets[key] = Bucket(key, [], [0] * len(start_bucket.bins), [0] * len(start_bucket.bins),\n",
    "                                       [0] * len(start_bucket.bins), uniques)\n",
    "            for i, bin in enumerate(start_bucket.bins):\n",
    "                idx = np.where(np.isin(uniques, bin) == 1)[0]\n",
    "                if len(idx) != 0:\n",
    "                    bin_count = counts[idx]\n",
    "                    unique_bin_keys = uniques[idx]\n",
    "                    # unique_bin_count = np.unique(bin_count)\n",
    "                    # bin_count = np.concatenate([counts[counts == j] for j in unique_bin_count])\n",
    "                    # unique_bin_keys = np.concatenate([uniques[counts == j] for j in unique_bin_count])\n",
    "                    rest_buckets[key].rest_bins_remaining = np.setdiff1d(rest_buckets[key].rest_bins_remaining,\n",
    "                                                                         unique_bin_keys)\n",
    "                    rest_buckets[key].bin_modes[i] = np.max(bin_count)\n",
    "                    rest_buckets[key].bin_vars[i] = np.var(bin_count)\n",
    "                    rest_buckets[key].bin_means[i] = np.mean(bin_count)\n",
    "\n",
    "        rest_buckets[start_key] = start_bucket\n",
    "        var_score = compute_variance_score(rest_buckets)\n",
    "        if len(start_bucket.bins) > best_bin_len:\n",
    "            best_variance_score = var_score\n",
    "            best_start_key = start_key\n",
    "            best_buckets = rest_buckets\n",
    "            best_bin_len = len(start_bucket.bins)\n",
    "        elif len(start_bucket.bins) >= best_bin_len * 0.9 and var_score < best_variance_score:\n",
    "            best_variance_score = var_score\n",
    "            best_start_key = start_key\n",
    "            best_buckets = rest_buckets\n",
    "            best_bin_len = len(start_bucket.bins)\n",
    "    \n",
    "    best_buckets = Bucket_group(best_buckets, best_start_key, sample_rate, primary_keys=primary_keys)\n",
    "    new_data = best_buckets.bucketize(data)\n",
    "    return new_data, best_buckets\n",
    "\n",
    "\n",
    "def fixed_start_key_bucketize(start_key, data, sample_rate, n_bins=30, primary_keys=[]):\n",
    "    \"\"\"\n",
    "    Perform sub-optimal bucketization on a group of equivalent join keys based on the pre-defined start_key.\n",
    "    :param data: a dict of (potentially sampled) table data of the keys\n",
    "                 the keys of this dict are one group of equivalent join keys\n",
    "    :param sample_rate: the sampling rate the data, could be all 1 if no sampling is performed\n",
    "    :param n_bins: how many bins can we allocate\n",
    "    :param primary_keys: the primary keys in the equivalent group since we don't need to bucketize PK.\n",
    "    :return: new data, where the keys are bucketized\n",
    "             the mode of each bucket\n",
    "    \"\"\"\n",
    "    unique_values = dict()\n",
    "    for key in data:\n",
    "        if key not in primary_keys:\n",
    "            unique_values[key] = np.unique(data[key], return_counts=True)\n",
    "\n",
    "    start_bucket = equal_freq_binning(start_key, unique_values[start_key], n_bins, len(data[start_key]))\n",
    "    rest_buckets = dict()\n",
    "    for key in data:\n",
    "        if key == start_key or key in primary_keys:\n",
    "            continue\n",
    "        uniques = unique_values[key][0]\n",
    "        counts = unique_values[key][1]\n",
    "        rest_buckets[key] = Bucket(key, [], [0] * len(start_bucket.bins), [0] * len(start_bucket.bins),\n",
    "                                   [0] * len(start_bucket.bins), uniques)\n",
    "        for i, bin in enumerate(start_bucket.bins):\n",
    "            idx = np.where(np.isin(uniques, bin) == 1)[0]\n",
    "            if len(idx) != 0:\n",
    "                bin_count = counts[idx]\n",
    "                unique_bin_keys = uniques[idx]\n",
    "                rest_buckets[key].rest_bins_remaining = np.setdiff1d(rest_buckets[key].rest_bins_remaining,\n",
    "                                                                     unique_bin_keys)\n",
    "                rest_buckets[key].bin_modes[i] = np.max(bin_count)\n",
    "                rest_buckets[key].bin_means[i] = np.mean(bin_count)\n",
    "\n",
    "    best_buckets = Bucket_group(rest_buckets, start_key, sample_rate, primary_keys=primary_keys)\n",
    "    new_data = best_buckets.bucketize(data)\n",
    "    return new_data, best_buckets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d81e2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/home/ubuntu/data_CE/{}.csv\"\n",
    "schema = gen_imdb_schema(data_path)\n",
    "all_keys, equivalent_keys = identify_key_values(schema)\n",
    "with open(\"new_table_buckets.pkl\", \"rb\") as f:\n",
    "    table_buckets = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fabd33ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/ubuntu/data_CE/job/job_sub_plan_queries.txt\", \"r\") as f:\n",
    "    sub_plan_queries = f.read()\n",
    "psql_raw = sub_plan_queries.split(\"query: 0\")[1:]\n",
    "queries = []\n",
    "q_file_names = []\n",
    "query_path = \"/home/ubuntu/data_CE/job/\"\n",
    "for file in os.listdir(query_path):\n",
    "    if file.endswith(\".sql\") and file[0].isnumeric():\n",
    "        q_file_names.append(file.split(\".sql\")[0]+\".pkl\")\n",
    "        with open(query_path+file, \"r\") as f:\n",
    "            q = f.readline()\n",
    "            queries.append(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c46b7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "psql_raw = sub_plan_queries.split(\"query: 0\")[1:]\n",
    "sub_plan_queries_str_all = []\n",
    "for per_query in psql_raw:\n",
    "    sub_plan_queries = []\n",
    "    sub_plan_queries_str = []\n",
    "    num_sub_plan_queries = len(per_query.split(\"query: \"))\n",
    "    all_info = per_query.split(\"RELOPTINFO (\")[1:]\n",
    "    assert num_sub_plan_queries*2 == len(all_info)\n",
    "    for i in range(num_sub_plan_queries):\n",
    "        idx = i*2\n",
    "        table1 = all_info[idx].split(\"): rows=\")[0]\n",
    "        table2 = all_info[idx+1].split(\"): rows=\")[0]\n",
    "        table_str = (table1, table2)\n",
    "        sub_plan_queries_str.append(table_str)\n",
    "    sub_plan_queries_str_all.append(sub_plan_queries_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a870ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "from Join_scheme.join_graph import get_join_hyper_graph, parse_query_all_join\n",
    "from Join_scheme.data_prepare import identify_key_values\n",
    "#from Sampling.load_sample import load_sample_imdb_one_query\n",
    "\n",
    "\n",
    "class Factor:\n",
    "    \"\"\"\n",
    "    This the class defines a multidimensional conditional probability on one table.\n",
    "    \"\"\"\n",
    "    def __init__(self, table, table_len, variables, pdfs, na_values=None):\n",
    "        self.table = table\n",
    "        self.table_len = table_len\n",
    "        self.variables = variables\n",
    "        self.pdfs = pdfs\n",
    "        self.na_values = na_values  # this is the percentage of data, which is not nan.\n",
    "\n",
    "\n",
    "class Group_Factor:\n",
    "    \"\"\"\n",
    "        This the class defines a multidimensional conditional probability on a group of tables.\n",
    "    \"\"\"\n",
    "    def __init__(self, tables, tables_size, pdfs, bin_modes, equivalent_groups=None, table_key_equivalent_group=None,\n",
    "                 na_values=None, join_cond=None):\n",
    "        self.table = tables\n",
    "        self.tables_size = tables_size\n",
    "        self.pdfs = pdfs\n",
    "        self.bin_modes = bin_modes\n",
    "        self.equivalent_groups = equivalent_groups\n",
    "        self.table_key_equivalent_group = table_key_equivalent_group\n",
    "        self.na_values = na_values\n",
    "        self.join_cond = join_cond\n",
    "\n",
    "\n",
    "class Bound_ensemble:\n",
    "    \"\"\"\n",
    "    This the class where we store all the trained models and perform inference on the bound.\n",
    "    \"\"\"\n",
    "    def __init__(self, table_buckets, schema, ground_truth_factors_no_filter=None):\n",
    "        self.table_buckets = table_buckets\n",
    "        self.schema = schema\n",
    "        self.all_keys, self.equivalent_keys = identify_key_values(schema)\n",
    "        self.all_join_conds = None\n",
    "        self.ground_truth_factors_no_filter = ground_truth_factors_no_filter\n",
    "        #self.reverse_table_alias = None\n",
    "\n",
    "    def parse_query_simple(self, query):\n",
    "        \"\"\"\n",
    "        If your selection query contains no aggregation and nested sub-queries, you can use this function to parse a\n",
    "        join query. Otherwise, use parse_query function.\n",
    "        \"\"\"\n",
    "        tables_all, join_cond, join_keys = parse_query_all_join(query)\n",
    "        #TODO: implement functions on parsing filter conditions.\n",
    "        table_filters = dict()\n",
    "        return tables_all, table_filters, join_cond, join_keys\n",
    "\n",
    "    def get_all_id_conidtional_distribution(self, query_file_name, tables_alias, join_keys):\n",
    "        #TODO: make it work on query-driven and sampling based\n",
    "        return load_sample_imdb_one_query(self.table_buckets, tables_alias, query_file_name, join_keys,\n",
    "                                          self.ground_truth_factors_no_filter)\n",
    "\n",
    "\n",
    "    def eliminate_one_key_group(self, tables, key_group, factors, relevant_keys):\n",
    "        \"\"\"This version only supports 2D distributions (i.e. the distribution learned with tree-structured PGM)\"\"\"\n",
    "        rest_group = None\n",
    "        rest_group_cardinalty = 0\n",
    "        eliminated_tables = []\n",
    "        rest_group_tables = []\n",
    "        for table in tables:\n",
    "            assert key_group in factors[table].equivalent_variables\n",
    "            temp = copy.deepcopy(factors[table].equivalent_variables)\n",
    "            temp.remove(key_group)\n",
    "            if len(temp) == 0:\n",
    "                eliminated_tables.append(table)\n",
    "            for key in temp:\n",
    "                if rest_group:\n",
    "                    assert factors[table].cardinalities[key] == rest_group_cardinalty\n",
    "                    rest_group_tables.append(table)\n",
    "                else:\n",
    "                    rest_group = key\n",
    "                    rest_group_cardinalty = factors[table].cardinalities[key]\n",
    "                    rest_group_tables = [table]\n",
    "\n",
    "        all_probs_eliminated = []\n",
    "        all_modes_eliminated = []\n",
    "        for table in eliminated_tables:\n",
    "            bin_modes = self.table_buckets[table].oned_bin_modes[relevant_keys[key_group][table]]\n",
    "            all_probs_eliminated.append(factors[table].pdfs)\n",
    "            all_modes_eliminated.append(np.minimum(bin_modes, factors[table].pdfs))\n",
    "\n",
    "        if rest_group:\n",
    "            new_factor_pdf = np.zeros(rest_group_cardinalty)\n",
    "        else:\n",
    "            return self.compute_bound_oned(all_probs_eliminated, all_modes_eliminated)\n",
    "\n",
    "        for i in range(rest_group_cardinalty):\n",
    "            for table in rest_group_tables:\n",
    "                idx_f = factors[table].equivalent_variables.index(key_group)\n",
    "                idx_b = self.table_buckets[table].id_attributes.index(relevant_keys[key_group][table])\n",
    "                bin_modes = self.table_buckets[table].twod_bin_modes[relevant_keys[key_group][table]]\n",
    "                if idx_f == 0 and idx_b == 0:\n",
    "                    all_probs_eliminated.append(factors[table].pdfs[:, i])\n",
    "                    all_modes_eliminated.append(np.minimum(bin_modes[:, i], factors[table].pdfs[:, i]))\n",
    "                elif idx_f == 0 and idx_b == 1:\n",
    "                    all_probs_eliminated.append(factors[table].pdfs[:, i])\n",
    "                    all_modes_eliminated.append(np.minimum(bin_modes[i, :], factors[table].pdfs[:, i]))\n",
    "                elif idx_f == 1 and idx_b == 0:\n",
    "                    all_probs_eliminated.append(factors[table].pdfs[i, :])\n",
    "                    all_modes_eliminated.append(np.minimum(bin_modes[:, i], factors[table].pdfs[i, :]))\n",
    "                else:\n",
    "                    all_probs_eliminated.append(factors[table].pdfs[i, :])\n",
    "                    all_modes_eliminated.append(np.minimum(bin_modes[i, :], factors[table].pdfs[i, :]))\n",
    "            new_factor_pdf[i] = self.compute_bound_oned(all_probs_eliminated, all_modes_eliminated)\n",
    "\n",
    "        for table in rest_group_tables:\n",
    "            factors[table] = Factor([rest_group], new_factor_pdf, [rest_group])\n",
    "\n",
    "        return None\n",
    "\n",
    "    def compute_bound_oned(self, all_probs, all_modes, return_factor=False):\n",
    "        temp_all_modes = []\n",
    "        for i in range(len(all_modes)):\n",
    "            temp_all_modes.append(np.minimum(all_probs[i], all_modes[i]))\n",
    "        all_probs = np.stack(all_probs, axis=0)\n",
    "        temp_all_modes = np.stack(temp_all_modes, axis=0)\n",
    "        multiplier = np.prod(temp_all_modes, axis=0)\n",
    "        non_zero_idx = np.where(multiplier != 0)[0]\n",
    "        min_number = np.amin(all_probs[:, non_zero_idx]/temp_all_modes[:, non_zero_idx], axis=0)\n",
    "        #print(min_number, multiplier[non_zero_idx])\n",
    "        if return_factor:\n",
    "            new_probs = np.zeros(multiplier.shape)\n",
    "            new_probs[non_zero_idx] = multiplier[non_zero_idx] * min_number\n",
    "            return new_probs, multiplier\n",
    "        else:\n",
    "            multiplier[non_zero_idx] = multiplier[non_zero_idx] * min_number\n",
    "            return np.sum(multiplier)\n",
    "\n",
    "    def get_optimal_elimination_order(self, equivalent_group, join_keys, factors):\n",
    "        \"\"\"\n",
    "        This function determines the optimial elimination order for each key group\n",
    "        \"\"\"\n",
    "        cardinalities = dict()\n",
    "        lengths = dict()\n",
    "        tables_involved = dict()\n",
    "        relevant_keys = dict()\n",
    "        for group in equivalent_group:\n",
    "            relevant_keys[group] = dict()\n",
    "            lengths[group] = len(equivalent_group[group])\n",
    "            cardinalities[group] = []\n",
    "            tables_involved[group] = set([])\n",
    "            for keys in equivalent_group[group]:\n",
    "                for table in join_keys:\n",
    "                    if keys in join_keys[table]:\n",
    "                        cardinalities[group].append(len(join_keys[table]))\n",
    "                        tables_involved[group].add(table)\n",
    "                        variables = factors[table].variables\n",
    "                        variables[variables.index(keys)] = group\n",
    "                        factors[table].variables = variables\n",
    "                        relevant_keys[group][table] = keys\n",
    "                        break\n",
    "            cardinalities[group] = np.asarray(cardinalities[group])\n",
    "\n",
    "        optimal_order = list(equivalent_group.keys())\n",
    "        for i in range(len(optimal_order)):\n",
    "            min_idx = i\n",
    "            for j in range(i+1, len(optimal_order)):\n",
    "                min_group = optimal_order[min_idx]\n",
    "                curr_group = optimal_order[j]\n",
    "                if np.max(cardinalities[curr_group]) < np.max(cardinalities[min_group]):\n",
    "                    min_idx = j\n",
    "                else:\n",
    "                    min_max_tables = np.max(cardinalities[min_group])\n",
    "                    min_num_max_tables = len(np.where(cardinalities[min_group] == min_max_tables)[0])\n",
    "                    curr_max_tables = np.max(cardinalities[curr_group])\n",
    "                    curr_num_max_tables = len(np.where(cardinalities[curr_group] == curr_max_tables)[0])\n",
    "                    if curr_num_max_tables < min_num_max_tables:\n",
    "                        min_idx = j\n",
    "                    elif lengths[curr_group] < lengths[min_group]:\n",
    "                        min_idx = j\n",
    "            optimal_order[i], optimal_order[min_idx] = optimal_order[min_idx], optimal_order[i]\n",
    "        return optimal_order, tables_involved, relevant_keys\n",
    "\n",
    "    def get_cardinality_bound_one(self, query_str):\n",
    "        tables_all, table_queries, join_cond, join_keys = self.parse_query_simple(query_str)\n",
    "        equivalent_group = get_join_hyper_graph(join_keys, self.equivalent_keys)\n",
    "        conditional_factors = self.get_all_id_conidtional_distribution(table_queries, join_keys, equivalent_group)\n",
    "        optimal_order, tables_involved, relevant_keys = self.get_optimal_elimination_order(equivalent_group, join_keys,\n",
    "                                                                            conditional_factors)\n",
    "\n",
    "        for key_group in optimal_order:\n",
    "            tables = tables_involved[key_group]\n",
    "            res = self.eliminate_one_key_group(tables, key_group, conditional_factors, relevant_keys)\n",
    "        return res\n",
    "\n",
    "    def get_sub_plan_queries_sql(self, query_str, sub_plan_query_str_all, query_name=None):\n",
    "        tables_all, table_queries, join_cond, join_keys = self.parse_query_simple(query_str)\n",
    "        equivalent_group, table_equivalent_group, table_key_equivalent_group = get_join_hyper_graph(join_keys,\n",
    "                                                                                            self.equivalent_keys)\n",
    "        cached_sub_queries_sql = dict()\n",
    "        cached_union_key_group = dict()\n",
    "        res_sql = []\n",
    "        for (left_tables, right_tables) in sub_plan_query_str_all:\n",
    "            assert \" \" not in left_tables, f\"{left_tables} contains more than one tables, violating left deep plan\"\n",
    "            sub_plan_query_list = right_tables.split(\" \") + [left_tables]\n",
    "            sub_plan_query_list.sort()\n",
    "            sub_plan_query_str = \" \".join(sub_plan_query_list)  # get the string name of the sub plan query\n",
    "            sql_header = \"SELECT COUNT(*) FROM \"\n",
    "            for alias in sub_plan_query_list:\n",
    "                sql_header += (tables_all[alias] + \" AS \" + alias + \", \")\n",
    "            sql_header = sql_header[:-2] + \" WHERE \"\n",
    "            if \" \" in right_tables:\n",
    "                assert right_tables in cached_sub_queries_sql, f\"{right_tables} not in cache, input is not ordered\"\n",
    "                right_sql = cached_sub_queries_sql[right_tables]\n",
    "                right_union_key_group = cached_union_key_group[right_tables]\n",
    "                if left_tables in table_queries:\n",
    "                    left_sql = table_queries[left_tables]\n",
    "                    curr_sql = right_sql + \" AND (\" + left_sql + \")\"\n",
    "                else:\n",
    "                    curr_sql = right_sql\n",
    "                additional_joins, union_key_group = self.get_additional_join_with_table_group(left_tables,\n",
    "                                                                        right_union_key_group,\n",
    "                                                                        table_equivalent_group,\n",
    "                                                                        table_key_equivalent_group)\n",
    "                for join in additional_joins:\n",
    "                    curr_sql = curr_sql + \" AND \" + join\n",
    "            else:\n",
    "                curr_sql = \"\"\n",
    "                if left_tables in table_queries:\n",
    "                    curr_sql += (\"(\" + table_queries[left_tables] + \")\")\n",
    "                if right_tables in table_queries:\n",
    "                    if curr_sql != \"\":\n",
    "                        curr_sql += \" AND \"\n",
    "                    curr_sql += (\"(\" + table_queries[right_tables] + \")\")\n",
    "\n",
    "                additional_joins, union_key_group = self.get_additional_joins_two_tables(left_tables, right_tables,\n",
    "                                                                        table_equivalent_group,\n",
    "                                                                        table_key_equivalent_group)\n",
    "                for join in additional_joins:\n",
    "                    if curr_sql == \"\":\n",
    "                        curr_sql += join\n",
    "                    else:\n",
    "                        curr_sql = curr_sql + \" AND \" + join\n",
    "            cached_sub_queries_sql[sub_plan_query_str] = curr_sql\n",
    "            cached_union_key_group[sub_plan_query_str] = union_key_group\n",
    "            res_sql.append(sql_header + curr_sql + \";\")\n",
    "        return res_sql\n",
    "\n",
    "    def get_cardinality_bound_all(self, query_str, sub_plan_query_str_all, query_name=None, debug=False, true_card=None):\n",
    "        \"\"\"\n",
    "        Get the cardinality bounds for all sub_plan_queires of a query.\n",
    "        Note: Due to efficiency, this current version only support left_deep plans (like the one generated by postgres),\n",
    "              but it can easily support right deep or bushy plans.\n",
    "        :param query_str: the target query\n",
    "        :param sub_plan_query_str_all: all sub_plan_queries of the target query,\n",
    "               it should be sorted by number of the tables in the sub_plan_query\n",
    "        \"\"\"\n",
    "        tables_all, table_queries, join_cond, join_keys = self.parse_query_simple(query_str)\n",
    "        #print(join_cond)\n",
    "        #print(join_keys)\n",
    "        equivalent_group, table_equivalent_group, table_key_equivalent_group, table_key_group_map = \\\n",
    "            get_join_hyper_graph(join_keys, self.equivalent_keys)\n",
    "        conditional_factors = self.get_all_id_conidtional_distribution(query_name, tables_all, join_keys)\n",
    "        #self.reverse_table_alias = {v: k for k, v in tables_all.items()}\n",
    "        cached_sub_queries = dict()\n",
    "        cardinality_bounds = []\n",
    "        for i, (left_tables, right_tables) in enumerate(sub_plan_query_str_all):\n",
    "            assert \" \" not in left_tables, f\"{left_tables} contains more than one tables, violating left deep plan\"\n",
    "            sub_plan_query_list = right_tables.split(\" \") + [left_tables]\n",
    "            sub_plan_query_list.sort()\n",
    "            sub_plan_query_str = \" \".join(sub_plan_query_list)  #get the string name of the sub plan query\n",
    "            #print(sub_plan_query_str)\n",
    "            if \" \" in right_tables:\n",
    "                assert right_tables in cached_sub_queries, f\"{right_tables} not in cache, input is not ordered\"\n",
    "                right_bound_factor = cached_sub_queries[right_tables]\n",
    "                curr_bound_factor, res = self.join_with_one_table(sub_plan_query_str,\n",
    "                                                                  left_tables,\n",
    "                                                                  tables_all,\n",
    "                                                                  right_bound_factor,\n",
    "                                                                  conditional_factors[left_tables],\n",
    "                                                                  table_equivalent_group,\n",
    "                                                                  table_key_equivalent_group,\n",
    "                                                                  table_key_group_map,\n",
    "                                                                  join_cond)\n",
    "            else:\n",
    "                curr_bound_factor, res = self.join_two_tables(sub_plan_query_str,\n",
    "                                                              left_tables,\n",
    "                                                              right_tables,\n",
    "                                                              tables_all,\n",
    "                                                              conditional_factors,\n",
    "                                                              join_keys,\n",
    "                                                              table_equivalent_group,\n",
    "                                                              table_key_equivalent_group,\n",
    "                                                              table_key_group_map,\n",
    "                                                              join_cond)\n",
    "            cached_sub_queries[sub_plan_query_str] = curr_bound_factor\n",
    "            res = max(res, 1)\n",
    "            if debug:\n",
    "                if true_card[i] == -1:\n",
    "                    error = \"NA\"\n",
    "                else:\n",
    "                    error = max(res/true_card[i], true_card[i]/res)\n",
    "                print(f\"{left_tables}, {right_tables}|| estimate: {res}, true: {true_card[i]}, error: {error}\")\n",
    "            cardinality_bounds.append(res)\n",
    "        return cardinality_bounds\n",
    "\n",
    "\n",
    "    def join_with_one_table(self, sub_plan_query_str, left_table, tables_all, right_bound_factor, cond_factor_left,\n",
    "                            table_equivalent_group, table_key_equivalent_group, table_key_group_map, join_cond):\n",
    "        \"\"\"\n",
    "        Get the cardinality bound by joining the left_table with the seen right_tables\n",
    "        :param left_table:\n",
    "        :param right_tables:\n",
    "        \"\"\"\n",
    "        equivalent_key_group, union_key_group_set, union_key_group, new_join_cond = \\\n",
    "            self.get_join_keys_with_table_group(left_table, right_bound_factor, tables_all, table_equivalent_group,\n",
    "                                                table_key_equivalent_group, table_key_group_map, join_cond)\n",
    "        bin_mode_left = self.table_buckets[tables_all[left_table]].oned_bin_modes\n",
    "        bin_mode_right = right_bound_factor.bin_modes\n",
    "        key_group_pdf = dict()\n",
    "        key_group_bin_mode = dict()\n",
    "        new_union_key_group = dict()\n",
    "        new_na_values = dict()\n",
    "        res = right_bound_factor.tables_size\n",
    "        print(\"==================================================\")\n",
    "        for key_group in equivalent_key_group:\n",
    "            print(key_group, equivalent_key_group[key_group], res)\n",
    "            #print(cond_factor_left.na_values)\n",
    "            #print(right_bound_factor.na_values)\n",
    "            all_pdfs = [cond_factor_left.pdfs[key] * cond_factor_left.table_len * cond_factor_left.na_values[key]\n",
    "                        for key in equivalent_key_group[key_group][\"left\"]] + \\\n",
    "                       [right_bound_factor.pdfs[key] * res * right_bound_factor.na_values[key]\n",
    "                        for key in equivalent_key_group[key_group][\"right\"]]\n",
    "            all_bin_modes = [bin_mode_left[key] for key in equivalent_key_group[key_group][\"left\"]] + \\\n",
    "                            [bin_mode_right[key] for key in equivalent_key_group[key_group][\"right\"]]\n",
    "            if key_group == \"info_type.id\":\n",
    "                print(all_pdfs)\n",
    "                print(\"****************************************************\")\n",
    "                print(all_bin_modes)\n",
    "            new_pdf, new_bin_mode = self.compute_bound_oned(all_pdfs, all_bin_modes, return_factor=True)\n",
    "            res = np.sum(new_pdf)\n",
    "            if res == 0:\n",
    "                res = 10.0\n",
    "                new_pdf[-1] = 1\n",
    "                key_group_pdf[key_group] = new_pdf\n",
    "            else:\n",
    "                key_group_pdf[key_group] = new_pdf / res\n",
    "            key_group_bin_mode[key_group] = new_bin_mode\n",
    "            new_union_key_group[key_group] = [key_group]\n",
    "            new_na_values[key_group] = 1\n",
    "\n",
    "        for group in union_key_group:\n",
    "            table, keys = union_key_group[group]\n",
    "            new_union_key_group[group] = keys\n",
    "            for key in keys:\n",
    "                if table == \"left\":\n",
    "                    key_group_pdf[key] = cond_factor_left.pdfs[key]\n",
    "                    key_group_bin_mode[key] = self.table_buckets[tables_all[left_table]].oned_bin_modes[key]\n",
    "                    new_na_values[key] = cond_factor_left.na_values[key]\n",
    "                else:\n",
    "                    key_group_pdf[key] = right_bound_factor.pdfs[key]\n",
    "                    key_group_bin_mode[key] = right_bound_factor.bin_modes[key]\n",
    "                    new_na_values[key] = right_bound_factor.na_values[key]\n",
    "\n",
    "        new_factor = Group_Factor(sub_plan_query_str, res, key_group_pdf, key_group_bin_mode,\n",
    "                                  union_key_group_set, new_union_key_group, new_na_values)\n",
    "        return new_factor, res\n",
    "\n",
    "    def get_join_keys_with_table_group(self, left_table, right_bound_factor, tables_all, table_equivalent_group,\n",
    "                                       table_key_equivalent_group, table_key_group_map, join_cond):\n",
    "        \"\"\"\n",
    "            Get the join keys between two tables\n",
    "        \"\"\"\n",
    "\n",
    "        actual_join_cond = []\n",
    "        for cond in join_cond[left_table]:\n",
    "            if cond in right_bound_factor.join_cond:\n",
    "                actual_join_cond.append(cond)\n",
    "        equivalent_key_group = dict()\n",
    "        union_key_group_set = table_equivalent_group[left_table].union(right_bound_factor.equivalent_groups)\n",
    "        union_key_group = dict()\n",
    "        new_join_cond = right_bound_factor.join_cond.union(join_cond[left_table])\n",
    "        if len(actual_join_cond) != 0:\n",
    "            for cond in actual_join_cond:\n",
    "                key1 = cond.split(\"=\")[0].strip()\n",
    "                key2 = cond.split(\"=\")[1].strip()\n",
    "                if key1.split(\".\")[0] == left_table:\n",
    "                    key_left = key1.replace(left_table, tables_all[left_table])\n",
    "                    key_group = table_key_group_map[left_table][key_left]\n",
    "                    if key_group not in equivalent_key_group:\n",
    "                        equivalent_key_group[key_group] = dict()\n",
    "                    if left_table in equivalent_key_group[key_group]:\n",
    "                        equivalent_key_group[key_group][\"left\"].append(key_left)\n",
    "                    else:\n",
    "                        equivalent_key_group[key_group][\"left\"] = [key_left]\n",
    "                    right_table = key2.split(\".\")[0]\n",
    "                    key_right = key2.replace(right_table, tables_all[right_table])\n",
    "                    key_group_t = table_key_group_map[right_table][key_right]\n",
    "                    assert key_group_t == key_group, f\"key group mismatch for join {cond}\"\n",
    "                    if \"right\" in equivalent_key_group[key_group]:\n",
    "                        equivalent_key_group[key_group][\"right\"].append(key_right)\n",
    "                    else:\n",
    "                        equivalent_key_group[key_group][\"right\"] = [key_right]\n",
    "                else:\n",
    "                    assert key2.split(\".\")[0] == left_table, f\"unrecognized table alias\"\n",
    "                    key_left = key2.replace(left_table, tables_all[left_table])\n",
    "                    key_group = table_key_group_map[left_table][key_left]\n",
    "                    if key_group not in equivalent_key_group:\n",
    "                        equivalent_key_group[key_group] = dict()\n",
    "                    if left_table in equivalent_key_group[key_group]:\n",
    "                        equivalent_key_group[key_group][\"left\"].append(key_left)\n",
    "                    else:\n",
    "                        equivalent_key_group[key_group][\"left\"] = [key_left]\n",
    "                    right_table = key1.split(\".\")[0]\n",
    "                    key_right = key1.replace(right_table, tables_all[right_table])\n",
    "                    key_group_t = table_key_group_map[right_table][key_right]\n",
    "                    assert key_group_t == key_group, f\"key group mismatch for join {cond}\"\n",
    "                    if \"right\" in equivalent_key_group[key_group]:\n",
    "                        equivalent_key_group[key_group][\"right\"].append(key_right)\n",
    "                    else:\n",
    "                        equivalent_key_group[key_group][\"right\"] = [key_right]\n",
    "\n",
    "            for group in union_key_group_set:\n",
    "                if group in equivalent_key_group:\n",
    "                    continue\n",
    "                elif group in table_key_equivalent_group[left_table]:\n",
    "                    union_key_group[group] = (\"left\", table_key_equivalent_group[left_table][group])\n",
    "                else:\n",
    "                    union_key_group[group] = (\"right\", right_bound_factor.table_key_equivalent_group[group])\n",
    "\n",
    "        else:\n",
    "            common_key_group = table_equivalent_group[left_table].intersection(right_bound_factor.equivalent_groups)\n",
    "            common_key_group = list(common_key_group)[0]\n",
    "            for group in union_key_group_set:\n",
    "                if group == common_key_group:\n",
    "                    equivalent_key_group[group] = dict()\n",
    "                    equivalent_key_group[group][\"left\"] = table_key_equivalent_group[left_table][group]\n",
    "                    equivalent_key_group[group][\"right\"] = right_bound_factor.table_key_equivalent_group[group]\n",
    "                elif group in table_key_equivalent_group[left_table]:\n",
    "                    union_key_group[group] = (\"left\", table_key_equivalent_group[left_table][group])\n",
    "                else:\n",
    "                    union_key_group[group] = (\"right\", right_bound_factor.table_key_equivalent_group[group])\n",
    "\n",
    "        return equivalent_key_group, union_key_group_set, union_key_group, new_join_cond\n",
    "\n",
    "    def get_additional_join_with_table_group(self, left_table, right_union_key_group, table_equivalent_group,\n",
    "                                             table_key_equivalent_group):\n",
    "        common_key_group = table_equivalent_group[left_table].intersection(set(right_union_key_group.keys()))\n",
    "        union_key_group_set = table_equivalent_group[left_table].union(set(right_union_key_group.keys()))\n",
    "        union_key_group = copy.deepcopy(right_union_key_group)\n",
    "        all_join_predicates = []\n",
    "        for group in union_key_group_set:\n",
    "            if group in common_key_group:\n",
    "                left_key = table_key_equivalent_group[left_table][group][0]\n",
    "                left_key = left_table + \".\" + left_key.split(\".\")[-1]\n",
    "                right_key = right_union_key_group[group]\n",
    "                join_predicate = left_key + \" = \" + right_key\n",
    "                all_join_predicates.append(join_predicate)\n",
    "            if group not in union_key_group:\n",
    "                assert group in table_key_equivalent_group[left_table]\n",
    "                left_key = table_key_equivalent_group[left_table][group][0]\n",
    "                left_key = left_table + \".\" + left_key.split(\".\")[-1]\n",
    "                union_key_group[group] = left_key\n",
    "\n",
    "        return all_join_predicates, union_key_group\n",
    "\n",
    "    def join_two_tables(self, sub_plan_query_str, left_table, right_table, tables_all, conditional_factors, join_keys,\n",
    "                        table_equivalent_group, table_key_equivalent_group, table_key_group_map, join_cond):\n",
    "        \"\"\"\n",
    "            Get the cardinality bound by joining the left_table with the right_table\n",
    "            :param left_table:\n",
    "            :param right_table:\n",
    "        \"\"\"\n",
    "        equivalent_key_group, union_key_group_set, union_key_group, new_join_cond = \\\n",
    "            self.get_join_keys_two_tables(left_table, right_table, table_equivalent_group, table_key_equivalent_group,\n",
    "                                          table_key_group_map, join_cond, join_keys, tables_all)\n",
    "        #print(left_table, right_table)\n",
    "        #print(equivalent_key_group)\n",
    "        #print(union_key_group)\n",
    "        #print(conditional_factors.keys())\n",
    "        cond_factor_left = conditional_factors[left_table]\n",
    "        cond_factor_right = conditional_factors[right_table]\n",
    "        bin_mode_left = self.table_buckets[tables_all[left_table]].oned_bin_modes\n",
    "        bin_mode_right = self.table_buckets[tables_all[right_table]].oned_bin_modes\n",
    "        key_group_pdf = dict()\n",
    "        key_group_bin_mode = dict()\n",
    "        new_union_key_group = dict()\n",
    "        res = cond_factor_right.table_len\n",
    "        new_na_values = dict()\n",
    "        #print(equivalent_key_group)\n",
    "        for key_group in equivalent_key_group:\n",
    "            #print(key_group)\n",
    "            #print(\"========================\")\n",
    "            #print(bin_mode_left.keys())\n",
    "            #print(equivalent_key_group[key_group][left_table])\n",
    "            #print(\"==========================================\")\n",
    "            #print(bin_mode_right.keys())\n",
    "            #print(equivalent_key_group[key_group][right_table])\n",
    "            all_pdfs = [cond_factor_left.pdfs[key] * cond_factor_left.table_len * cond_factor_left.na_values[key]\n",
    "                        for key in equivalent_key_group[key_group][left_table]] + \\\n",
    "                       [cond_factor_right.pdfs[key] * res * cond_factor_right.na_values[key]\n",
    "                        for key in equivalent_key_group[key_group][right_table]]\n",
    "            all_bin_modes = [bin_mode_left[key] for key in equivalent_key_group[key_group][left_table]] + \\\n",
    "                            [bin_mode_right[key] for key in equivalent_key_group[key_group][right_table]]\n",
    "            #print(\"====================================================\")\n",
    "            #print(equivalent_key_group[key_group][left_table])\n",
    "            #print(equivalent_key_group[key_group][right_table])\n",
    "            new_pdf, new_bin_mode = self.compute_bound_oned(all_pdfs, all_bin_modes, return_factor=True)\n",
    "            res = np.sum(new_pdf)\n",
    "            key_group_pdf[key_group] = new_pdf / res\n",
    "            key_group_bin_mode[key_group] = new_bin_mode\n",
    "            new_union_key_group[key_group] = [key_group]\n",
    "            new_na_values[key_group] = 1.0\n",
    "\n",
    "        for group in union_key_group:\n",
    "            table, keys = union_key_group[group]\n",
    "            new_union_key_group[group] = keys\n",
    "            for key in keys:\n",
    "                key_group_pdf[key] = conditional_factors[table].pdfs[key]\n",
    "                key_group_bin_mode[key] = self.table_buckets[tables_all[table]].oned_bin_modes[key]\n",
    "                new_na_values[key] = conditional_factors[table].na_values[key]\n",
    "\n",
    "        new_factor = Group_Factor(sub_plan_query_str, res, key_group_pdf, key_group_bin_mode,\n",
    "                                  union_key_group_set, new_union_key_group, new_na_values, new_join_cond)\n",
    "        return new_factor, res\n",
    "\n",
    "    def get_join_keys_two_tables(self, left_table, right_table, table_equivalent_group, table_key_equivalent_group,\n",
    "                                 table_key_group_map, join_cond, join_keys, tables_all):\n",
    "        \"\"\"\n",
    "            Get the join keys between two tables\n",
    "        \"\"\"\n",
    "        actual_join_cond = []\n",
    "        for cond in join_cond[left_table]:\n",
    "            if cond in join_cond[right_table]:\n",
    "                actual_join_cond.append(cond)\n",
    "        equivalent_key_group = dict()\n",
    "        union_key_group_set = table_equivalent_group[left_table].union(table_equivalent_group[right_table])\n",
    "        union_key_group = dict()\n",
    "        new_join_cond = join_cond[left_table].union(join_cond[right_table])\n",
    "        if len(actual_join_cond) != 0:\n",
    "            for cond in actual_join_cond:\n",
    "                key1 = cond.split(\"=\")[0].strip()\n",
    "                key2 = cond.split(\"=\")[1].strip()\n",
    "                if key1.split(\".\")[0] == left_table:\n",
    "                    key_left = key1.replace(left_table, tables_all[left_table])\n",
    "                    key_group = table_key_group_map[left_table][key_left]\n",
    "                    if key_group not in equivalent_key_group:\n",
    "                        equivalent_key_group[key_group] = dict()\n",
    "                    if left_table in equivalent_key_group[key_group]:\n",
    "                        equivalent_key_group[key_group][left_table].append(key_left)\n",
    "                    else:\n",
    "                        equivalent_key_group[key_group][left_table] = [key_left]\n",
    "                    assert key2.split(\".\")[0] == right_table, f\"unrecognized table alias\"\n",
    "                    key_right = key2.replace(right_table, tables_all[right_table])\n",
    "                    key_group_t = table_key_group_map[right_table][key_right]\n",
    "                    assert key_group_t == key_group, f\"key group mismatch for join {cond}\"\n",
    "                    if right_table in equivalent_key_group[key_group]:\n",
    "                        equivalent_key_group[key_group][right_table].append(key_right)\n",
    "                    else:\n",
    "                        equivalent_key_group[key_group][right_table] = [key_right]\n",
    "                else:\n",
    "                    assert key2.split(\".\")[0] == left_table, f\"unrecognized table alias\"\n",
    "                    key_left = key2.replace(left_table, tables_all[left_table])\n",
    "                    key_group = table_key_group_map[left_table][key_left]\n",
    "                    if key_group not in equivalent_key_group:\n",
    "                        equivalent_key_group[key_group] = dict()\n",
    "                    if left_table in equivalent_key_group[key_group]:\n",
    "                        equivalent_key_group[key_group][left_table].append(key_left)\n",
    "                    else:\n",
    "                        equivalent_key_group[key_group][left_table] = [key_left]\n",
    "                    assert key1.split(\".\")[0] == right_table, f\"unrecognized table alias\"\n",
    "                    key_right = key1.replace(right_table, tables_all[right_table])\n",
    "                    key_group_t = table_key_group_map[right_table][key_right]\n",
    "                    assert key_group_t == key_group, f\"key group mismatch for join {cond}\"\n",
    "                    if right_table in equivalent_key_group[key_group]:\n",
    "                        equivalent_key_group[key_group][right_table].append(key_right)\n",
    "                    else:\n",
    "                        equivalent_key_group[key_group][right_table] = [key_right]\n",
    "\n",
    "            for group in union_key_group_set:\n",
    "                if group in equivalent_key_group:\n",
    "                    continue\n",
    "                elif group in table_key_equivalent_group[left_table]:\n",
    "                    union_key_group[group] = (left_table, table_key_equivalent_group[left_table][group])\n",
    "                else:\n",
    "                    union_key_group[group] = (right_table, table_key_equivalent_group[right_table][group])\n",
    "\n",
    "        else:\n",
    "            common_key_group = table_equivalent_group[left_table].intersection(table_equivalent_group[right_table])\n",
    "            common_key_group = list(common_key_group)[0]\n",
    "            for group in union_key_group_set:\n",
    "                if group == common_key_group:\n",
    "                    equivalent_key_group[group] = dict()\n",
    "                    equivalent_key_group[group][left_table] = table_key_equivalent_group[left_table][group]\n",
    "                    equivalent_key_group[group][right_table] = table_key_equivalent_group[right_table][group]\n",
    "                elif group in table_key_equivalent_group[left_table]:\n",
    "                    union_key_group[group] = (left_table, table_key_equivalent_group[left_table][group])\n",
    "                else:\n",
    "                    union_key_group[group] = (right_table, table_key_equivalent_group[right_table][group])\n",
    "\n",
    "        return equivalent_key_group, union_key_group_set, union_key_group, new_join_cond\n",
    "\n",
    "    def get_additional_joins_two_tables(self, left_table, right_table, table_equivalent_group,\n",
    "                                        table_key_equivalent_group):\n",
    "        common_key_group = table_equivalent_group[left_table].intersection(table_equivalent_group[right_table])\n",
    "        union_key_group_set = table_equivalent_group[left_table].union(table_equivalent_group[right_table])\n",
    "        union_key_group = dict()\n",
    "        all_join_predicates = []\n",
    "        for group in union_key_group_set:\n",
    "            if group in common_key_group:\n",
    "                left_key = table_key_equivalent_group[left_table][group][0]\n",
    "                left_key = left_table + \".\" + left_key.split(\".\")[-1]\n",
    "                right_key = table_key_equivalent_group[right_table][group][0]\n",
    "                right_key = right_table + \".\" + right_key.split(\".\")[-1]\n",
    "                join_predicate = left_key + \" = \" + right_key\n",
    "                all_join_predicates.append(join_predicate)\n",
    "            if group in table_key_equivalent_group[left_table]:\n",
    "                left_key = table_key_equivalent_group[left_table][group][0]\n",
    "                left_key = left_table + \".\" + left_key.split(\".\")[-1]\n",
    "                union_key_group[group] = left_key\n",
    "            else:\n",
    "                right_key = table_key_equivalent_group[right_table][group][0]\n",
    "                right_key = right_table + \".\" + right_key.split(\".\")[-1]\n",
    "                union_key_group[group] = right_key\n",
    "\n",
    "        return all_join_predicates, union_key_group\n",
    "\n",
    "    def get_sub_plan_join_key(self, sub_plan_query, join_cond):\n",
    "        # returning a subset of join_keys covered by the tables in sub_plan_query\n",
    "        touched_join_cond = set()\n",
    "        untouched_join_cond = set()\n",
    "        for tab in join_cond:\n",
    "            if tab in sub_plan_query:\n",
    "                touched_join_cond = touched_join_cond.union(join_cond[tab])\n",
    "            else:\n",
    "                untouched_join_cond = untouched_join_cond.union(join_cond[tab])\n",
    "        touched_join_cond -= untouched_join_cond\n",
    "\n",
    "        join_keys = dict()\n",
    "        for cond in touched_join_cond:\n",
    "            key1 = cond.split(\"=\")[0].strip()\n",
    "            table1 = key1.split(\".\")[0].strip()\n",
    "            if table1 not in join_keys:\n",
    "                join_keys[table1] = set([key1])\n",
    "            else:\n",
    "                join_keys[table1].add(key1)\n",
    "\n",
    "            key2 = cond.split(\"=\")[1].strip()\n",
    "            table2 = key2.split(\".\")[0].strip()\n",
    "            if table2 not in join_keys:\n",
    "                join_keys[table2] = set([key2])\n",
    "            else:\n",
    "                join_keys[table2].add(key2)\n",
    "\n",
    "        return join_keys\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b6d9ebfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"ground_truth_factors_no_filter.pkl\", \"rb\") as f:\n",
    "    ground_truth_factors_no_filter = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb5daf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle5 as pickle\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def load_sample_imdb(table_buckets, tables_alias, query_file_orders, join_keys, table_key_equivalent_group,\n",
    "                     SPERCENTAGE=1.0, qdir=\"/home/ubuntu/data_CE/saved_models/binned_cards/{}/job/all_job/\"):\n",
    "    qdir = qdir.format(SPERCENTAGE)\n",
    "    all_sample_factors = []\n",
    "    for fn in query_file_orders:\n",
    "        conditional_factors = load_sample_imdb_one_query(table_buckets, tables_alias, fn, join_keys,\n",
    "                                                         table_key_equivalent_group, SPERCENTAGE, qdir)\n",
    "        all_sample_factors.append(conditional_factors)\n",
    "    return all_sample_factors\n",
    "\n",
    "\n",
    "def load_sample_imdb_one_query(table_buckets, tables_alias, query_file_name, join_keys, table_key_equivalent_group,\n",
    "                               SPERCENTAGE=10.0, qdir=\"/home/ubuntu/data_CE/saved_models/binned_cards/{}/job/all_job/\"):\n",
    "    qdir = qdir.format(SPERCENTAGE)\n",
    "    fpath = os.path.join(qdir, query_file_name)\n",
    "    with open(fpath, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "    conditional_factors = dict()\n",
    "    table_pdfs = dict()\n",
    "    filter_size = dict()\n",
    "    for i, alias in enumerate(data[\"all_aliases\"]):\n",
    "        column = data[\"all_columns\"][i]\n",
    "        alias = alias[0]\n",
    "        key = tables_alias[alias] + \".\" + column\n",
    "        cards = data[\"results\"][i][0]\n",
    "        n_bins = table_buckets[tables_alias[alias]].bin_sizes[key]\n",
    "        pdfs = np.zeros(n_bins)\n",
    "        for (j, val) in cards:\n",
    "            if j is None:\n",
    "                j = 0\n",
    "            pdfs[j] += val\n",
    "        table_len = np.sum(pdfs)\n",
    "        print(alias+\".\"+column, table_len, pdfs)\n",
    "        if table_len == 0:\n",
    "            # no sample satisfy the filter, set it with a small value\n",
    "            #print(\"========================\", alias+\".\"+column)\n",
    "            table_len = 1\n",
    "            pdfs = table_key_equivalent_group[tables_alias[alias]].pdfs[key]\n",
    "        else:\n",
    "            pdfs /= table_len\n",
    "        if alias not in table_pdfs:\n",
    "            table_pdfs[alias] = dict()\n",
    "            filter_size[alias] = table_len\n",
    "        table_pdfs[alias][key] = pdfs\n",
    "\n",
    "    for alias in tables_alias:\n",
    "        if alias in table_pdfs:\n",
    "            table_len = min(table_key_equivalent_group[tables_alias[alias]].table_len,\n",
    "                            filter_size[alias]/(SPERCENTAGE/100))\n",
    "            na_values = table_key_equivalent_group[tables_alias[alias]].na_values\n",
    "            conditional_factors[alias] = Factor(tables_alias[alias], table_len, list(table_pdfs[alias].keys()),\n",
    "                                                table_pdfs[alias], na_values)\n",
    "        else:\n",
    "            #TODO: ground-truth distribution\n",
    "            conditional_factors[alias] = table_key_equivalent_group[tables_alias[alias]]\n",
    "    return conditional_factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d7745d4e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'true_card' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-e6667af0e151>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mBE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBound_ensemble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtable_buckets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mground_truth_factors_no_filter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m temp = BE.get_cardinality_bound_all(queries[idx], sub_plan_queries_str_all[idx], \n\u001b[0;32m----> 5\u001b[0;31m                                     q_file_names[idx], True, true_card[q_file.split(\".pkl\")[0]])\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'true_card' is not defined"
     ]
    }
   ],
   "source": [
    "q_file = \"29b.pkl\"\n",
    "idx = q_file_names.index(q_file)\n",
    "BE = Bound_ensemble(table_buckets, schema, ground_truth_factors_no_filter)\n",
    "temp = BE.get_cardinality_bound_all(queries[idx], sub_plan_queries_str_all[idx], \n",
    "                                    q_file_names[idx], True, all_true_card)\n",
    "                                    #true_card[q_file.split(\".pkl\")[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "10a94efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_true_card = np.load(\"/home/ubuntu/CEB/all_true_card_job.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d6bf438d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it3.id 1.0 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      "cct1.id 1.0 [1. 0.]\n",
      "an.person_id 89637.0 [5.3653e+04 1.3933e+04 7.7240e+03 4.3310e+03 2.6190e+03 1.7270e+03\n",
      " 1.0960e+03 8.2200e+02 5.5600e+02 5.0900e+02 3.2600e+02 3.0000e+02\n",
      " 2.2700e+02 1.7600e+02 3.0100e+02 2.2900e+02 1.7000e+02 1.7100e+02\n",
      " 1.1700e+02 1.5100e+02 9.7000e+01 1.0100e+02 9.2000e+01 7.1000e+01\n",
      " 5.5000e+01 3.8000e+01 3.8000e+01 7.0000e+00 0.0000e+00]\n",
      "ci.person_id 83877.0 [3.2274e+04 1.2645e+04 8.2360e+03 4.3730e+03 3.4830e+03 3.3990e+03\n",
      " 1.9380e+03 2.1630e+03 1.7320e+03 9.4600e+02 7.4400e+02 1.4620e+03\n",
      " 7.4500e+02 4.5300e+02 1.3350e+03 1.4700e+03 8.4900e+02 4.8200e+02\n",
      " 9.4600e+02 4.6700e+02 5.1300e+02 3.9900e+02 2.1400e+02 2.3400e+02\n",
      " 1.6700e+02 1.1800e+02 5.4000e+01 5.0000e+00 2.0310e+03]\n",
      "ci.role_id 83877.0 [5.7194e+04 2.6651e+04 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00\n",
      " 0.0000e+00 7.0000e+00 0.0000e+00 2.5000e+01 0.0000e+00 0.0000e+00]\n",
      "ci.person_role_id 83877.0 [27967.  5700.  4220.  3037.  2505.  1981.  2775.  2046.  1654.  1753.\n",
      "  1468.  1183.  1022.  1740.  1836.  1366.  1626.  1234.  1232.   826.\n",
      "   785.   943.  1361.  3205.  3540.   194.   612.  6066.]\n",
      "ci.movie_id 83877.0 [5.6595e+04 1.1349e+04 2.3770e+03 1.4370e+03 1.0560e+03 7.1600e+02\n",
      " 6.0800e+02 4.3000e+02 4.0900e+02 3.3400e+02 3.5000e+02 2.0600e+02\n",
      " 2.3700e+02 2.7600e+02 1.2400e+02 1.6900e+02 1.2000e+02 1.0500e+02\n",
      " 6.7000e+01 8.9000e+01 6.3000e+01 5.8000e+01 4.2000e+01 1.0500e+02\n",
      " 5.1000e+01 5.8000e+01 2.3000e+01 2.7000e+01 2.5000e+01 1.9000e+01\n",
      " 4.6000e+01 1.4000e+01 5.0000e+00 1.7000e+01 3.9000e+01 1.1000e+01\n",
      " 5.0000e+00 3.4000e+01 1.7000e+01 1.5000e+01 4.0000e+00 8.0000e+01\n",
      " 1.0000e+01 1.0000e+01 3.0000e+00 8.0000e+00 0.0000e+00 8.0000e+00\n",
      " 2.0000e+00 1.0000e+00 2.0000e+00 1.2000e+01 3.0000e+00 2.5000e+01\n",
      " 2.0000e+00 1.0000e+00 2.0000e+00 0.0000e+00 2.0000e+00 0.0000e+00\n",
      " 0.0000e+00 2.0000e+00 9.0000e+00 0.0000e+00 8.0000e+00 0.0000e+00\n",
      " 2.0000e+00 0.0000e+00 4.0000e+00 3.0000e+00 2.0000e+00 0.0000e+00\n",
      " 7.0000e+00 5.9370e+03]\n",
      "it.id 1.0 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "pi.info_type_id 296654.0 [     0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.   7567.      0.  62029.      0.   4972.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "   1266.   4145.   1088.      0.      0.      0.      0. 215587.      0.]\n",
      "pi.person_id 296654.0 [82985. 45861. 30607. 20488. 14729. 11177.  8836.  7257.  6012.  5242.\n",
      "  4485.  3979.  3380.  2992.  4698.  3902.  3552.  4042.  3192.  3281.\n",
      "  3060.  3246.  2921.  3040.  3018.  2868.  2928.  1276.  3600.]\n",
      "k.id 0.0 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "mc.company_id 260774.0 [14878.  8880.  6210.  4930.  4132.  3590.  3224.  2847.  2538.  4544.\n",
      "  4150.  3558.  3249.  2963.  3733.  3677.  3194.  3076.  2973.  2653.\n",
      "  3260.  2986.  2896.  3180.  2560.  2870.  2605.  2919.  3112.  2927.\n",
      "  2872.  2750.  2662.  2814.  2827.  2725.  2689.  2661.  2814.  2780.\n",
      "  2865.  2760.  2689.  2687.  2739.  2661.  2664.  2810.  2730.  2638.\n",
      "  2550.  2542.  2646.  2749.  2744.  2834.  2806.  2695.  2484.  2870.\n",
      "  2732.  2714.  2911.  2889.  2616.  3259.  3336.  2830.  3308.  4175.\n",
      "  2677.  3537.  4721.  6097. 12901.]\n",
      "mc.company_type_id 260774.0 [127029. 133745.      0.]\n",
      "mc.movie_id 260774.0 [1.65324e+05 2.79540e+04 1.18140e+04 7.01300e+03 4.92800e+03 3.87500e+03\n",
      " 3.11100e+03 2.51800e+03 2.43700e+03 2.03000e+03 1.92400e+03 1.69500e+03\n",
      " 1.41000e+03 1.28900e+03 1.02500e+03 1.08200e+03 8.38000e+02 7.69000e+02\n",
      " 7.40000e+02 6.50000e+02 6.29000e+02 6.44000e+02 5.04000e+02 5.37000e+02\n",
      " 5.51000e+02 4.58000e+02 4.80000e+02 3.86000e+02 3.89000e+02 3.68000e+02\n",
      " 3.17000e+02 3.68000e+02 2.70000e+02 3.43000e+02 2.87000e+02 2.86000e+02\n",
      " 2.61000e+02 2.12000e+02 2.14000e+02 1.94000e+02 1.98000e+02 1.93000e+02\n",
      " 1.94000e+02 1.45000e+02 1.38000e+02 1.71000e+02 1.57000e+02 1.28000e+02\n",
      " 1.08000e+02 1.08000e+02 1.20000e+02 1.33000e+02 9.10000e+01 1.01000e+02\n",
      " 5.50000e+01 1.43000e+02 5.80000e+01 7.50000e+01 5.30000e+01 5.60000e+01\n",
      " 6.10000e+01 6.70000e+01 8.20000e+01 7.60000e+01 6.60000e+01 6.40000e+01\n",
      " 5.20000e+01 8.70000e+01 4.70000e+01 4.40000e+01 4.20000e+01 3.90000e+01\n",
      " 3.60000e+01 7.46200e+03]\n",
      "mi.info_type_id 39075.0 [4.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00\n",
      " 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00\n",
      " 0.0000e+00 3.9071e+04 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00\n",
      " 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00\n",
      " 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00\n",
      " 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00\n",
      " 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00\n",
      " 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00\n",
      " 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00\n",
      " 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00\n",
      " 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00\n",
      " 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00]\n",
      "mi.movie_id 39075.0 [2.6464e+04 3.6670e+03 9.5600e+02 5.0400e+02 3.5800e+02 2.4900e+02\n",
      " 2.0700e+02 2.0200e+02 1.4400e+02 1.2900e+02 1.0700e+02 8.8000e+01\n",
      " 7.6000e+01 7.7000e+01 5.2000e+01 6.6000e+01 4.1000e+01 4.7000e+01\n",
      " 4.1000e+01 4.2000e+01 4.1000e+01 3.0000e+01 3.0000e+01 2.6000e+01\n",
      " 2.8000e+01 3.0000e+01 2.4000e+01 2.0000e+01 2.9000e+01 2.5000e+01\n",
      " 2.2000e+01 2.2000e+01 2.3000e+01 2.6000e+01 2.3000e+01 1.8000e+01\n",
      " 1.3000e+01 2.2000e+01 1.6000e+01 1.3000e+01 1.4000e+01 1.1000e+01\n",
      " 1.1000e+01 6.0000e+00 7.0000e+00 1.3000e+01 1.2000e+01 1.1000e+01\n",
      " 8.0000e+00 1.0000e+01 6.0000e+00 1.0000e+01 6.0000e+00 9.0000e+00\n",
      " 5.0000e+00 8.0000e+00 5.0000e+00 1.0000e+01 3.0000e+00 4.0000e+00\n",
      " 4.0000e+00 1.0000e+01 5.0000e+00 6.0000e+00 1.0000e+00 1.0000e+00\n",
      " 3.0000e+00 0.0000e+00 4.0000e+00 4.0000e+00 2.0000e+00 0.0000e+00\n",
      " 2.0000e+00 4.8660e+03]\n",
      "mk.keyword_id 451675.0 [10687.  8466.  6800.  6222.  5791.  5280.  5045.  4407.  8324.  7468.\n",
      "  6610.  6092.  5633.  5242.  4662.  4340.  5851.  5782.  5140.  5798.\n",
      "  4895.  4490.  4975.  5064.  4984.  4609.  5844.  4628.  4740.  5297.\n",
      "  4618.  4738.  4708.  4703.  4653.  4560.  4960.  4626.  4778.  5140.\n",
      "  4852.  4587.  4749.  5204.  4575.  4787.  4764.  4906.  4887.  4818.\n",
      "  4508.  4648.  4709.  4484.  4673.  4495.  5071.  4571.  4619.  4697.\n",
      "  4480.  4800.  4649.  4920.  4581.  4714.  4921.  4950.  4926.  4766.\n",
      "  4789.  4626.  4543.  4560.  4579.  4717.  4574.  4935.  4772.  4847.\n",
      "  4811.  4897.  7758.  5884.  7206. 11216.]\n",
      "mk.movie_id 451675.0 [86081. 41085. 28879. 22679. 20162. 18539. 16921. 15950. 14966. 13490.\n",
      " 12666. 11152.  9579.  8415.  7007.  6610.  5466.  5399.  5108.  4766.\n",
      "  4281.  4533.  3697.  3671.  3916.  3644.  3567.  3022.  3154.  2773.\n",
      "  2672.  2872.  2304.  2618.  2408.  2198.  2453.  2150.  2050.  1717.\n",
      "  1810.  1682.  1750.  1519.  1529.  1782.  1531.  1105.  1312.  1060.\n",
      "  1059.  1625.   888.  1380.   769.  1287.   757.   876.   683.   780.\n",
      "   657.   665.   962.   847.   731.   697.   629.   823.   728.   565.\n",
      "   667.   497.   516.  2887.]\n",
      "cct2.id 1.0 [1. 0.]\n",
      "n.id 5033.0 [3.842e+03 1.500e+02 8.000e+01 2.800e+01 2.100e+01 1.400e+01 1.100e+01\n",
      " 8.000e+00 6.000e+00 1.000e+00 5.000e+00 3.000e+00 3.000e+00 4.000e+00\n",
      " 1.000e+00 3.000e+00 0.000e+00 3.000e+00 1.000e+00 0.000e+00 1.000e+00\n",
      " 0.000e+00 0.000e+00 0.000e+00 1.000e+00 0.000e+00 1.000e+00 0.000e+00\n",
      " 8.460e+02]\n",
      "rt.id 1.0 [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "cc.status_id 13509.0 [ 2517. 10992.]\n",
      "cc.subject_id 13509.0 [8634. 4875.]\n",
      "cc.movie_id 13509.0 [8.209e+03 1.220e+03 7.350e+02 4.460e+02 3.570e+02 2.920e+02 2.610e+02\n",
      " 2.200e+02 2.080e+02 1.820e+02 1.390e+02 1.480e+02 1.000e+02 9.800e+01\n",
      " 5.800e+01 6.900e+01 6.700e+01 3.300e+01 5.300e+01 3.900e+01 3.800e+01\n",
      " 3.700e+01 2.400e+01 3.200e+01 3.500e+01 2.700e+01 3.400e+01 2.000e+01\n",
      " 2.500e+01 1.800e+01 2.200e+01 1.200e+01 2.800e+01 1.200e+01 1.700e+01\n",
      " 1.100e+01 1.500e+01 1.000e+01 1.300e+01 1.100e+01 1.000e+01 5.000e+00\n",
      " 1.000e+01 1.200e+01 9.000e+00 5.000e+00 7.000e+00 5.000e+00 4.000e+00\n",
      " 5.000e+00 4.000e+00 5.000e+00 6.000e+00 4.000e+00 2.000e+00 5.000e+00\n",
      " 3.000e+00 1.000e+00 1.000e+00 3.000e+00 2.000e+00 1.000e+00 2.000e+00\n",
      " 5.000e+00 2.000e+00 4.000e+00 1.000e+00 0.000e+00 4.000e+00 4.000e+00\n",
      " 2.000e+00 1.000e+00 0.000e+00 0.000e+00]\n",
      "chn.id 0.0 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.]\n",
      "cn.id 8369.0 [2.958e+03 7.380e+02 3.150e+02 1.750e+02 1.070e+02 7.800e+01 6.300e+01\n",
      " 4.300e+01 4.000e+01 4.700e+01 4.600e+01 3.100e+01 3.700e+01 1.500e+01\n",
      " 2.100e+01 1.500e+01 1.200e+01 1.300e+01 1.200e+01 1.300e+01 1.300e+01\n",
      " 1.000e+01 8.000e+00 7.000e+00 7.000e+00 7.000e+00 6.000e+00 9.000e+00\n",
      " 8.000e+00 5.000e+00 3.000e+00 5.000e+00 1.000e+00 2.000e+00 3.000e+00\n",
      " 1.000e+00 1.000e+00 3.000e+00 2.000e+00 2.000e+00 1.000e+00 3.000e+00\n",
      " 2.000e+00 3.000e+00 1.000e+00 1.000e+00 3.000e+00 3.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 0.000e+00 2.000e+00 0.000e+00\n",
      " 1.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00\n",
      " 1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
      " 0.000e+00 0.000e+00 0.000e+00 1.000e+00 3.458e+03]\n",
      "t.kind_id 0.0 [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "t.id 0.0 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n",
      "==================================================\n",
      "title.id {'left': ['complete_cast.movie_id'], 'right': ['cast_info.movie_id']} 29386810.0\n",
      "==================================================\n",
      "char_name.id {'left': ['char_name.id'], 'right': ['cast_info.person_role_id']} 29386810.0\n",
      "==================================================\n",
      "title.id {'left': ['movie_companies.movie_id'], 'right': ['cast_info.movie_id']} 29386810.0\n",
      "==================================================\n",
      "title.id {'left': ['movie_info.movie_id'], 'right': ['cast_info.movie_id']} 29386810.0\n",
      "==================================================\n",
      "title.id {'left': ['movie_keyword.movie_id'], 'right': ['cast_info.movie_id']} 29386810.0\n",
      "==================================================\n",
      "name.id {'left': ['name.id'], 'right': ['cast_info.person_id', 'aka_name.person_id']} 29386810.0\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'cast_info.person_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-8e95f2f5f0b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mq_file_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqueries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_cardinality_bound_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqueries\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mq_file_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_plan_queries_str_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mq_file_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_file_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mq_file_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mq_file_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mq_file_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-83bf0ccde40c>\u001b[0m in \u001b[0;36mget_cardinality_bound_all\u001b[0;34m(self, query_str, sub_plan_query_str_all, query_name, debug, true_card)\u001b[0m\n\u001b[1;32m    283\u001b[0m                                                                   \u001b[0mtable_key_equivalent_group\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m                                                                   \u001b[0mtable_key_group_map\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m                                                                   join_cond)\n\u001b[0m\u001b[1;32m    286\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m                 curr_bound_factor, res = self.join_two_tables(sub_plan_query_str,\n",
      "\u001b[0;32m<ipython-input-16-83bf0ccde40c>\u001b[0m in \u001b[0;36mjoin_with_one_table\u001b[0;34m(self, sub_plan_query_str, left_table, tables_all, right_bound_factor, cond_factor_left, table_equivalent_group, table_key_equivalent_group, table_key_group_map, join_cond)\u001b[0m\n\u001b[1;32m    332\u001b[0m                         for key in equivalent_key_group[key_group][\"left\"]] + \\\n\u001b[1;32m    333\u001b[0m                        [right_bound_factor.pdfs[key] * res * right_bound_factor.na_values[key]\n\u001b[0;32m--> 334\u001b[0;31m                         for key in equivalent_key_group[key_group][\"right\"]]\n\u001b[0m\u001b[1;32m    335\u001b[0m             \u001b[0mall_bin_modes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbin_mode_left\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mequivalent_key_group\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey_group\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"left\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m                             \u001b[0;34m[\u001b[0m\u001b[0mbin_mode_right\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mequivalent_key_group\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey_group\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"right\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-83bf0ccde40c>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    332\u001b[0m                         for key in equivalent_key_group[key_group][\"left\"]] + \\\n\u001b[1;32m    333\u001b[0m                        [right_bound_factor.pdfs[key] * res * right_bound_factor.na_values[key]\n\u001b[0;32m--> 334\u001b[0;31m                         for key in equivalent_key_group[key_group][\"right\"]]\n\u001b[0m\u001b[1;32m    335\u001b[0m             \u001b[0mall_bin_modes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbin_mode_left\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mequivalent_key_group\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey_group\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"left\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m                             \u001b[0;34m[\u001b[0m\u001b[0mbin_mode_right\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mequivalent_key_group\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey_group\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"right\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'cast_info.person_id'"
     ]
    }
   ],
   "source": [
    "BE = Bound_ensemble(table_buckets, schema, ground_truth_factors_no_filter)\n",
    "res = dict()\n",
    "for q_file_id in range(len(queries)):\n",
    "    temp = BE.get_cardinality_bound_all(queries[q_file_id], sub_plan_queries_str_all[q_file_id], q_file_names[q_file_id])\n",
    "    res[q_file_names[q_file_id].split(\".pkl\")[0]] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2f547592",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'1a'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-bf8146794896>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mq_file_names_real\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".sql\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0msub_query_len\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".sql\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mstart_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: '1a'"
     ]
    }
   ],
   "source": [
    "q_file_names_real = []\n",
    "sub_query_len = []\n",
    "for query_no in range(1, 34):\n",
    "    for suffix in ['a', 'b', 'c', 'd', 'e', 'f', 'g']:\n",
    "        file = f\"{query_no}{suffix}.sql\"\n",
    "        if file in os.listdir(query_path):\n",
    "            q_file_names_real.append(file.split(\".sql\")[0])\n",
    "            sub_query_len.append(len(res[file.split(\".sql\")[0]]))\n",
    "            \n",
    "start_idx = 0\n",
    "true_card = dict()\n",
    "all_true_card_new = []\n",
    "for i, q_file in enumerate(q_file_names_real):\n",
    "    print(\"=============================================\")\n",
    "    print(f\"query no {i}, {q_file} with {len(res[q_file])} number of sub-plan queries\")\n",
    "    pred = res[q_file]\n",
    "    pred = np.asarray(pred)\n",
    "    end_idx = start_idx + sub_query_len[i]\n",
    "    true = all_true_card[start_idx: end_idx]\n",
    "    temp_true = copy.deepcopy(true)\n",
    "    temp_true[true == -1] = pred[true == -1]\n",
    "    all_true_card_new.append(temp_true)\n",
    "    true_card[q_file] = true\n",
    "    assert len(pred) == len(true)\n",
    "    pred = pred[true != -1]\n",
    "    pred[pred <= 0] = 1\n",
    "    true = true[true != -1]\n",
    "    q_errors = np.maximum(pred/true, true/pred)\n",
    "    for i in [50, 90, 95, 99, 100]:\n",
    "        print(f\"q-error {i}% percentile is {np.percentile(q_errors, i)}\")\n",
    "    start_idx = end_idx\n",
    "all_true_card_new = np.concatenate(all_true_card_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f92b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ok = 0\n",
    "with open(\"CE_scheme_est.txt\", \"w\") as f:\n",
    "    for t in q_file_names_real:\n",
    "        for val in res[t]:\n",
    "            ok += 1\n",
    "            f.write(str(val)+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb09ff83",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"true_est_job.txt\", \"w\") as f:\n",
    "    for i in all_true_card_new:\n",
    "        f.write(str(i)+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523c5232",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_factors_no_filter[\"char_name\"].pdfs['char_name.id'] * ground_truth_factors_no_filter[\"char_name\"].table_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdb39cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056a9aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/Users/ziniuw/Desktop/research/Learned_QO/CC_model/CE_scheme_models/stats/model_200.pkl\"\n",
    "with open(model_path, \"rb\") as f:\n",
    "    new_BE = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de13cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_str = \"SELECT COUNT(*) FROM users as u, comments as c, posts as p WHERE p.OwnerUserId = u.Id AND p.Id = c.PostId AND u.UpVotes>=0 AND u.CreationDate>='2010-08-21 21:27:38'::timestamp AND c.CreationDate>='2010-07-21 11:05:37'::timestamp AND c.CreationDate<='2014-08-25 17:59:25'::timestamp\"\n",
    "t = time.time()\n",
    "res = new_BE.get_cardinality_bound(query57)\n",
    "print(time.time() - t)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ff26f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_file = \"/Users/ziniuw/Desktop/past_research/End-to-End-CardEst-Benchmark/workloads/stats_CEB/sub_plan_queries/stats_CEB_sub_queries.sql\"\n",
    "with open(query_file, \"r\") as f:\n",
    "    queries = f.readlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef76e71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "qerror = []\n",
    "latency = []\n",
    "pred = []\n",
    "for i, query_str in enumerate(queries):\n",
    "    query = query_str.split(\"||\")[0][:-1]\n",
    "    print(\"========================\")\n",
    "    true_card = int(query_str.split(\"||\")[-1])\n",
    "    t = time.time()\n",
    "    res = new_BE.get_cardinality_bound(query)\n",
    "    pred.append(res)\n",
    "    latency.append(time.time() - t)\n",
    "    qerror.append(res/true_card)\n",
    "    print(f\"estimating query {i}: predicted {res}, true_card {true_card}, qerror {res/true_card}, latency {time.time() - t}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e595aff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "qerror = np.asarray(qerror)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e6d0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_qerror = copy.deepcopy(qerror)\n",
    "temp_qerror[temp_qerror < 1] = 1/temp_qerror[temp_qerror < 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73add80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [50, 90, 95, 99, 100]:\n",
    "    print(f\"q-error {i}% percentile is {np.percentile(temp_qerror, i)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f81d047",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"stats_CEB_sub_queries_CE_scheme.txt\", \"w\") as f:\n",
    "    for p in pred:\n",
    "        f.write(str(p)+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301c046f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"stats_CEB_exec.sql\", \"r\") as f:\n",
    "    queries = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e23ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"stats_CEB_exec.sql\", \"w\") as f:\n",
    "    for q in queries:\n",
    "        q = q.split(\"||\")[-1]\n",
    "        f.write(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38019fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
