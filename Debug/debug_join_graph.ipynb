{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6025f1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "sys.path.append(\"/home/ubuntu/CE_scheme/\")\n",
    "from Schemas.imdb.schema import gen_imdb_schema\n",
    "from Join_scheme.data_prepare import read_table_csv\n",
    "from Join_scheme.binning import identify_key_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d81e2b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'kind_type.id': {'aka_title.kind_id', 'title.kind_id', 'kind_type.id'}, 'info_type.id': {'movie_info.info_type_id', 'movie_info_idx.info_type_id', 'info_type.id', 'person_info.info_type_id'}, 'title.id': {'title.id', 'complete_cast.movie_id', 'movie_companies.movie_id', 'aka_title.movie_id', 'cast_info.movie_id', 'movie_info_idx.movie_id', 'movie_keyword.movie_id', 'movie_info.movie_id'}, 'name.id': {'aka_name.person_id', 'person_info.person_id', 'name.id', 'cast_info.person_id'}, 'char_name.id': {'cast_info.person_role_id', 'char_name.id'}, 'role_type.id': {'cast_info.role_id', 'role_type.id'}, 'comp_cast_type.id': {'complete_cast.status_id', 'comp_cast_type.id', 'complete_cast.subject_id'}, 'keyword.id': {'movie_keyword.keyword_id', 'keyword.id'}, 'company_name.id': {'company_name.id', 'movie_companies.company_id'}, 'company_type.id': {'movie_companies.company_type_id', 'company_type.id'}}\n"
     ]
    }
   ],
   "source": [
    "data_path = \"/home/ubuntu/data_CE/{}.csv\"\n",
    "schema = gen_imdb_schema(data_path)\n",
    "all_keys, equivalent_keys = identify_key_values(schema)\n",
    "print(equivalent_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fabd33ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/ubuntu/data_CE/job/job_sub_plan_queries.txt\", \"r\") as f:\n",
    "    sub_plan_queries = f.read()\n",
    "psql_raw = sub_plan_queries.split(\"query: 0\")[1:]\n",
    "queries = []\n",
    "query_path = \"/home/ubuntu/data_CE/job/\"\n",
    "for file in os.listdir(query_path):\n",
    "    if file.endswith(\".sql\"):\n",
    "        with open(query_path+file, \"r\") as f:\n",
    "            q = f.readline()\n",
    "            queries.append(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37074e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_plan_queries_str_all = []\n",
    "for per_query in psql_raw:\n",
    "    sub_plan_queries = []\n",
    "    sub_plan_queries_str = []\n",
    "    num_sub_plan_queries = len(per_query.split(\"query: \"))\n",
    "    all_info = per_query.split(\"RELOPTINFO (\")[1:]\n",
    "    assert num_sub_plan_queries*2 == len(all_info)\n",
    "    for i in range(num_sub_plan_queries):\n",
    "        idx = i*2\n",
    "        table1 = all_info[idx].split(\"): rows=\")[0]\n",
    "        table2 = all_info[idx+1].split(\"): rows=\")[0]\n",
    "        table_str = table1 + \" \" + table2\n",
    "        sub_plan_queries_str.append(table_str)\n",
    "    sub_plan_queries_str_all.append(sub_plan_queries_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0a870ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "from Join_scheme.join_graph import process_condition, get_join_hyper_graph, parse_query_all_join, \\\n",
    "    parse_query_all_single_table\n",
    "from Join_scheme.data_prepare import identify_key_values\n",
    "\n",
    "\n",
    "class Factor:\n",
    "    \"\"\"\n",
    "    This the class defines a multidimensional conditional probability.\n",
    "    \"\"\"\n",
    "    def __init__(self, variables, pdfs, equivalent_variables=[]):\n",
    "        self.variables = variables\n",
    "        self.equivalent_variables = equivalent_variables\n",
    "        self.pdfs = pdfs\n",
    "        self.cardinalities = dict()\n",
    "        for i, var in enumerate(self.variables):\n",
    "            self.cardinalities[var] = len(pdfs[i])\n",
    "            if len(equivalent_variables) != 0:\n",
    "                self.cardinalities[equivalent_variables[i]] = len(pdfs[i])\n",
    "\n",
    "\n",
    "class Bound_ensemble:\n",
    "    \"\"\"\n",
    "    This the class where we store all the trained models and perform inference on the bound.\n",
    "    \"\"\"\n",
    "    def __init__(self, bns, table_buckets, schema):\n",
    "        self.bns = bns\n",
    "        self.table_buckets = table_buckets\n",
    "        self.schema = schema\n",
    "        self.all_keys, self.equivalent_keys = identify_key_values(schema)\n",
    "        self.all_join_conds = None\n",
    "        self.reverse_table_alias = None\n",
    "\n",
    "    def parse_query_simple(self, query):\n",
    "        \"\"\"\n",
    "        If your selection query contains no aggregation and nested sub-queries, you can use this function to parse a\n",
    "        join query. Otherwise, use parse_query function.\n",
    "        \"\"\"\n",
    "        tables_all, join_cond, join_keys = parse_query_all_join(query)\n",
    "        #TODO: implement functions on parsing filter conditions.\n",
    "        table_query = parse_query_all_single_table(query)\n",
    "        return tables_all, table_query, join_cond, join_keys\n",
    "\n",
    "    def get_all_id_conidtional_distribution(self, table_queries, join_keys, equivalent_group):\n",
    "        #TODO: make it work on query-driven and sampling based\n",
    "        res = dict()\n",
    "        for table in join_keys:\n",
    "            key_attrs = list(join_keys[table])\n",
    "            if table in table_queries:\n",
    "                table_query = table_queries[table]\n",
    "            else:\n",
    "                table_query = {}\n",
    "            id_attrs, probs = self.bns[table].query_id_prob(table_query, key_attrs)\n",
    "            new_id_attrs = []\n",
    "            for K in id_attrs:\n",
    "                for PK in equivalent_group:\n",
    "                    if K in equivalent_group[PK]:\n",
    "                        new_id_attrs.append(PK)\n",
    "            assert len(new_id_attrs) == len(id_attrs)\n",
    "            res[table] = Factor(id_attrs, probs, new_id_attrs)\n",
    "        return res\n",
    "\n",
    "    def eliminate_one_key_group_general(self, tables, key_group, factors):\n",
    "        rest_groups = dict()\n",
    "        rest_group_tables = dict()\n",
    "        for table in tables:\n",
    "            assert key_group in factors[table].equivalent_variables\n",
    "            temp = copy.deepcopy(factors[table].equivalent_variables)\n",
    "            temp.remove(key_group)\n",
    "            for keys in temp:\n",
    "                if keys in rest_groups:\n",
    "                    assert factors[table].cardinalities[keys] == rest_groups[keys]\n",
    "                    rest_group_tables[keys].append(table)\n",
    "                else:\n",
    "                    rest_groups[keys] = factors[table].cardinalities[keys]\n",
    "                    rest_group_tables[keys] = [table]\n",
    "\n",
    "        new_factor_variables = []\n",
    "        new_factor_cardinalities = []\n",
    "        for key in rest_groups:\n",
    "            new_factor_variables.append(key)\n",
    "            new_factor_cardinalities.append(rest_groups[key])\n",
    "        new_factor_pdf = np.zeros(tuple(new_factor_cardinalities))\n",
    "\n",
    "    def eliminate_one_key_group(self, tables, key_group, factors, relevant_keys):\n",
    "        \"\"\"This version only supports 2D distributions\"\"\"\n",
    "        rest_group = None\n",
    "        rest_group_cardinalty = 0\n",
    "        eliminated_tables = []\n",
    "        rest_group_tables = []\n",
    "        for table in tables:\n",
    "            assert key_group in factors[table].equivalent_variables\n",
    "            temp = copy.deepcopy(factors[table].equivalent_variables)\n",
    "            temp.remove(key_group)\n",
    "            if len(temp) == 0:\n",
    "                eliminated_tables.append(table)\n",
    "            for key in temp:\n",
    "                if rest_group:\n",
    "                    assert factors[table].cardinalities[key] == rest_group_cardinalty\n",
    "                    rest_group_tables.append(table)\n",
    "                else:\n",
    "                    rest_group = key\n",
    "                    rest_group_cardinalty = factors[table].cardinalities[key]\n",
    "                    rest_group_tables = [table]\n",
    "\n",
    "        all_probs_eliminated = []\n",
    "        all_modes_eliminated = []\n",
    "        for table in eliminated_tables:\n",
    "            bin_modes = self.table_buckets[table].oned_bin_modes[relevant_keys[key_group][table]]\n",
    "            all_probs_eliminated.append(factors[table].pdfs)\n",
    "            all_modes_eliminated.append(np.minimum(bin_modes, factors[table].pdfs))\n",
    "\n",
    "        if rest_group:\n",
    "            new_factor_pdf = np.zeros(rest_group_cardinalty)\n",
    "        else:\n",
    "            return self.compute_bound_oned(all_probs_eliminated, all_modes_eliminated)\n",
    "\n",
    "        for i in range(rest_group_cardinalty):\n",
    "            for table in rest_group_tables:\n",
    "                idx_f = factors[table].equivalent_variables.index(key_group)\n",
    "                idx_b = self.table_buckets[table].id_attributes.index(relevant_keys[key_group][table])\n",
    "                bin_modes = self.table_buckets[table].twod_bin_modes[relevant_keys[key_group][table]]\n",
    "                if idx_f == 0 and idx_b == 0:\n",
    "                    all_probs_eliminated.append(factors[table].pdfs[:, i])\n",
    "                    all_modes_eliminated.append(np.minimum(bin_modes[:, i], factors[table].pdfs[:, i]))\n",
    "                elif idx_f == 0 and idx_b == 1:\n",
    "                    all_probs_eliminated.append(factors[table].pdfs[:, i])\n",
    "                    all_modes_eliminated.append(np.minimum(bin_modes[i, :], factors[table].pdfs[:, i]))\n",
    "                elif idx_f == 1 and idx_b == 0:\n",
    "                    all_probs_eliminated.append(factors[table].pdfs[i, :])\n",
    "                    all_modes_eliminated.append(np.minimum(bin_modes[:, i], factors[table].pdfs[i, :]))\n",
    "                else:\n",
    "                    all_probs_eliminated.append(factors[table].pdfs[i, :])\n",
    "                    all_modes_eliminated.append(np.minimum(bin_modes[i, :], factors[table].pdfs[i, :]))\n",
    "            new_factor_pdf[i] = self.compute_bound_oned(all_probs_eliminated, all_modes_eliminated)\n",
    "\n",
    "        for table in rest_group_tables:\n",
    "            factors[table] = Factor([rest_group], new_factor_pdf, [rest_group])\n",
    "\n",
    "        return None\n",
    "\n",
    "    def compute_bound_oned(self, all_probs, all_modes):\n",
    "        all_probs = np.stack(all_probs, axis=0)\n",
    "        all_modes = np.stack(all_modes, axis=0)\n",
    "        multiplier = np.prod(all_modes, axis=0)\n",
    "        non_zero_idx = np.where(multiplier != 0)[0]\n",
    "        min_number = np.amin(all_probs[:, non_zero_idx]/all_modes[:, non_zero_idx], axis=0)\n",
    "        multiplier[non_zero_idx] = multiplier[non_zero_idx] * min_number\n",
    "        return np.sum(multiplier)\n",
    "\n",
    "    def get_optimal_elimination_order(self, equivalent_group, join_keys, factors):\n",
    "        \"\"\"\n",
    "        This function determines the optimial elimination order for each key group\n",
    "        \"\"\"\n",
    "        cardinalities = dict()\n",
    "        lengths = dict()\n",
    "        tables_involved = dict()\n",
    "        relevant_keys = dict()\n",
    "        for group in equivalent_group:\n",
    "            relevant_keys[group] = dict()\n",
    "            lengths[group] = len(equivalent_group[group])\n",
    "            cardinalities[group] = []\n",
    "            tables_involved[group] = set([])\n",
    "            for keys in equivalent_group[group]:\n",
    "                for table in join_keys:\n",
    "                    if keys in join_keys[table]:\n",
    "                        cardinalities[group].append(len(join_keys[table]))\n",
    "                        tables_involved[group].add(table)\n",
    "                        variables = factors[table].variables\n",
    "                        variables[variables.index(keys)] = group\n",
    "                        factors[table].variables = variables\n",
    "                        relevant_keys[group][table] = keys\n",
    "                        break\n",
    "            cardinalities[group] = np.asarray(cardinalities[group])\n",
    "\n",
    "        optimal_order = list(equivalent_group.keys())\n",
    "        for i in range(len(optimal_order)):\n",
    "            min_idx = i\n",
    "            for j in range(i+1, len(optimal_order)):\n",
    "                min_group = optimal_order[min_idx]\n",
    "                curr_group = optimal_order[j]\n",
    "                if np.max(cardinalities[curr_group]) < np.max(cardinalities[min_group]):\n",
    "                    min_idx = j\n",
    "                else:\n",
    "                    min_max_tables = np.max(cardinalities[min_group])\n",
    "                    min_num_max_tables = len(np.where(cardinalities[min_group] == min_max_tables)[0])\n",
    "                    curr_max_tables = np.max(cardinalities[curr_group])\n",
    "                    curr_num_max_tables = len(np.where(cardinalities[curr_group] == curr_max_tables)[0])\n",
    "                    if curr_num_max_tables < min_num_max_tables:\n",
    "                        min_idx = j\n",
    "                    elif lengths[curr_group] < lengths[min_group]:\n",
    "                        min_idx = j\n",
    "            optimal_order[i], optimal_order[min_idx] = optimal_order[min_idx], optimal_order[i]\n",
    "        return optimal_order, tables_involved, relevant_keys\n",
    "\n",
    "    def get_cardinality_bound_one(self, query_str):\n",
    "        tables_all, table_queries, join_cond, join_keys = self.parse_query_simple(query_str)\n",
    "        equivalent_group = get_join_hyper_graph(join_keys, self.equivalent_keys)\n",
    "        conditional_factors = self.get_all_id_conidtional_distribution(table_queries, join_keys, equivalent_group)\n",
    "        optimal_order, tables_involved, relevant_keys = self.get_optimal_elimination_order(equivalent_group, join_keys,\n",
    "                                                                            conditional_factors)\n",
    "\n",
    "        for key_group in optimal_order:\n",
    "            tables = tables_involved[key_group]\n",
    "            res = self.eliminate_one_key_group(tables, key_group, conditional_factors, relevant_keys)\n",
    "        return res\n",
    "\n",
    "    def get_cardinality_bound_all(self, query_str, sub_plan_query_str_all):\n",
    "        tables_all, table_queries, join_cond, join_keys = self.parse_query_simple(query_str)\n",
    "        print(join_cond)\n",
    "        print(tables_all, len(tables_all))\n",
    "        print(join_keys, len(join_keys))\n",
    "        equivalent_group = get_join_hyper_graph(join_keys, self.equivalent_keys)\n",
    "        #conditional_factors = self.get_all_id_conidtional_distribution(table_queries, join_keys, equivalent_group)\n",
    "        #self.all_join_conds = set()\n",
    "        #self.reverse_table_alias = {v: k for k, v in tables_all.items()}\n",
    "        res = []\n",
    "        for sub_plan_query_str in sub_plan_query_str_all:\n",
    "            sub_plan_query = set([tables_all[t] for t in sub_plan_query_str.split(\" \")])\n",
    "            curr_join_keys = self.get_sub_plan_join_key(sub_plan_query, join_cond)\n",
    "            assert len(sub_plan_query) == len(curr_join_keys)\n",
    "            #print(sub_plan_query)\n",
    "            #print(curr_join_keys)\n",
    "            #curr_equivalent_group = self.get_sub_plan_equivalent_group(sub_plan_query, equivalent_group)\n",
    "            #curr_factors = self.get_sub_plan_conditional_factors(sub_plan_query, conditional_factors)\n",
    "            #optimal_order, tables_involved, relevant_keys = self.get_optimal_elimination_order(curr_equivalent_group,\n",
    "                #                                                                               curr_join_keys,\n",
    "               #                                                                                curr_factors)\n",
    "\n",
    "            #for key_group in optimal_order:\n",
    "             #   tables = tables_involved[key_group]\n",
    "              #  res.append(self.eliminate_one_key_group(tables, key_group, conditional_factors, relevant_keys))\n",
    "        return res\n",
    "\n",
    "    def get_sub_plan_join_key(self, sub_plan_query, join_cond):\n",
    "        # returning a subset of join_keys covered by the tables in sub_plan_query\n",
    "        touched_join_cond = set()\n",
    "        untouched_join_cond = set()\n",
    "        for tab in join_cond:\n",
    "            if tab in sub_plan_query:\n",
    "                touched_join_cond = touched_join_cond.union(join_cond[tab])\n",
    "            else:\n",
    "                untouched_join_cond = untouched_join_cond.union(join_cond[tab])\n",
    "        touched_join_cond -= untouched_join_cond\n",
    "\n",
    "        join_keys = dict()\n",
    "        for cond in touched_join_cond:\n",
    "            key1 = cond.split(\"=\")[0].strip()\n",
    "            table1 = key1.split(\".\")[0].strip()\n",
    "            if table1 not in join_keys:\n",
    "                join_keys[table1] = set([key1])\n",
    "            else:\n",
    "                join_keys[table1].add(key1)\n",
    "\n",
    "            key2 = cond.split(\"=\")[1].strip()\n",
    "            table2 = key2.split(\".\")[0].strip()\n",
    "            if table2 not in join_keys:\n",
    "                join_keys[table2] = set([key2])\n",
    "            else:\n",
    "                join_keys[table2].add(key2)\n",
    "\n",
    "        return join_keys\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "597c4905",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"SELECT MIN(chn.name) AS voiced_char, MIN(n.name) AS voicing_actress, MIN(t.title) AS voiced_animation FROM aka_name AS an, complete_cast AS cc, comp_cast_type AS cct1, comp_cast_type AS cct2, char_name AS chn, cast_info AS ci, company_name AS cn, info_type AS it, info_type AS it3, keyword AS k, movie_companies AS mc, movie_info AS mi, movie_keyword AS mk, name AS n, person_info AS pi, role_type AS rt, title AS t WHERE cct1.kind  ='cast' AND cct2.kind  ='complete+verified' AND chn.name  = 'Queen' AND ci.note  in ('(voice)', '(voice) (uncredited)', '(voice: English version)') AND cn.country_code ='[us]' AND it.info  = 'release dates' AND it3.info  = 'height' AND k.keyword  = 'computer-animation' AND mi.info  like 'USA:%200%' AND n.gender ='f' and n.name like '%An%' AND rt.role ='actress' AND t.title  = 'Shrek 2' AND t.production_year  between 2000 and 2005 AND t.id = mi.movie_id AND t.id = mc.movie_id AND t.id = ci.movie_id AND t.id = mk.movie_id AND t.id = cc.movie_id AND mc.movie_id = ci.movie_id AND mc.movie_id = mi.movie_id AND mc.movie_id = mk.movie_id AND mc.movie_id = cc.movie_id AND mi.movie_id = ci.movie_id AND mi.movie_id = mk.movie_id AND mi.movie_id = cc.movie_id AND ci.movie_id = mk.movie_id AND ci.movie_id = cc.movie_id AND mk.movie_id = cc.movie_id AND cn.id = mc.company_id AND it.id = mi.info_type_id AND n.id = ci.person_id AND rt.id = ci.role_id AND n.id = an.person_id AND ci.person_id = an.person_id AND chn.id = ci.person_role_id AND n.id = pi.person_id AND ci.person_id = pi.person_id AND it3.id = pi.info_type_id AND k.id = mk.keyword_id AND cct1.id = cc.subject_id AND cct2.id = cc.status_id;\\n\""
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a144bdf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': {'title.id = movie_keyword.movie_id', 'title.id = movie_companies.movie_id', 'title.id = complete_cast.movie_id', 'title.id = movie_info.movie_id', 'title.id = cast_info.movie_id'}, 'movie_info': {'movie_info.movie_id = movie_keyword.movie_id', 'movie_info.movie_id = complete_cast.movie_id', 'info_type.id = movie_info.info_type_id', 'title.id = movie_info.movie_id', 'movie_info.movie_id = cast_info.movie_id', 'movie_companies.movie_id = movie_info.movie_id'}, 'movie_companies': {'movie_companies.movie_id = cast_info.movie_id', 'title.id = movie_companies.movie_id', 'movie_companies.movie_id = movie_keyword.movie_id', 'movie_companies.movie_id = movie_info.movie_id', 'company_name.id = movie_companies.company_id', 'movie_companies.movie_id = complete_cast.movie_id'}, 'cast_info': {'name.id = cast_info.person_id', 'movie_companies.movie_id = cast_info.movie_id', 'cast_info.person_id = person_info.person_id', 'cast_info.movie_id = movie_keyword.movie_id', 'char_name.id = cast_info.person_role_id', 'cast_info.movie_id = complete_cast.movie_id', 'cast_info.person_id = aka_name.person_id', 'title.id = cast_info.movie_id', 'movie_info.movie_id = cast_info.movie_id', 'role_type.id = cast_info.role_id'}, 'movie_keyword': {'title.id = movie_keyword.movie_id', 'movie_info.movie_id = movie_keyword.movie_id', 'cast_info.movie_id = movie_keyword.movie_id', 'keyword.id = mkeyword.keyword_id', 'movie_keyword.movie_id = complete_cast.movie_id', 'movie_companies.movie_id = movie_keyword.movie_id'}, 'complete_cast': {'comp_cast_type.id = complete_cast.status_id', 'title.id = complete_cast.movie_id', 'movie_keyword.movie_id = complete_cast.movie_id', 'movie_info.movie_id = complete_cast.movie_id', 'cast_info.movie_id = complete_cast.movie_id', 'comp_cast_type.id = complete_cast.subject_id', 'movie_companies.movie_id = complete_cast.movie_id'}, 'company_name': {'company_name.id = movie_companies.company_id'}, 'info_type': {'info_type.id = person_info.info_type_id', 'info_type.id = movie_info.info_type_id'}, 'name': {'name.id = cast_info.person_id', 'name.id = aname.person_id', 'name.id = person_info.person_id'}, 'role_type': {'role_type.id = cast_info.role_id'}, 'aka_name': {'name.id = aname.person_id', 'cast_info.person_id = aka_name.person_id'}, 'char_name': {'char_name.id = cast_info.person_role_id'}, 'person_info': {'name.id = person_info.person_id', 'info_type.id = person_info.info_type_id', 'cast_info.person_id = person_info.person_id'}, 'keyword': {'keyword.id = mkeyword.keyword_id'}, 'comp_cast_type': {'comp_cast_type.id = complete_cast.subject_id', 'comp_cast_type.id = complete_cast.status_id'}}\n",
      "{'an': 'aka_name', 'cc': 'complete_cast', 'cct1': 'comp_cast_type', 'cct2': 'comp_cast_type', 'chn': 'char_name', 'ci': 'cast_info', 'cn': 'company_name', 'it': 'info_type', 'it3': 'info_type', 'k': 'keyword', 'mc': 'movie_companies', 'mi': 'movie_info', 'mk': 'movie_keyword', 'n': 'name', 'pi': 'person_info', 'rt': 'role_type', 't': 'title'} 17\n",
      "{'title': {'title.id'}, 'movie_info': {'movie_info.info_type_id', 'movie_info.movie_id'}, 'movie_companies': {'movie_companies.movie_id', 'movie_companies.company_id'}, 'cast_info': {'cast_info.role_id', 'cast_info.person_role_id', 'cast_info.person_id', 'cast_info.movie_id'}, 'movie_keyword': {'movie_keyword.keyword_id', 'movie_keyword.movie_id'}, 'complete_cast': {'complete_cast.status_id', 'complete_cast.movie_id', 'complete_cast.subject_id'}, 'company_name': {'company_name.id'}, 'info_type': {'info_type.id'}, 'name': {'name.id'}, 'role_type': {'role_type.id'}, 'aka_name': {'aka_name.person_id'}, 'char_name': {'char_name.id'}, 'person_info': {'person_info.info_type_id', 'person_info.person_id'}, 'keyword': {'keyword.id'}, 'comp_cast_type': {'comp_cast_type.id'}} 15\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-84225ad73b0c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mBE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBound_ensemble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mBE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_cardinality_bound_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqueries\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_plan_queries_str_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m#print(time.time()-t)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-51-ff2bb4fb43b2>\u001b[0m in \u001b[0;36mget_cardinality_bound_all\u001b[0;34m(self, query_str, sub_plan_query_str_all)\u001b[0m\n\u001b[1;32m    221\u001b[0m             \u001b[0msub_plan_query\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtables_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msub_plan_query_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m             \u001b[0mcurr_join_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_sub_plan_join_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_plan_query\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoin_cond\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m             \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_plan_query\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_join_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m             \u001b[0;31m#print(sub_plan_query)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m             \u001b[0;31m#print(curr_join_keys)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "BE = Bound_ensemble(None, None, schema)\n",
    "BE.get_cardinality_bound_all(queries[0], sub_plan_queries_str_all[0])\n",
    "#print(time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056a9aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/Users/ziniuw/Desktop/research/Learned_QO/CC_model/CE_scheme_models/stats/model_200.pkl\"\n",
    "with open(model_path, \"rb\") as f:\n",
    "    new_BE = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de13cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_str = \"SELECT COUNT(*) FROM users as u, comments as c, posts as p WHERE p.OwnerUserId = u.Id AND p.Id = c.PostId AND u.UpVotes>=0 AND u.CreationDate>='2010-08-21 21:27:38'::timestamp AND c.CreationDate>='2010-07-21 11:05:37'::timestamp AND c.CreationDate<='2014-08-25 17:59:25'::timestamp\"\n",
    "t = time.time()\n",
    "res = new_BE.get_cardinality_bound(query57)\n",
    "print(time.time() - t)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ff26f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_file = \"/Users/ziniuw/Desktop/past_research/End-to-End-CardEst-Benchmark/workloads/stats_CEB/sub_plan_queries/stats_CEB_sub_queries.sql\"\n",
    "with open(query_file, \"r\") as f:\n",
    "    queries = f.readlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef76e71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "qerror = []\n",
    "latency = []\n",
    "pred = []\n",
    "for i, query_str in enumerate(queries):\n",
    "    query = query_str.split(\"||\")[0][:-1]\n",
    "    print(\"========================\")\n",
    "    true_card = int(query_str.split(\"||\")[-1])\n",
    "    t = time.time()\n",
    "    res = new_BE.get_cardinality_bound(query)\n",
    "    pred.append(res)\n",
    "    latency.append(time.time() - t)\n",
    "    qerror.append(res/true_card)\n",
    "    print(f\"estimating query {i}: predicted {res}, true_card {true_card}, qerror {res/true_card}, latency {time.time() - t}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e595aff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "qerror = np.asarray(qerror)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e6d0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_qerror = copy.deepcopy(qerror)\n",
    "temp_qerror[temp_qerror < 1] = 1/temp_qerror[temp_qerror < 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73add80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [50, 90, 95, 99, 100]:\n",
    "    print(f\"q-error {i}% percentile is {np.percentile(temp_qerror, i)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f81d047",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"stats_CEB_sub_queries_CE_scheme.txt\", \"w\") as f:\n",
    "    for p in pred:\n",
    "        f.write(str(p)+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301c046f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"stats_CEB_exec.sql\", \"r\") as f:\n",
    "    queries = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e23ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"stats_CEB_exec.sql\", \"w\") as f:\n",
    "    for q in queries:\n",
    "        q = q.split(\"||\")[-1]\n",
    "        f.write(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38019fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
